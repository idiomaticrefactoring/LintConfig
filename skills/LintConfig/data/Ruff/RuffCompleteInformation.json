{
  "AIR001": {
    "description": "[Status: stable] [Name: airflow-variable-name-task-id-mismatch] [Message: Task variable name should match the <code>task_id</code>: \"{task_id}\"] What it does Checks that the task variable name matches the task_id value for Airflow Operators. Why is this bad? When initializing an Airflow Operator, for consistency, the variable name should match the task_id value. This makes it easier to follow the flow of the DAG. Example from airflow.operators import PythonOperator incorrect_name = PythonOperator(task_id=\"my_task\") Use instead: from airflow.operators import PythonOperator my_task = PythonOperator(task_id=\"my_task\")",
    "option": {}
  },
  "AIR002": {
    "description": "[Status: stable] [Name: airflow-dag-no-schedule-argument] [Message: <code>DAG</code> or <code>@dag</code> should have an explicit <code>schedule</code> argument] What it does Checks for a DAG() class or @dag() decorator without an explicit schedule (or schedule_interval for Airflow 1) parameter. Why is this bad? The default value of the schedule parameter on Airflow 2 and schedule_interval on Airflow 1 is timedelta(days=1), which is almost never what a user is looking for. Airflow 3 changed the default value to None, and would break existing dags using the implicit default. If your dag does not have an explicit schedule / schedule_interval argument, Airflow 2 schedules a run for it every day (at the time determined by start_date). Such a dag will no longer be scheduled on Airflow 3 at all, without any exceptions or other messages visible to the user. Example from airflow import DAG # Using the implicit default schedule. dag = DAG(dag_id=\"my_dag\") Use instead: from datetime import timedelta from airflow import DAG dag = DAG(dag_id=\"my_dag\", schedule=timedelta(days=1))",
    "option": {}
  },
  "AIR301": {
    "description": "[Status: stable] [Name: airflow3-removal] [Message: <code>{deprecated}</code> is removed in Airflow 3.0] What it does Checks for uses of deprecated Airflow functions and values. Why is this bad? Airflow 3.0 removed various deprecated functions, members, and other values. Some have more modern replacements. Others are considered too niche and not worth continued maintenance in Airflow. Example from airflow.utils.dates import days_ago yesterday = days_ago(today, 1) Use instead: from datetime import timedelta yesterday = today - timedelta(days=1)",
    "option": {}
  },
  "AIR302": {
    "description": "[Status: stable] [Name: airflow3-moved-to-provider] [Message: <code>{deprecated}</code> is moved into <code>{provider}</code> provider in Airflow 3.0;] What it does Checks for uses of Airflow functions and values that have been moved to its providers (e.g., apache-airflow-providers-fab). Why is this bad? Airflow 3.0 moved various deprecated functions, members, and other values to its providers. The user needs to install the corresponding provider and replace the original usage with the one in the provider. Example from airflow.auth.managers.fab.fab_auth_manager import FabAuthManager fab_auth_manager_app = FabAuthManager().get_fastapi_app() Use instead: from airflow.providers.fab.auth_manager.fab_auth_manager import FabAuthManager fab_auth_manager_app = FabAuthManager().get_fastapi_app()",
    "option": {}
  },
  "AIR303": {
    "description": "[Status: preview] [Name: airflow3-incompatible-function-signature] [Message: <code>{function_name}</code> signature is changed in Airflow 3.0] What it does Checks for Airflow function calls that will raise a runtime error in Airflow 3.0 due to function signature changes, such as functions that changed to accept only keyword arguments, parameter reordering, or parameter type changes. Why is this bad? Airflow 3.0 introduces changes to function signatures. Code that worked in Airflow 2.x will raise a runtime error if not updated in Airflow 3.0. Example from airflow.lineage.hook import HookLineageCollector collector = HookLineageCollector() # Passing positional arguments will raise a runtime error in Airflow 3.0 collector.create_asset(\"s3://bucket/key\") Use instead: from airflow.lineage.hook import HookLineageCollector collector = HookLineageCollector() # Passing arguments as keyword arguments instead of positional arguments collector.create_asset(uri=\"s3://bucket/key\")",
    "option": {}
  },
  "AIR311": {
    "description": "[Status: stable] [Name: airflow3-suggested-update] [Message: <code>{deprecated}</code> is removed in Airflow 3.0; It still works in Airflow 3.0 but is expected to be removed in a future version.] What it does Checks for uses of deprecated Airflow functions and values that still have a compatibility layer. Why is this bad? Airflow 3.0 removed various deprecated functions, members, and other values. Some have more modern replacements. Others are considered too niche and not worth continued maintenance in Airflow. Even though these symbols still work fine on Airflow 3.0, they are expected to be removed in a future version. Where available, users should replace the removed functionality with the new alternatives. Example from airflow import Dataset Dataset(uri=\"test://test/\") Use instead: from airflow.sdk import Asset Asset(uri=\"test://test/\")",
    "option": {}
  },
  "AIR312": {
    "description": "[Status: stable] [Name: airflow3-suggested-to-move-to-provider] [Message: <code>{deprecated}</code> is deprecated and moved into <code>{provider}</code> provider in Airflow 3.0; It still works in Airflow 3.0 but is expected to be removed in a future version.] What it does Checks for uses of Airflow functions and values that have been moved to its providers but still have a compatibility layer (e.g., apache-airflow-providers-standard). Why is this bad? Airflow 3.0 moved various deprecated functions, members, and other values to its providers. Even though these symbols still work fine on Airflow 3.0, they are expected to be removed in a future version. The user is suggested to install the corresponding provider and replace the original usage with the one in the provider. Example from airflow.operators.python import PythonOperator def print_context(ds=None, **kwargs): print(kwargs) print(ds) print_the_context = PythonOperator( task_id=\"print_the_context\", python_callable=print_context ) Use instead: from airflow.providers.standard.operators.python import PythonOperator def print_context(ds=None, **kwargs): print(kwargs) print(ds) print_the_context = PythonOperator( task_id=\"print_the_context\", python_callable=print_context )",
    "option": {}
  },
  "ERA001": {
    "description": "[Status: stable] [Name: commented-out-code] [Message: Found commented-out code] What it does Checks for commented-out Python code. Why is this bad? Commented-out code is dead code, and is often included inadvertently. It should be removed. Known problems Prone to false positives when checking comments that resemble Python code, but are not actually Python code (#4845). Example # print(\"Hello, world!\")",
    "option": {
      "lint.task-tags": {
        "option_description": "A list of task tags to recognize (e.g., \"TODO\", \"FIXME\", \"XXX\").\n\nComments starting with these tags will be ignored by commented-out code\ndetection (ERA), and skipped by line-length rules (E501) if\nignore-overlong-task-comments is set to true.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\ntask-tags = [\"HACK\"]\n\n\n\n[lint]\ntask-tags = [\"HACK\"]",
        "option_data_type": "list[str]",
        "default_value": "[\"TODO\", \"FIXME\", \"XXX\"]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "FAST001": {
    "description": "[Status: stable] [Name: fast-api-redundant-response-model] [Message: FastAPI route with redundant <code>response_model</code> argument] What it does Checks for FastAPI routes that use the optional response_model parameter with the same type as the return type. Why is this bad? FastAPI routes automatically infer the response model type from the return type, so specifying it explicitly is redundant. The response_model parameter is used to override the default response model type. For example, response_model can be used to specify that a non-serializable response type should instead be serialized via an alternative type. For more information, see the FastAPI documentation. Example from fastapi import FastAPI from pydantic import BaseModel app = FastAPI() class Item(BaseModel): name: str @app.post(\"/items/\", response_model=Item) async def create_item(item: Item) -> Item: return item Use instead: from fastapi import FastAPI from pydantic import BaseModel app = FastAPI() class Item(BaseModel): name: str @app.post(\"/items/\") async def create_item(item: Item) -> Item: return item Fix safety This fix is always unsafe, as removing the response_model argument can change runtime behavior and API documentation generation. Additionally, comments inside the decorator might be removed when the argument is deleted.",
    "option": {}
  },
  "FAST002": {
    "description": "[Status: stable] [Name: fast-api-non-annotated-dependency] [Message: FastAPI dependency without <code>Annotated</code>] What it does Identifies FastAPI routes with deprecated uses of Depends or similar. Why is this bad? The FastAPI documentation recommends the use of typing.Annotated for defining route dependencies and parameters, rather than using Depends, Query or similar as a default value for a parameter. Using this approach everywhere helps ensure consistency and clarity in defining dependencies and parameters. Annotated was added to the typing module in Python 3.9; however, the third-party typing_extensions package provides a backport that can be used on older versions of Python. Example from fastapi import Depends, FastAPI app = FastAPI() async def common_parameters(q: str | None = None, skip: int = 0, limit: int = 100): return {\"q\": q, \"skip\": skip, \"limit\": limit} @app.get(\"/items/\") async def read_items(commons: dict = Depends(common_parameters)): return commons Use instead: from typing import Annotated from fastapi import Depends, FastAPI app = FastAPI() async def common_parameters(q: str | None = None, skip: int = 0, limit: int = 100): return {\"q\": q, \"skip\": skip, \"limit\": limit} @app.get(\"/items/\") async def read_items(commons: Annotated[dict, Depends(common_parameters)]): return commons Fix safety This fix is always unsafe, as adding/removing/changing a function parameter's default value can change runtime behavior. Additionally, comments inside the deprecated uses might be removed. Availability Because this rule relies on the third-party typing_extensions module for Python versions before 3.9, if the target version is < 3.9 and typing_extensions imports have been disabled by the lint.typing-extensions linter option the diagnostic will not be emitted and no fix will be offered.",
    "option": {
      "lint.typing-extensions": {
        "option_description": "Whether to allow imports from the third-party typing_extensions module for Python versions\nbefore a symbol was added to the first-party typing module.\n\nMany rules try to import symbols from the typing module but fall back to\ntyping_extensions for earlier versions of Python. This option can be used to disable this\nfallback behavior in cases where typing_extensions is not installed.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\n# Disable `typing_extensions` imports\ntyping-extensions = false\n\n\n\n[lint]\n# Disable `typing_extensions` imports\ntyping-extensions = false",
        "option_data_type": "bool",
        "default_value": "true",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "FAST003": {
    "description": "[Status: stable] [Name: fast-api-unused-path-parameter] [Message: Parameter <code>{arg_name}</code> appears in route path, but not in <code>{function_name}</code> signature] What it does Identifies FastAPI routes that declare path parameters in the route path that are not included in the function signature. Why is this bad? Path parameters are used to extract values from the URL path. If a path parameter is declared in the route path but not in the function signature, it will not be accessible in the function body, which is likely a mistake. If a path parameter is declared in the route path, but as a positional-only argument in the function signature, it will also not be accessible in the function body, as FastAPI will not inject the parameter. Known problems If the path parameter is not a valid Python identifier (e.g., user-id, as opposed to user_id), FastAPI will normalize it. However, this rule simply ignores such path parameters, as FastAPI's normalization behavior is undocumented. Example from fastapi import FastAPI app = FastAPI() @app.get(\"/things/{thing_id}\") async def read_thing(query: str): ... Use instead: from fastapi import FastAPI app = FastAPI() @app.get(\"/things/{thing_id}\") async def read_thing(thing_id: int, query: str): ... Fix safety This rule's fix is marked as unsafe, as modifying a function signature can change the behavior of the code.",
    "option": {}
  },
  "YTT101": {
    "description": "[Status: stable] [Name: sys-version-slice3] [Message: <code>sys.version[:3]</code> referenced (python3.10), use <code>sys.version_info</code>] What it does Checks for uses of sys.version[:3]. Why is this bad? If the current major or minor version consists of multiple digits, sys.version[:3] will truncate the version number (e.g., \"3.10\" would become \"3.1\"). This is likely unintended, and can lead to subtle bugs if the version string is used to test against a specific Python version. Instead, use sys.version_info to access the current major and minor version numbers as a tuple, which can be compared to other tuples without issue. Example import sys sys.version[:3] # Evaluates to \"3.1\" on Python 3.10. Use instead: import sys sys.version_info[:2] # Evaluates to (3, 10) on Python 3.10.",
    "option": {}
  },
  "YTT102": {
    "description": "[Status: stable] [Name: sys-version2] [Message: <code>sys.version[2]</code> referenced (python3.10), use <code>sys.version_info</code>] What it does Checks for uses of sys.version[2]. Why is this bad? If the current major or minor version consists of multiple digits, sys.version[2] will select the first digit of the minor number only (e.g., \"3.10\" would evaluate to \"1\"). This is likely unintended, and can lead to subtle bugs if the version is used to test against a minor version number. Instead, use sys.version_info.minor to access the current minor version number. Example import sys sys.version[2] # Evaluates to \"1\" on Python 3.10. Use instead: import sys f\"{sys.version_info.minor}\" # Evaluates to \"10\" on Python 3.10.",
    "option": {}
  },
  "YTT103": {
    "description": "[Status: stable] [Name: sys-version-cmp-str3] [Message: <code>sys.version</code> compared to string (python3.10), use <code>sys.version_info</code>] What it does Checks for comparisons that test sys.version against string literals, such that the comparison will evaluate to False on Python 3.10 or later. Why is this bad? Comparing sys.version to a string is error-prone and may cause subtle bugs, as the comparison will be performed lexicographically, not semantically. For example, sys.version > \"3.9\" will evaluate to False when using Python 3.10, as \"3.10\" is lexicographically \"less\" than \"3.9\". Instead, use sys.version_info to access the current major and minor version numbers as a tuple, which can be compared to other tuples without issue. Example import sys sys.version > \"3.9\" # `False` on Python 3.10. Use instead: import sys sys.version_info > (3, 9) # `True` on Python 3.10.",
    "option": {}
  },
  "YTT201": {
    "description": "[Status: stable] [Name: sys-version-info0-eq3] [Message: <code>sys.version_info[0] == 3</code> referenced (python4), use <code>&gt;=</code>] What it does Checks for equality comparisons against the major version returned by sys.version_info (e.g., sys.version_info[0] == 3 or sys.version_info[0] != 3). Why is this bad? Using sys.version_info[0] == 3 to verify that the major version is Python 3 or greater will fail if the major version number is ever incremented (e.g., to Python 4). This is likely unintended, as code that uses this comparison is likely intended to be run on Python 2, but would now run on Python 4 too. Similarly, using sys.version_info[0] != 3 to check for Python 2 will also fail if the major version number is incremented. Instead, use >= to check if the major version number is 3 or greater, or < to check if the major version number is less than 3, to future-proof the code. Example import sys if sys.version_info[0] == 3: ... else: print(\"Python 2\") # This will be printed on Python 4. Use instead: import sys if sys.version_info >= (3,): ... else: print(\"Python 2\") # This will not be printed on Python 4.",
    "option": {}
  },
  "YTT202": {
    "description": "[Status: stable] [Name: six-py3] [Message: <code>six.PY3</code> referenced (python4), use <code>not six.PY2</code>] What it does Checks for uses of six.PY3. Why is this bad? six.PY3 will evaluate to False on Python 4 and greater. This is likely unintended, and may cause code intended to run on Python 2 to run on Python 4 too. Instead, use not six.PY2 to validate that the current Python major version is not equal to 2, to future-proof the code. Example import six six.PY3 # `False` on Python 4. Use instead: import six not six.PY2 # `True` on Python 4.",
    "option": {}
  },
  "YTT203": {
    "description": "[Status: stable] [Name: sys-version-info1-cmp-int] [Message: <code>sys.version_info[1]</code> compared to integer (python4), compare <code>sys.version_info</code> to tuple] What it does Checks for comparisons that test sys.version_info[1] against an integer. Why is this bad? Comparisons based on the current minor version number alone can cause subtle bugs and would likely lead to unintended effects if the Python major version number were ever incremented (e.g., to Python 4). Instead, compare sys.version_info to a tuple, including the major and minor version numbers, to future-proof the code. Example import sys if sys.version_info[1] < 7: print(\"Python 3.6 or earlier.\") # This will be printed on Python 4.0. Use instead: import sys if sys.version_info < (3, 7): print(\"Python 3.6 or earlier.\")",
    "option": {}
  },
  "YTT204": {
    "description": "[Status: stable] [Name: sys-version-info-minor-cmp-int] [Message: <code>sys.version_info.minor</code> compared to integer (python4), compare <code>sys.version_info</code> to tuple] What it does Checks for comparisons that test sys.version_info.minor against an integer. Why is this bad? Comparisons based on the current minor version number alone can cause subtle bugs and would likely lead to unintended effects if the Python major version number were ever incremented (e.g., to Python 4). Instead, compare sys.version_info to a tuple, including the major and minor version numbers, to future-proof the code. Example import sys if sys.version_info.minor < 7: print(\"Python 3.6 or earlier.\") # This will be printed on Python 4.0. Use instead: import sys if sys.version_info < (3, 7): print(\"Python 3.6 or earlier.\")",
    "option": {}
  },
  "YTT301": {
    "description": "[Status: stable] [Name: sys-version0] [Message: <code>sys.version[0]</code> referenced (python10), use <code>sys.version_info</code>] What it does Checks for uses of sys.version[0]. Why is this bad? If the current major or minor version consists of multiple digits, sys.version[0] will select the first digit of the major version number only (e.g., \"10.2\" would evaluate to \"1\"). This is likely unintended, and can lead to subtle bugs if the version string is used to test against a major version number. Instead, use sys.version_info.major to access the current major version number. Example import sys sys.version[0] # If using Python 10, this evaluates to \"1\". Use instead: import sys f\"{sys.version_info.major}\" # If using Python 10, this evaluates to \"10\".",
    "option": {}
  },
  "YTT302": {
    "description": "[Status: stable] [Name: sys-version-cmp-str10] [Message: <code>sys.version</code> compared to string (python10), use <code>sys.version_info</code>] What it does Checks for comparisons that test sys.version against string literals, such that the comparison would fail if the major version number were ever incremented to Python 10 or higher. Why is this bad? Comparing sys.version to a string is error-prone and may cause subtle bugs, as the comparison will be performed lexicographically, not semantically. Instead, use sys.version_info to access the current major and minor version numbers as a tuple, which can be compared to other tuples without issue. Example import sys sys.version >= \"3\" # `False` on Python 10. Use instead: import sys sys.version_info >= (3,) # `True` on Python 10.",
    "option": {}
  },
  "YTT303": {
    "description": "[Status: stable] [Name: sys-version-slice1] [Message: <code>sys.version[:1]</code> referenced (python10), use <code>sys.version_info</code>] What it does Checks for uses of sys.version[:1]. Why is this bad? If the major version number consists of more than one digit, this will select the first digit of the major version number only (e.g., \"10.0\" would evaluate to \"1\"). This is likely unintended, and can lead to subtle bugs in future versions of Python if the version string is used to test against a specific major version number. Instead, use sys.version_info.major to access the current major version number. Example import sys sys.version[:1] # If using Python 10, this evaluates to \"1\". Use instead: import sys f\"{sys.version_info.major}\" # If using Python 10, this evaluates to \"10\".",
    "option": {}
  },
  "ANN001": {
    "description": "[Status: stable] [Name: missing-type-function-argument] [Message: Missing type annotation for function argument <code>{name}</code>] What it does Checks that function arguments have type annotations. Why is this bad? Type annotations are a good way to document the types of function arguments. They also help catch bugs, when used alongside a type checker, by ensuring that the types of any provided arguments match expectation. Example def foo(x): ... Use instead: def foo(x: int): ...",
    "option": {
      "lint.flake8-annotations.suppress-dummy-args": {
        "option_description": "Whether to suppress ANN000-level violations for arguments matching the\n\"dummy\" variable regex (like _).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-annotations]\nsuppress-dummy-args = true\n\n\n\n[lint.flake8-annotations]\nsuppress-dummy-args = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "ANN002": {
    "description": "[Status: stable] [Name: missing-type-args] [Message: Missing type annotation for <code>*{name}</code>] What it does Checks that function *args arguments have type annotations. Why is this bad? Type annotations are a good way to document the types of function arguments. They also help catch bugs, when used alongside a type checker, by ensuring that the types of any provided arguments match expectation. Example def foo(*args): ... Use instead: def foo(*args: int): ...",
    "option": {
      "lint.flake8-annotations.suppress-dummy-args": {
        "option_description": "Whether to suppress ANN000-level violations for arguments matching the\n\"dummy\" variable regex (like _).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-annotations]\nsuppress-dummy-args = true\n\n\n\n[lint.flake8-annotations]\nsuppress-dummy-args = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "ANN003": {
    "description": "[Status: stable] [Name: missing-type-kwargs] [Message: Missing type annotation for <code>**{name}</code>] What it does Checks that function **kwargs arguments have type annotations. Why is this bad? Type annotations are a good way to document the types of function arguments. They also help catch bugs, when used alongside a type checker, by ensuring that the types of any provided arguments match expectation. Example def foo(**kwargs): ... Use instead: def foo(**kwargs: int): ...",
    "option": {
      "lint.flake8-annotations.suppress-dummy-args": {
        "option_description": "Whether to suppress ANN000-level violations for arguments matching the\n\"dummy\" variable regex (like _).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-annotations]\nsuppress-dummy-args = true\n\n\n\n[lint.flake8-annotations]\nsuppress-dummy-args = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "ANN201": {
    "description": "[Status: stable] [Name: missing-return-type-undocumented-public-function] [Message: Missing return type annotation for public function <code>{name}</code>] What it does Checks that public functions and methods have return type annotations. Why is this bad? Type annotations are a good way to document the return types of functions. They also help catch bugs, when used alongside a type checker, by ensuring that the types of any returned values, and the types expected by callers, match expectation. Example def add(a, b): return a + b Use instead: def add(a: int, b: int) -> int: return a + b Availability Because this rule relies on the third-party typing_extensions module for some Python versions, its diagnostic will not be emitted, and no fix will be offered, if typing_extensions imports have been disabled by the lint.typing-extensions linter option.",
    "option": {
      "lint.typing-extensions": {
        "option_description": "Whether to allow imports from the third-party typing_extensions module for Python versions\nbefore a symbol was added to the first-party typing module.\n\nMany rules try to import symbols from the typing module but fall back to\ntyping_extensions for earlier versions of Python. This option can be used to disable this\nfallback behavior in cases where typing_extensions is not installed.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\n# Disable `typing_extensions` imports\ntyping-extensions = false\n\n\n\n[lint]\n# Disable `typing_extensions` imports\ntyping-extensions = false",
        "option_data_type": "bool",
        "default_value": "true",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "ANN202": {
    "description": "[Status: stable] [Name: missing-return-type-private-function] [Message: Missing return type annotation for private function <code>{name}</code>] What it does Checks that private functions and methods have return type annotations. Why is this bad? Type annotations are a good way to document the return types of functions. They also help catch bugs, when used alongside a type checker, by ensuring that the types of any returned values, and the types expected by callers, match expectation. Example def _add(a, b): return a + b Use instead: def _add(a: int, b: int) -> int: return a + b Availability Because this rule relies on the third-party typing_extensions module for some Python versions, its diagnostic will not be emitted, and no fix will be offered, if typing_extensions imports have been disabled by the lint.typing-extensions linter option.",
    "option": {
      "lint.typing-extensions": {
        "option_description": "Whether to allow imports from the third-party typing_extensions module for Python versions\nbefore a symbol was added to the first-party typing module.\n\nMany rules try to import symbols from the typing module but fall back to\ntyping_extensions for earlier versions of Python. This option can be used to disable this\nfallback behavior in cases where typing_extensions is not installed.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\n# Disable `typing_extensions` imports\ntyping-extensions = false\n\n\n\n[lint]\n# Disable `typing_extensions` imports\ntyping-extensions = false",
        "option_data_type": "bool",
        "default_value": "true",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "ANN204": {
    "description": "[Status: stable] [Name: missing-return-type-special-method] [Message: Missing return type annotation for special method <code>{name}</code>] What it does Checks that \"special\" methods, like __init__, __new__, and __call__, have return type annotations. Why is this bad? Type annotations are a good way to document the return types of functions. They also help catch bugs, when used alongside a type checker, by ensuring that the types of any returned values, and the types expected by callers, match expectation. Note that type checkers often allow you to omit the return type annotation for __init__ methods, as long as at least one argument has a type annotation. To opt in to this behavior, use the mypy-init-return setting in your pyproject.toml or ruff.toml file: [tool.ruff.lint.flake8-annotations] mypy-init-return = true Example class Foo: def __init__(self, x: int): self.x = x Use instead: class Foo: def __init__(self, x: int) -> None: self.x = x",
    "option": {
      "lint.flake8-annotations.mypy-init-return": {
        "option_description": "Whether to allow the omission of a return type hint for __init__ if at\nleast one argument is annotated.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-annotations]\nmypy-init-return = true\n\n\n\n[lint.flake8-annotations]\nmypy-init-return = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "ANN205": {
    "description": "[Status: stable] [Name: missing-return-type-static-method] [Message: Missing return type annotation for staticmethod <code>{name}</code>] What it does Checks that static methods have return type annotations. Why is this bad? Type annotations are a good way to document the return types of functions. They also help catch bugs, when used alongside a type checker, by ensuring that the types of any returned values, and the types expected by callers, match expectation. Example class Foo: @staticmethod def bar(): return 1 Use instead: class Foo: @staticmethod def bar() -> int: return 1",
    "option": {
      "lint.flake8-annotations.suppress-none-returning": {
        "option_description": "Whether to suppress ANN200-level violations for functions that meet\neither of the following criteria:\n\nContain no return statement.\nExplicit return statement(s) all return None (explicitly or\n  implicitly).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-annotations]\nsuppress-none-returning = true\n\n\n\n[lint.flake8-annotations]\nsuppress-none-returning = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "ANN206": {
    "description": "[Status: stable] [Name: missing-return-type-class-method] [Message: Missing return type annotation for classmethod <code>{name}</code>] What it does Checks that class methods have return type annotations. Why is this bad? Type annotations are a good way to document the return types of functions. They also help catch bugs, when used alongside a type checker, by ensuring that the types of any returned values, and the types expected by callers, match expectation. Example class Foo: @classmethod def bar(cls): return 1 Use instead: class Foo: @classmethod def bar(cls) -> int: return 1",
    "option": {
      "lint.flake8-annotations.suppress-none-returning": {
        "option_description": "Whether to suppress ANN200-level violations for functions that meet\neither of the following criteria:\n\nContain no return statement.\nExplicit return statement(s) all return None (explicitly or\n  implicitly).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-annotations]\nsuppress-none-returning = true\n\n\n\n[lint.flake8-annotations]\nsuppress-none-returning = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "ANN401": {
    "description": "[Status: stable] [Name: any-type] [Message: Dynamically typed expressions (typing.Any) are disallowed in <code>{name}</code>] What it does Checks that function arguments are annotated with a more specific type than Any. Why is this bad? Any is a special type indicating an unconstrained type. When an expression is annotated with type Any, type checkers will allow all operations on it. It's better to be explicit about the type of an expression, and to use Any as an \"escape hatch\" only when it is really needed. Example from typing import Any def foo(x: Any): ... Use instead: def foo(x: int): ... Known problems Type aliases are unsupported and can lead to false positives. For example, the following will trigger this rule inadvertently: from typing import Any MyAny = Any def foo(x: MyAny): ...",
    "option": {
      "lint.flake8-annotations.allow-star-arg-any": {
        "option_description": "Whether to suppress ANN401 for dynamically typed *args and\n**kwargs arguments.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-annotations]\nallow-star-arg-any = true\n\n\n\n[lint.flake8-annotations]\nallow-star-arg-any = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "ASYNC100": {
    "description": "[Status: stable] [Name: cancel-scope-no-checkpoint] [Message: A <code>with {method_name}(...):</code> context does not contain any <code>await</code> statements. This makes it pointless, as the timeout can only be triggered by a checkpoint.] What it does Checks for timeout context managers which do not contain a checkpoint. For the purposes of this check, yield is considered a checkpoint, since checkpoints may occur in the caller to which we yield. Why is this bad? Some asynchronous context managers, such as asyncio.timeout and trio.move_on_after, have no effect unless they contain a checkpoint. The use of such context managers without an await, async with or async for statement is likely a mistake. Example import asyncio async def func(): async with asyncio.timeout(2): do_something() Use instead: import asyncio async def func(): async with asyncio.timeout(2): do_something() await awaitable()",
    "option": {}
  },
  "ASYNC105": {
    "description": "[Status: stable] [Name: trio-sync-call] [Message: Call to <code>{method_name}</code> is not immediately awaited] What it does Checks for calls to trio functions that are not immediately awaited. Why is this bad? Many of the functions exposed by trio are asynchronous, and must be awaited to take effect. Calling a trio function without an await can lead to RuntimeWarning diagnostics and unexpected behaviour. Example import trio async def double_sleep(x): trio.sleep(2 * x) Use instead: import trio async def double_sleep(x): await trio.sleep(2 * x) Fix safety This rule's fix is marked as unsafe, as adding an await to a function call changes its semantics and runtime behavior.",
    "option": {}
  },
  "ASYNC109": {
    "description": "[Status: stable] [Name: async-function-with-timeout] [Message: Async function definition with a <code>timeout</code> parameter] What it does Checks for async function definitions with timeout parameters. Why is this bad? Rather than implementing asynchronous timeout behavior manually, prefer built-in timeout functionality, such as asyncio.timeout, trio.fail_after, or anyio.move_on_after, among others. This rule is highly opinionated to enforce a design pattern called \"structured concurrency\" that allows for async functions to be oblivious to timeouts, instead letting callers to handle the logic with a context manager. Details This rule attempts to detect which async framework your code is using by analysing the imports in the file it's checking. If it sees an anyio import in your code, it will assume anyio is your framework of choice; if it sees a trio import, it will assume trio; if it sees neither, it will assume asyncio. asyncio.timeout was added in Python 3.11, so if asyncio is detected as the framework being used, this rule will be ignored when your configured target-version is set to less than Python 3.11. For functions that wrap asyncio.timeout, trio.fail_after or anyio.move_on_after, false positives from this rule can be avoided by using a different parameter name. Example async def long_running_task(timeout): ... async def main(): await long_running_task(timeout=2) Use instead: async def long_running_task(): ... async def main(): async with asyncio.timeout(2): await long_running_task()",
    "option": {}
  },
  "ASYNC110": {
    "description": "[Status: stable] [Name: async-busy-wait] [Message: Use <code>{module}.Event</code> instead of awaiting <code>{module}.sleep</code> in a <code>while</code> loop] What it does Checks for the use of an async sleep function in a while loop. Why is this bad? Busy-waiting for a condition in a loop forces a tradeoff between efficiency and latency: shorter sleep times improve responsiveness but waste more CPU cycles, while longer sleep times reduce CPU usage but delay reaction to state changes. Waiting on an Event object like asyncio.Event, trio.Event, or anyio.Event eliminates this tradeoff, allowing immediate response with minimal wasted CPU time. Example import asyncio DONE = False async def func(): while not DONE: await asyncio.sleep(1) Use instead: import asyncio DONE = asyncio.Event() async def func(): await DONE.wait()",
    "option": {}
  },
  "ASYNC115": {
    "description": "[Status: stable] [Name: async-zero-sleep] [Message: Use <code>{module}.lowlevel.checkpoint()</code> instead of <code>{module}.sleep(0)</code>] What it does Checks for uses of trio.sleep(0) or anyio.sleep(0). Why is this bad? trio.sleep(0) is equivalent to calling trio.lowlevel.checkpoint(). However, the latter better conveys the intent of the code. Example import trio async def func(): await trio.sleep(0) Use instead: import trio async def func(): await trio.lowlevel.checkpoint() Fix safety This rule's fix is marked as unsafe if there's comments in the trio.sleep(0) expression, as comments may be removed. For example, the fix would be marked as unsafe in the following case: import trio async def func(): await trio.sleep( # comment # comment 0 )",
    "option": {}
  },
  "ASYNC116": {
    "description": "[Status: stable] [Name: long-sleep-not-forever] [Message: <code>{module}.sleep()</code> with >24 hour interval should usually be <code>{module}.sleep_forever()</code>] What it does Checks for uses of trio.sleep() or anyio.sleep() with a delay greater than 24 hours. Why is this bad? Calling sleep() with a delay greater than 24 hours is usually intended to sleep indefinitely. Instead of using a large delay, trio.sleep_forever() or anyio.sleep_forever() better conveys the intent. Example import trio async def func(): await trio.sleep(86401) Use instead: import trio async def func(): await trio.sleep_forever() Fix safety This fix is marked as unsafe as it changes program behavior.",
    "option": {}
  },
  "ASYNC210": {
    "description": "[Status: stable] [Name: blocking-http-call-in-async-function] [Message: Async functions should not call blocking HTTP methods] What it does Checks that async functions do not contain blocking HTTP calls. Why is this bad? Blocking an async function via a blocking HTTP call will block the entire event loop, preventing it from executing other tasks while waiting for the HTTP response, negating the benefits of asynchronous programming. Instead of making a blocking HTTP call, use an asynchronous HTTP client library such as aiohttp or httpx. Example import urllib async def fetch(): urllib.request.urlopen(\"https://example.com/foo/bar\").read() Use instead: import aiohttp async def fetch(): async with aiohttp.ClientSession() as session: async with session.get(\"https://example.com/foo/bar\") as resp: ...",
    "option": {}
  },
  "ASYNC212": {
    "description": "[Status: preview] [Name: blocking-http-call-httpx-in-async-function] [Message: Blocking httpx method {name}.{call}() in async context, use httpx.AsyncClient] What it does Checks that async functions do not use blocking httpx clients. Why is this bad? Blocking an async function via a blocking HTTP call will block the entire event loop, preventing it from executing other tasks while waiting for the HTTP response, negating the benefits of asynchronous programming. Instead of using the blocking httpx client, use the asynchronous client. Example import httpx async def fetch(): client = httpx.Client() response = client.get(...) Use instead: import httpx async def fetch(): async with httpx.AsyncClient() as client: response = await client.get(...)",
    "option": {}
  },
  "ASYNC220": {
    "description": "[Status: stable] [Name: create-subprocess-in-async-function] [Message: Async functions should not create subprocesses with blocking methods] What it does Checks that async functions do not create subprocesses with blocking methods. Why is this bad? Blocking an async function via a blocking call will block the entire event loop, preventing it from executing other tasks while waiting for the call to complete, negating the benefits of asynchronous programming. Instead of making a blocking call, use an equivalent asynchronous library or function, like trio.run_process() or anyio.run_process(). Example import os async def foo(): os.popen(cmd) Use instead: import asyncio async def foo(): asyncio.create_subprocess_shell(cmd)",
    "option": {}
  },
  "ASYNC221": {
    "description": "[Status: stable] [Name: run-process-in-async-function] [Message: Async functions should not run processes with blocking methods] What it does Checks that async functions do not run processes with blocking methods. Why is this bad? Blocking an async function via a blocking call will block the entire event loop, preventing it from executing other tasks while waiting for the call to complete, negating the benefits of asynchronous programming. Instead of making a blocking call, use an equivalent asynchronous library or function, like trio.run_process() or anyio.run_process(). Example import subprocess async def foo(): subprocess.run(cmd) Use instead: import asyncio async def foo(): asyncio.create_subprocess_shell(cmd)",
    "option": {}
  },
  "ASYNC222": {
    "description": "[Status: stable] [Name: wait-for-process-in-async-function] [Message: Async functions should not wait on processes with blocking methods] What it does Checks that async functions do not wait on processes with blocking methods. Why is this bad? Blocking an async function via a blocking call will block the entire event loop, preventing it from executing other tasks while waiting for the call to complete, negating the benefits of asynchronous programming. Instead of making a blocking call, use an equivalent asynchronous library or function, like trio.to_thread.run_sync() or anyio.to_thread.run_sync(). Example import os async def foo(): os.waitpid(0) Use instead: import asyncio import os def wait_for_process(): os.waitpid(0) async def foo(): await asyncio.loop.run_in_executor(None, wait_for_process)",
    "option": {}
  },
  "ASYNC230": {
    "description": "[Status: stable] [Name: blocking-open-call-in-async-function] [Message: Async functions should not open files with blocking methods like <code>open</code>] What it does Checks that async functions do not open files with blocking methods like open. Why is this bad? Blocking an async function via a blocking call will block the entire event loop, preventing it from executing other tasks while waiting for the call to complete, negating the benefits of asynchronous programming. Instead of making a blocking call, use an equivalent asynchronous library or function. Example async def foo(): with open(\"bar.txt\") as f: contents = f.read() Use instead: import anyio async def foo(): async with await anyio.open_file(\"bar.txt\") as f: contents = await f.read()",
    "option": {}
  },
  "ASYNC240": {
    "description": "[Status: preview] [Name: blocking-path-method-in-async-function] [Message: Async functions should not use {path_library} methods, use trio.Path or anyio.path] What it does Checks that async functions do not call blocking os.path or pathlib.Path methods. Why is this bad? Calling some os.path or pathlib.Path methods in an async function will block the entire event loop, preventing it from executing other tasks while waiting for the operation. This negates the benefits of asynchronous programming. Instead, use the methods' async equivalents from trio.Path or anyio.Path. Example import os async def func(): path = \"my_file.txt\" file_exists = os.path.exists(path) Use instead: import trio async def func(): path = trio.Path(\"my_file.txt\") file_exists = await path.exists() Non-blocking methods are OK to use: import pathlib async def func(): path = pathlib.Path(\"my_file.txt\") file_dirname = path.dirname() new_path = os.path.join(\"/tmp/src/\", path)",
    "option": {}
  },
  "ASYNC250": {
    "description": "[Status: preview] [Name: blocking-input-in-async-function] [Message: Blocking call to input() in async context] What it does Checks that async functions do not contain blocking usage of input from user. Why is this bad? Blocking an async function via a blocking input call will block the entire event loop, preventing it from executing other tasks while waiting for user input, negating the benefits of asynchronous programming. Instead of making a blocking input call directly, wrap the input call in an executor to execute the blocking call on another thread. Example async def foo(): username = input(\"Username:\") Use instead: import asyncio async def foo(): loop = asyncio.get_running_loop() username = await loop.run_in_executor(None, input, \"Username:\")",
    "option": {}
  },
  "ASYNC251": {
    "description": "[Status: stable] [Name: blocking-sleep-in-async-function] [Message: Async functions should not call <code>time.sleep</code>] What it does Checks that async functions do not call time.sleep. Why is this bad? Blocking an async function via a time.sleep call will block the entire event loop, preventing it from executing other tasks while waiting for the time.sleep, negating the benefits of asynchronous programming. Instead of time.sleep, use asyncio.sleep. Example import time async def fetch(): time.sleep(1) Use instead: import asyncio async def fetch(): await asyncio.sleep(1)",
    "option": {}
  },
  "S101": {
    "description": "[Status: stable] [Name: assert] [Message: Use of <code>assert</code> detected] What it does Checks for uses of the assert keyword. Why is this bad? Assertions are removed when Python is run with optimization requested (i.e., when the -O flag is present), which is a common practice in production environments. As such, assertions should not be used for runtime validation of user input or to enforce interface constraints. Consider raising a meaningful error instead of using assert. Example assert x > 0, \"Expected positive value.\" Use instead: if not x > 0: raise ValueError(\"Expected positive value.\") # or even better: if x <= 0: raise ValueError(\"Expected positive value.\")",
    "option": {}
  },
  "S102": {
    "description": "[Status: stable] [Name: exec-builtin] [Message: Use of <code>exec</code> detected] What it does Checks for uses of the builtin exec function. Why is this bad? The exec() function is insecure as it allows for arbitrary code execution. Example exec(\"print('Hello World')\")",
    "option": {}
  },
  "S103": {
    "description": "[Status: stable] [Name: bad-file-permissions] [Message: <code>os.chmod</code> setting a permissive mask <code>{mask:#o}</code> on file or directory] What it does Checks for files with overly permissive permissions. Why is this bad? Overly permissive file permissions may allow unintended access and arbitrary code execution. Example import os os.chmod(\"/etc/secrets.txt\", 0o666) # rw-rw-rw- Use instead: import os os.chmod(\"/etc/secrets.txt\", 0o600) # rw-------",
    "option": {}
  },
  "S104": {
    "description": "[Status: stable] [Name: hardcoded-bind-all-interfaces] [Message: Possible binding to all interfaces] What it does Checks for hardcoded bindings to all network interfaces (0.0.0.0). Why is this bad? Binding to all network interfaces is insecure as it allows access from unintended interfaces, which may be poorly secured or unauthorized. Instead, bind to specific interfaces. Example ALLOWED_HOSTS = [\"0.0.0.0\"] Use instead: ALLOWED_HOSTS = [\"127.0.0.1\", \"localhost\"]",
    "option": {}
  },
  "S105": {
    "description": "[Status: stable] [Name: hardcoded-password-string] [Message: Possible hardcoded password assigned to: \"{}\"] What it does Checks for potential uses of hardcoded passwords in strings. Why is this bad? Including a hardcoded password in source code is a security risk, as an attacker could discover the password and use it to gain unauthorized access. Instead, store passwords and other secrets in configuration files, environment variables, or other sources that are excluded from version control. Example SECRET_KEY = \"hunter2\" Use instead: import os SECRET_KEY = os.environ[\"SECRET_KEY\"]",
    "option": {}
  },
  "S106": {
    "description": "[Status: stable] [Name: hardcoded-password-func-arg] [Message: Possible hardcoded password assigned to argument: \"{}\"] What it does Checks for potential uses of hardcoded passwords in function calls. Why is this bad? Including a hardcoded password in source code is a security risk, as an attacker could discover the password and use it to gain unauthorized access. Instead, store passwords and other secrets in configuration files, environment variables, or other sources that are excluded from version control. Example connect_to_server(password=\"hunter2\") Use instead: import os connect_to_server(password=os.environ[\"PASSWORD\"])",
    "option": {}
  },
  "S107": {
    "description": "[Status: stable] [Name: hardcoded-password-default] [Message: Possible hardcoded password assigned to function default: \"{}\"] What it does Checks for potential uses of hardcoded passwords in function argument defaults. Why is this bad? Including a hardcoded password in source code is a security risk, as an attacker could discover the password and use it to gain unauthorized access. Instead, store passwords and other secrets in configuration files, environment variables, or other sources that are excluded from version control. Example def connect_to_server(password=\"hunter2\"): ... Use instead: import os def connect_to_server(password=os.environ[\"PASSWORD\"]): ...",
    "option": {}
  },
  "S108": {
    "description": "[Status: stable] [Name: hardcoded-temp-file] [Message: Probable insecure usage of temporary file or directory: \"{}\"] What it does Checks for the use of hardcoded temporary file or directory paths. Why is this bad? The use of hardcoded paths for temporary files can be insecure. If an attacker discovers the location of a hardcoded path, they can replace the contents of the file or directory with a malicious payload. Other programs may also read or write contents to these hardcoded paths, causing unexpected behavior. Example with open(\"/tmp/foo.txt\", \"w\") as file: ... Use instead: import tempfile with tempfile.NamedTemporaryFile() as file: ...",
    "option": {
      "lint.flake8-bandit.hardcoded-tmp-directory": {
        "option_description": "A list of directories to consider temporary (see S108).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-bandit]\nhardcoded-tmp-directory = [\"/foo/bar\"]\n\n\n\n[lint.flake8-bandit]\nhardcoded-tmp-directory = [\"/foo/bar\"]",
        "option_data_type": "list[str]",
        "default_value": "[\"/tmp\", \"/var/tmp\", \"/dev/shm\"]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.flake8-bandit.hardcoded-tmp-directory-extend": {
        "option_description": "A list of directories to consider temporary, in addition to those\nspecified by hardcoded-tmp-directory (see S108).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-bandit]\nhardcoded-tmp-directory-extend = [\"/foo/bar\"]\n\n\n\n[lint.flake8-bandit]\nhardcoded-tmp-directory-extend = [\"/foo/bar\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "S110": {
    "description": "[Status: stable] [Name: try-except-pass] [Message: <code>try</code>-<code>except</code>-<code>pass</code> detected, consider logging the exception] What it does Checks for uses of the try-except-pass pattern. Why is this bad? The try-except-pass pattern suppresses all exceptions. Suppressing exceptions may hide errors that could otherwise reveal unexpected behavior, security vulnerabilities, or malicious activity. Instead, consider logging the exception. Example try: ... except Exception: pass Use instead: import logging try: ... except Exception as exc: logging.exception(\"Exception occurred\")",
    "option": {
      "lint.flake8-bandit.check-typed-exception": {
        "option_description": "Whether to disallow try-except-pass (S110) for specific\nexception types. By default, try-except-pass is only\ndisallowed for Exception and BaseException.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-bandit]\ncheck-typed-exception = true\n\n\n\n[lint.flake8-bandit]\ncheck-typed-exception = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "S112": {
    "description": "[Status: stable] [Name: try-except-continue] [Message: <code>try</code>-<code>except</code>-<code>continue</code> detected, consider logging the exception] What it does Checks for uses of the try-except-continue pattern. Why is this bad? The try-except-continue pattern suppresses all exceptions. Suppressing exceptions may hide errors that could otherwise reveal unexpected behavior, security vulnerabilities, or malicious activity. Instead, consider logging the exception. Example import logging while predicate: try: ... except Exception: continue Use instead: import logging while predicate: try: ... except Exception as exc: logging.exception(\"Error occurred\")",
    "option": {
      "lint.flake8-bandit.check-typed-exception": {
        "option_description": "Whether to disallow try-except-pass (S110) for specific\nexception types. By default, try-except-pass is only\ndisallowed for Exception and BaseException.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-bandit]\ncheck-typed-exception = true\n\n\n\n[lint.flake8-bandit]\ncheck-typed-exception = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "S113": {
    "description": "[Status: stable] [Name: request-without-timeout] [Message: Probable use of <code>{module}</code> call without timeout] What it does Checks for uses of the Python requests or httpx module that omit the timeout parameter. Why is this bad? The timeout parameter is used to set the maximum time to wait for a response from the server. By omitting the timeout parameter, the program may hang indefinitely while awaiting a response. Example import requests requests.get(\"https://www.example.com/\") Use instead: import requests requests.get(\"https://www.example.com/\", timeout=10)",
    "option": {}
  },
  "S201": {
    "description": "[Status: stable] [Name: flask-debug-true] [Message: Use of <code>debug=True</code> in Flask app detected] What it does Checks for uses of debug=True in Flask. Why is this bad? Enabling debug mode shows an interactive debugger in the browser if an error occurs, and allows running arbitrary Python code from the browser. This could leak sensitive information, or allow an attacker to run arbitrary code. Example from flask import Flask app = Flask() app.run(debug=True) Use instead: import os from flask import Flask app = Flask() app.run(debug=os.environ[\"ENV\"] == \"dev\")",
    "option": {}
  },
  "S202": {
    "description": "[Status: stable] [Name: tarfile-unsafe-members] [Message: Uses of <code>tarfile.extractall()</code>] What it does Checks for uses of tarfile.extractall. Why is this bad? Extracting archives from untrusted sources without prior inspection is a security risk, as maliciously crafted archives may contain files that will be written outside of the target directory. For example, the archive could include files with absolute paths (e.g., /etc/passwd), or relative paths with parent directory references (e.g., ../etc/passwd). On Python 3.12 and later, use filter='data' to prevent the most dangerous security issues (see: PEP 706). On earlier versions, set the members argument to a trusted subset of the archive's members. Example import tarfile import tempfile tar = tarfile.open(filename) tar.extractall(path=tempfile.mkdtemp()) tar.close()",
    "option": {}
  },
  "S301": {
    "description": "[Status: stable] [Name: suspicious-pickle-usage] [Message: <code>pickle</code> and modules that wrap it can be unsafe when used to deserialize untrusted data, possible security issue] What it does Checks for calls to pickle functions or modules that wrap them. Why is this bad? Deserializing untrusted data with pickle and other deserialization modules is insecure as it can allow for the creation of arbitrary objects, which can then be used to achieve arbitrary code execution and otherwise unexpected behavior. Avoid deserializing untrusted data with pickle and other deserialization modules. Instead, consider safer formats, such as JSON. If you must deserialize untrusted data with pickle, consider signing the data with a secret key and verifying the signature before deserializing the payload, This will prevent an attacker from injecting arbitrary objects into the serialized data. In preview, this rule will also flag references to pickle functions. Example import pickle with open(\"foo.pickle\", \"rb\") as file: foo = pickle.load(file) Use instead: import json with open(\"foo.json\", \"rb\") as file: foo = json.load(file)",
    "option": {}
  },
  "S302": {
    "description": "[Status: stable] [Name: suspicious-marshal-usage] [Message: Deserialization with the <code>marshal</code> module is possibly dangerous] What it does Checks for calls to marshal functions. Why is this bad? Deserializing untrusted data with marshal is insecure, as it can allow for the creation of arbitrary objects, which can then be used to achieve arbitrary code execution and otherwise unexpected behavior. Avoid deserializing untrusted data with marshal. Instead, consider safer formats, such as JSON. If you must deserialize untrusted data with marshal, consider signing the data with a secret key and verifying the signature before deserializing the payload. This will prevent an attacker from injecting arbitrary objects into the serialized data. In preview, this rule will also flag references to marshal functions. Example import marshal with open(\"foo.marshal\", \"rb\") as file: foo = marshal.load(file) Use instead: import json with open(\"foo.json\", \"rb\") as file: foo = json.load(file)",
    "option": {}
  },
  "S303": {
    "description": "[Status: stable] [Name: suspicious-insecure-hash-usage] [Message: Use of insecure MD2, MD4, MD5, or SHA1 hash function] What it does Checks for uses of weak or broken cryptographic hash functions. Why is this bad? Weak or broken cryptographic hash functions may be susceptible to collision attacks (where two different inputs produce the same hash) or pre-image attacks (where an attacker can find an input that produces a given hash). This can lead to security vulnerabilities in applications that rely on these hash functions. Avoid using weak or broken cryptographic hash functions in security contexts. Instead, use a known secure hash function such as SHA-256. In preview, this rule will also flag references to insecure hash functions. Example from cryptography.hazmat.primitives import hashes digest = hashes.Hash(hashes.MD5()) digest.update(b\"Hello, world!\") digest.finalize() Use instead: from cryptography.hazmat.primitives import hashes digest = hashes.Hash(hashes.SHA256()) digest.update(b\"Hello, world!\") digest.finalize()",
    "option": {}
  },
  "S304": {
    "description": "[Status: stable] [Name: suspicious-insecure-cipher-usage] [Message: Use of insecure cipher, replace with a known secure cipher such as AES] What it does Checks for uses of weak or broken cryptographic ciphers. Why is this bad? Weak or broken cryptographic ciphers may be susceptible to attacks that allow an attacker to decrypt ciphertext without knowing the key or otherwise compromise the security of the cipher, such as forgeries. Use strong, modern cryptographic ciphers instead of weak or broken ones. In preview, this rule will also flag references to insecure ciphers. Example from cryptography.hazmat.primitives.ciphers import Cipher, algorithms algorithm = algorithms.ARC4(key) cipher = Cipher(algorithm, mode=None) encryptor = cipher.encryptor() Use instead: from cryptography.fernet import Fernet fernet = Fernet(key)",
    "option": {}
  },
  "S305": {
    "description": "[Status: stable] [Name: suspicious-insecure-cipher-mode-usage] [Message: Use of insecure block cipher mode, replace with a known secure mode such as CBC or CTR] What it does Checks for uses of weak or broken cryptographic cipher modes. Why is this bad? Weak or broken cryptographic ciphers may be susceptible to attacks that allow an attacker to decrypt ciphertext without knowing the key or otherwise compromise the security of the cipher, such as forgeries. Use strong, modern cryptographic ciphers instead of weak or broken ones. In preview, this rule will also flag references to insecure cipher modes. Example from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes algorithm = algorithms.ARC4(key) cipher = Cipher(algorithm, mode=modes.ECB(iv)) encryptor = cipher.encryptor() Use instead: from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes algorithm = algorithms.ARC4(key) cipher = Cipher(algorithm, mode=modes.CTR(iv)) encryptor = cipher.encryptor()",
    "option": {}
  },
  "S306": {
    "description": "[Status: stable] [Name: suspicious-mktemp-usage] [Message: Use of insecure and deprecated function (<code>mktemp</code>)] What it does Checks for uses of tempfile.mktemp. Why is this bad? tempfile.mktemp returns a pathname of a file that does not exist at the time the call is made; then, the caller is responsible for creating the file and subsequently using it. This is insecure because another process could create a file with the same name between the time the function returns and the time the caller creates the file. tempfile.mktemp is deprecated in favor of tempfile.mkstemp which creates the file when it is called. Consider using tempfile.mkstemp instead, either directly or via a context manager such as tempfile.TemporaryFile. In preview, this rule will also flag references to tempfile.mktemp. Example import tempfile tmp_file = tempfile.mktemp() with open(tmp_file, \"w\") as file: file.write(\"Hello, world!\") Use instead: import tempfile with tempfile.TemporaryFile() as file: file.write(\"Hello, world!\")",
    "option": {}
  },
  "S307": {
    "description": "[Status: stable] [Name: suspicious-eval-usage] [Message: Use of possibly insecure function; consider using <code>ast.literal_eval</code>] What it does Checks for uses of the builtin eval() function. Why is this bad? The eval() function is insecure as it enables arbitrary code execution. If you need to evaluate an expression from a string, consider using ast.literal_eval() instead, which will raise an exception if the expression is not a valid Python literal. In preview, this rule will also flag references to eval. Example x = eval(input(\"Enter a number: \")) Use instead: from ast import literal_eval x = literal_eval(input(\"Enter a number: \"))",
    "option": {}
  },
  "S308": {
    "description": "[Status: stable] [Name: suspicious-mark-safe-usage] [Message: Use of <code>mark_safe</code> may expose cross-site scripting vulnerabilities] What it does Checks for uses of calls to django.utils.safestring.mark_safe. Why is this bad? Cross-site scripting (XSS) vulnerabilities allow attackers to execute arbitrary JavaScript. To guard against XSS attacks, Django templates assumes that data is unsafe and automatically escapes malicious strings before rending them. django.utils.safestring.mark_safe marks a string as safe for use in HTML templates, bypassing XSS protection. Its usage can be dangerous if the contents of the string are dynamically generated, because it may allow cross-site scripting attacks if the string is not properly escaped. For dynamically generated strings, consider utilizing django.utils.html.format_html. In preview, this rule will also flag references to django.utils.safestring.mark_safe. Example from django.utils.safestring import mark_safe def render_username(username): return mark_safe(f\"<i>{username}</i>\") # Dangerous if username is user-provided. Use instead: from django.utils.html import format_html def render_username(username): return format_html(\"<i>{}</i>\", username) # username is escaped.",
    "option": {}
  },
  "S310": {
    "description": "[Status: stable] [Name: suspicious-url-open-usage] [Message: Audit URL open for permitted schemes. Allowing use of <code>file:</code> or custom schemes is often unexpected.] What it does Checks for instances where URL open functions are used with unexpected schemes. Why is this bad? Some URL open functions allow the use of file: or custom schemes (for use instead of http: or https:). An attacker may be able to use these schemes to access or modify unauthorized resources, and cause unexpected behavior. To mitigate this risk, audit all uses of URL open functions and ensure that only permitted schemes are used (e.g., allowing http: and https:, and disallowing file: and ftp:). In preview, this rule will also flag references to URL open functions. Example from urllib.request import urlopen url = input(\"Enter a URL: \") with urlopen(url) as response: ... Use instead: from urllib.request import urlopen url = input(\"Enter a URL: \") if not url.startswith((\"http:\", \"https:\")): raise ValueError(\"URL must start with 'http:' or 'https:'\") with urlopen(url) as response: ...",
    "option": {}
  },
  "S311": {
    "description": "[Status: stable] [Name: suspicious-non-cryptographic-random-usage] [Message: Standard pseudo-random generators are not suitable for cryptographic purposes] What it does Checks for uses of cryptographically weak pseudo-random number generators. Why is this bad? Cryptographically weak pseudo-random number generators are insecure, as they are easily predictable. This can allow an attacker to guess the generated numbers and compromise the security of the system. Instead, use a cryptographically secure pseudo-random number generator (such as using the secrets module) when generating random numbers for security purposes. In preview, this rule will also flag references to these generators. Example import random random.randrange(10) Use instead: import secrets secrets.randbelow(10)",
    "option": {}
  },
  "S312": {
    "description": "[Status: stable] [Name: suspicious-telnet-usage] [Message: Telnet is considered insecure. Use SSH or some other encrypted protocol.] What it does Checks for the use of Telnet-related functions. Why is this bad? Telnet is considered insecure because it does not encrypt data sent over the connection and is vulnerable to numerous attacks. Instead, consider using a more secure protocol such as SSH. In preview, this rule will also flag references to Telnet-related functions.",
    "option": {}
  },
  "S313": {
    "description": "[Status: stable] [Name: suspicious-xmlc-element-tree-usage] [Message: Using <code>xml</code> to parse untrusted data is known to be vulnerable to XML attacks; use <code>defusedxml</code> equivalents] What it does Checks for uses of insecure XML parsers. Why is this bad? Many XML parsers are vulnerable to XML attacks (such as entity expansion), which cause excessive memory and CPU usage by exploiting recursion. An attacker could use such methods to access unauthorized resources. Consider using the defusedxml package when parsing untrusted XML data, to protect against XML attacks. In preview, this rule will also flag references to insecure XML parsers. Example from xml.etree.cElementTree import parse tree = parse(\"untrusted.xml\") # Vulnerable to XML attacks. Use instead: from defusedxml.cElementTree import parse tree = parse(\"untrusted.xml\")",
    "option": {}
  },
  "S314": {
    "description": "[Status: stable] [Name: suspicious-xml-element-tree-usage] [Message: Using <code>xml</code> to parse untrusted data is known to be vulnerable to XML attacks; use <code>defusedxml</code> equivalents] What it does Checks for uses of insecure XML parsers. Why is this bad? Many XML parsers are vulnerable to XML attacks (such as entity expansion), which cause excessive memory and CPU usage by exploiting recursion. An attacker could use such methods to access unauthorized resources. Consider using the defusedxml package when parsing untrusted XML data, to protect against XML attacks. In preview, this rule will also flag references to insecure XML parsers. Example from xml.etree.ElementTree import parse tree = parse(\"untrusted.xml\") # Vulnerable to XML attacks. Use instead: from defusedxml.ElementTree import parse tree = parse(\"untrusted.xml\")",
    "option": {}
  },
  "S315": {
    "description": "[Status: stable] [Name: suspicious-xml-expat-reader-usage] [Message: Using <code>xml</code> to parse untrusted data is known to be vulnerable to XML attacks; use <code>defusedxml</code> equivalents] What it does Checks for uses of insecure XML parsers. Why is this bad? Many XML parsers are vulnerable to XML attacks (such as entity expansion), which cause excessive memory and CPU usage by exploiting recursion. An attacker could use such methods to access unauthorized resources. Consider using the defusedxml package when parsing untrusted XML data, to protect against XML attacks. In preview, this rule will also flag references to insecure XML parsers. Example from xml.sax.expatreader import create_parser parser = create_parser() Use instead: from defusedxml.sax import create_parser parser = create_parser()",
    "option": {}
  },
  "S316": {
    "description": "[Status: stable] [Name: suspicious-xml-expat-builder-usage] [Message: Using <code>xml</code> to parse untrusted data is known to be vulnerable to XML attacks; use <code>defusedxml</code> equivalents] What it does Checks for uses of insecure XML parsers. Why is this bad? Many XML parsers are vulnerable to XML attacks (such as entity expansion), which cause excessive memory and CPU usage by exploiting recursion. An attacker could use such methods to access unauthorized resources. Consider using the defusedxml package when parsing untrusted XML data, to protect against XML attacks. In preview, this rule will also flag references to insecure XML parsers. Example from xml.dom.expatbuilder import parse parse(\"untrusted.xml\") Use instead: from defusedxml.expatbuilder import parse tree = parse(\"untrusted.xml\")",
    "option": {}
  },
  "S317": {
    "description": "[Status: stable] [Name: suspicious-xml-sax-usage] [Message: Using <code>xml</code> to parse untrusted data is known to be vulnerable to XML attacks; use <code>defusedxml</code> equivalents] What it does Checks for uses of insecure XML parsers. Why is this bad? Many XML parsers are vulnerable to XML attacks (such as entity expansion), which cause excessive memory and CPU usage by exploiting recursion. An attacker could use such methods to access unauthorized resources. Consider using the defusedxml package when parsing untrusted XML data, to protect against XML attacks. In preview, this rule will also flag references to insecure XML parsers. Example from xml.sax import make_parser make_parser() Use instead: from defusedxml.sax import make_parser make_parser()",
    "option": {}
  },
  "S318": {
    "description": "[Status: stable] [Name: suspicious-xml-mini-dom-usage] [Message: Using <code>xml</code> to parse untrusted data is known to be vulnerable to XML attacks; use <code>defusedxml</code> equivalents] What it does Checks for uses of insecure XML parsers. Why is this bad? Many XML parsers are vulnerable to XML attacks (such as entity expansion), which cause excessive memory and CPU usage by exploiting recursion. An attacker could use such methods to access unauthorized resources. Consider using the defusedxml package when parsing untrusted XML data, to protect against XML attacks. In preview, this rule will also flag references to insecure XML parsers. Example from xml.dom.minidom import parse content = parse(\"untrusted.xml\") Use instead: from defusedxml.minidom import parse content = parse(\"untrusted.xml\")",
    "option": {}
  },
  "S319": {
    "description": "[Status: stable] [Name: suspicious-xml-pull-dom-usage] [Message: Using <code>xml</code> to parse untrusted data is known to be vulnerable to XML attacks; use <code>defusedxml</code> equivalents] What it does Checks for uses of insecure XML parsers. Why is this bad? Many XML parsers are vulnerable to XML attacks (such as entity expansion), which cause excessive memory and CPU usage by exploiting recursion. An attacker could use such methods to access unauthorized resources. Consider using the defusedxml package when parsing untrusted XML data, to protect against XML attacks. In preview, this rule will also flag references to insecure XML parsers. Example from xml.dom.pulldom import parse content = parse(\"untrusted.xml\") Use instead: from defusedxml.pulldom import parse content = parse(\"untrusted.xml\")",
    "option": {}
  },
  "S321": {
    "description": "[Status: stable] [Name: suspicious-ftp-lib-usage] [Message: FTP-related functions are being called. FTP is considered insecure. Use SSH/SFTP/SCP or some other encrypted protocol.] What it does Checks for the use of FTP-related functions. Why is this bad? FTP is considered insecure as it does not encrypt data sent over the connection and is thus vulnerable to numerous attacks. Instead, consider using FTPS (which secures FTP using SSL/TLS) or SFTP. In preview, this rule will also flag references to FTP-related functions.",
    "option": {}
  },
  "S323": {
    "description": "[Status: stable] [Name: suspicious-unverified-context-usage] [Message: Python allows using an insecure context via the <code>_create_unverified_context</code> that reverts to the previous behavior that does not validate certificates or perform hostname checks.] What it does Checks for uses of ssl._create_unverified_context. Why is this bad? PEP 476 enabled certificate and hostname validation by default in Python standard library HTTP clients. Previously, Python did not validate certificates by default, which could allow an attacker to perform a \"man in the middle\" attack by intercepting and modifying traffic between client and server. To support legacy environments, ssl._create_unverified_context reverts to the previous behavior that does perform verification. Otherwise, use ssl.create_default_context to create a secure context. In preview, this rule will also flag references to ssl._create_unverified_context. Example import ssl context = ssl._create_unverified_context() Use instead: import ssl context = ssl.create_default_context()",
    "option": {}
  },
  "S324": {
    "description": "[Status: stable] [Name: hashlib-insecure-hash-function] [Message: Probable use of insecure hash functions in <code>{library}</code>: <code>{string}</code>] What it does Checks for uses of weak or broken cryptographic hash functions in hashlib and crypt libraries. Why is this bad? Weak or broken cryptographic hash functions may be susceptible to collision attacks (where two different inputs produce the same hash) or pre-image attacks (where an attacker can find an input that produces a given hash). This can lead to security vulnerabilities in applications that rely on these hash functions. Avoid using weak or broken cryptographic hash functions in security contexts. Instead, use a known secure hash function such as SHA256. Note: This rule targets the following weak algorithm names in hashlib: md4, md5, sha, and sha1. It also flags uses of crypt.crypt and crypt.mksalt when configured with METHOD_CRYPT, METHOD_MD5, or METHOD_BLOWFISH. It does not attempt to lint OpenSSL- or platform-specific aliases and OIDs (for example: \"sha-1\", \"ssl3-sha1\", \"ssl3-md5\", or \"1.3.14.3.2.26\"), nor variations with trailing spaces, as the set of accepted aliases depends on the underlying OpenSSL version and varies across platforms and Python builds. Example import hashlib def certificate_is_valid(certificate: bytes, known_hash: str) -> bool: hash = hashlib.md5(certificate).hexdigest() return hash == known_hash Use instead: import hashlib def certificate_is_valid(certificate: bytes, known_hash: str) -> bool: hash = hashlib.sha256(certificate).hexdigest() return hash == known_hash or add usedforsecurity=False if the hashing algorithm is not used in a security context, e.g. as a non-cryptographic one-way compression function: import hashlib def certificate_is_valid(certificate: bytes, known_hash: str) -> bool: hash = hashlib.md5(certificate, usedforsecurity=False).hexdigest() return hash == known_hash",
    "option": {}
  },
  "S401": {
    "description": "[Status: preview] [Name: suspicious-telnetlib-import] [Message: <code>telnetlib</code> and related modules are considered insecure. Use SSH or another encrypted protocol.] What it does Checks for imports of the telnetlib module. Why is this bad? Telnet is considered insecure. It is deprecated since version 3.11, and was removed in version 3.13. Instead, use SSH or another encrypted protocol. Example import telnetlib",
    "option": {}
  },
  "S402": {
    "description": "[Status: preview] [Name: suspicious-ftplib-import] [Message: <code>ftplib</code> and related modules are considered insecure. Use SSH, SFTP, SCP, or another encrypted protocol.] What it does Checks for imports of the ftplib module. Why is this bad? FTP is considered insecure. Instead, use SSH, SFTP, SCP, or another encrypted protocol. Example import ftplib",
    "option": {}
  },
  "S403": {
    "description": "[Status: preview] [Name: suspicious-pickle-import] [Message: <code>pickle</code>, <code>cPickle</code>, <code>dill</code>, and <code>shelve</code> modules are possibly insecure] What it does Checks for imports of the pickle, cPickle, dill, and shelve modules. Why is this bad? It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Consider possible security implications associated with these modules. Example import pickle",
    "option": {}
  },
  "S404": {
    "description": "[Status: preview] [Name: suspicious-subprocess-import] [Message: <code>subprocess</code> module is possibly insecure] What it does Checks for imports of the subprocess module. Why is this bad? It is possible to inject malicious commands into subprocess calls. Consider possible security implications associated with this module. Example import subprocess",
    "option": {}
  },
  "S405": {
    "description": "[Status: preview] [Name: suspicious-xml-etree-import] [Message: <code>xml.etree</code> methods are vulnerable to XML attacks] What it does Checks for imports of the xml.etree.cElementTree and xml.etree.ElementTree modules Why is this bad? Using various methods from these modules to parse untrusted XML data is known to be vulnerable to XML attacks. Replace vulnerable imports with the equivalent defusedxml package, or make sure defusedxml.defuse_stdlib() is called before parsing XML data. Example import xml.etree.cElementTree",
    "option": {}
  },
  "S406": {
    "description": "[Status: preview] [Name: suspicious-xml-sax-import] [Message: <code>xml.sax</code> methods are vulnerable to XML attacks] What it does Checks for imports of the xml.sax module. Why is this bad? Using various methods from these modules to parse untrusted XML data is known to be vulnerable to XML attacks. Replace vulnerable imports with the equivalent defusedxml package, or make sure defusedxml.defuse_stdlib() is called before parsing XML data. Example import xml.sax",
    "option": {}
  },
  "S407": {
    "description": "[Status: preview] [Name: suspicious-xml-expat-import] [Message: <code>xml.dom.expatbuilder</code> is vulnerable to XML attacks] What it does Checks for imports of the xml.dom.expatbuilder module. Why is this bad? Using various methods from these modules to parse untrusted XML data is known to be vulnerable to XML attacks. Replace vulnerable imports with the equivalent defusedxml package, or make sure defusedxml.defuse_stdlib() is called before parsing XML data. Example import xml.dom.expatbuilder",
    "option": {}
  },
  "S408": {
    "description": "[Status: preview] [Name: suspicious-xml-minidom-import] [Message: <code>xml.dom.minidom</code> is vulnerable to XML attacks] What it does Checks for imports of the xml.dom.minidom module. Why is this bad? Using various methods from these modules to parse untrusted XML data is known to be vulnerable to XML attacks. Replace vulnerable imports with the equivalent defusedxml package, or make sure defusedxml.defuse_stdlib() is called before parsing XML data. Example import xml.dom.minidom",
    "option": {}
  },
  "S409": {
    "description": "[Status: preview] [Name: suspicious-xml-pulldom-import] [Message: <code>xml.dom.pulldom</code> is vulnerable to XML attacks] What it does Checks for imports of the xml.dom.pulldom module. Why is this bad? Using various methods from these modules to parse untrusted XML data is known to be vulnerable to XML attacks. Replace vulnerable imports with the equivalent defusedxml package, or make sure defusedxml.defuse_stdlib() is called before parsing XML data. Example import xml.dom.pulldom",
    "option": {}
  },
  "S411": {
    "description": "[Status: preview] [Name: suspicious-xmlrpc-import] [Message: XMLRPC is vulnerable to remote XML attacks] What it does Checks for imports of the xmlrpc module. Why is this bad? XMLRPC is a particularly dangerous XML module, as it is also concerned with communicating data over a network. Use the defused.xmlrpc.monkey_patch() function to monkey-patch the xmlrpclib module and mitigate remote XML attacks. Example import xmlrpc",
    "option": {}
  },
  "S412": {
    "description": "[Status: preview] [Name: suspicious-httpoxy-import] [Message: <code>httpoxy</code> is a set of vulnerabilities that affect application code running inCGI, or CGI-like environments. The use of CGI for web applications should be avoided] What it does Checks for imports of wsgiref.handlers.CGIHandler and twisted.web.twcgi.CGIScript. Why is this bad? httpoxy is a set of vulnerabilities that affect application code running in CGI or CGI-like environments. The use of CGI for web applications should be avoided to prevent this class of attack. Example from wsgiref.handlers import CGIHandler",
    "option": {}
  },
  "S413": {
    "description": "[Status: preview] [Name: suspicious-pycrypto-import] [Message: <code>pycrypto</code> library is known to have publicly disclosed buffer overflow vulnerability] What it does Checks for imports of several unsafe cryptography modules. Why is this bad? The pycrypto library is known to have a publicly disclosed buffer overflow vulnerability. It is no longer actively maintained and has been deprecated in favor of the pyca/cryptography library. Example import Crypto.Random",
    "option": {}
  },
  "S415": {
    "description": "[Status: preview] [Name: suspicious-pyghmi-import] [Message: An IPMI-related module is being imported. Prefer an encrypted protocol over IPMI.] What it does Checks for imports of the pyghmi module. Why is this bad? pyghmi is an IPMI-related module, but IPMI is considered insecure. Instead, use an encrypted protocol. Example import pyghmi",
    "option": {}
  },
  "S501": {
    "description": "[Status: stable] [Name: request-with-no-cert-validation] [Message: Probable use of <code>{string}</code> call with <code>verify=False</code> disabling SSL certificate checks] What it does Checks for HTTPS requests that disable SSL certificate checks. Why is this bad? If SSL certificates are not verified, an attacker could perform a \"man in the middle\" attack by intercepting and modifying traffic between the client and server. Example import requests requests.get(\"https://www.example.com\", verify=False) Use instead: import requests requests.get(\"https://www.example.com\") # By default, `verify=True`.",
    "option": {}
  },
  "S502": {
    "description": "[Status: stable] [Name: ssl-insecure-version] [Message: Call made with insecure SSL protocol: <code>{protocol}</code>] What it does Checks for function calls with parameters that indicate the use of insecure SSL and TLS protocol versions. Why is this bad? Several highly publicized exploitable flaws have been discovered in all versions of SSL and early versions of TLS. The following versions are considered insecure, and should be avoided: SSL v2 SSL v3 TLS v1 TLS v1.1 This method supports detection on the Python's built-in ssl module and the pyOpenSSL module. Example import ssl ssl.wrap_socket(ssl_version=ssl.PROTOCOL_TLSv1) Use instead: import ssl ssl.wrap_socket(ssl_version=ssl.PROTOCOL_TLSv1_2)",
    "option": {}
  },
  "S503": {
    "description": "[Status: stable] [Name: ssl-with-bad-defaults] [Message: Argument default set to insecure SSL protocol: <code>{protocol}</code>] What it does Checks for function definitions with default arguments set to insecure SSL and TLS protocol versions. Why is this bad? Several highly publicized exploitable flaws have been discovered in all versions of SSL and early versions of TLS. The following versions are considered insecure, and should be avoided: SSL v2 SSL v3 TLS v1 TLS v1.1 Example import ssl def func(version=ssl.PROTOCOL_TLSv1): ... Use instead: import ssl def func(version=ssl.PROTOCOL_TLSv1_2): ...",
    "option": {}
  },
  "S504": {
    "description": "[Status: stable] [Name: ssl-with-no-version] [Message: <code>ssl.wrap_socket</code> called without an `ssl_version``] What it does Checks for calls to ssl.wrap_socket() without an ssl_version. Why is this bad? This method is known to provide a default value that maximizes compatibility, but permits use of insecure protocols. Example import ssl ssl.wrap_socket() Use instead: import ssl ssl.wrap_socket(ssl_version=ssl.PROTOCOL_TLSv1_2)",
    "option": {}
  },
  "S505": {
    "description": "[Status: stable] [Name: weak-cryptographic-key] [Message: {cryptographic_key} key sizes below {minimum_key_size} bits are considered breakable] What it does Checks for uses of cryptographic keys with vulnerable key sizes. Why is this bad? Small keys are easily breakable. For DSA and RSA, keys should be at least 2048 bits long. For EC, keys should be at least 224 bits long. Example from cryptography.hazmat.primitives.asymmetric import dsa, ec dsa.generate_private_key(key_size=512) ec.generate_private_key(curve=ec.SECT163K1()) Use instead: from cryptography.hazmat.primitives.asymmetric import dsa, ec dsa.generate_private_key(key_size=4096) ec.generate_private_key(curve=ec.SECP384R1())",
    "option": {}
  },
  "S506": {
    "description": "[Status: stable] [Name: unsafe-yaml-load] [Message: Probable use of unsafe loader <code>{name}</code> with <code>yaml.load</code>. Allows instantiation of arbitrary objects. Consider <code>yaml.safe_load</code>.] What it does Checks for uses of the yaml.load function. Why is this bad? Running the yaml.load function over untrusted YAML files is insecure, as yaml.load allows for the creation of arbitrary Python objects, which can then be used to execute arbitrary code. Instead, consider using yaml.safe_load, which allows for the creation of simple Python objects like integers and lists, but prohibits the creation of more complex objects like functions and classes. Example import yaml yaml.load(untrusted_yaml) Use instead: import yaml yaml.safe_load(untrusted_yaml)",
    "option": {}
  },
  "S507": {
    "description": "[Status: stable] [Name: ssh-no-host-key-verification] [Message: Paramiko call with policy set to automatically trust the unknown host key] What it does Checks for uses of policies disabling SSH verification in Paramiko. Why is this bad? By default, Paramiko checks the identity of the remote host when establishing an SSH connection. Disabling the verification might lead to the client connecting to a malicious host, without the client knowing. Example from paramiko import client ssh_client = client.SSHClient() ssh_client.set_missing_host_key_policy(client.AutoAddPolicy) Use instead: from paramiko import client ssh_client = client.SSHClient() ssh_client.set_missing_host_key_policy(client.RejectPolicy)",
    "option": {}
  },
  "S508": {
    "description": "[Status: stable] [Name: snmp-insecure-version] [Message: The use of SNMPv1 and SNMPv2 is insecure. Use SNMPv3 if able.] What it does Checks for uses of SNMPv1 or SNMPv2. Why is this bad? The SNMPv1 and SNMPv2 protocols are considered insecure as they do not support encryption. Instead, prefer SNMPv3, which supports encryption. Example from pysnmp.hlapi import CommunityData CommunityData(\"public\", mpModel=0) Use instead: from pysnmp.hlapi import CommunityData CommunityData(\"public\", mpModel=2)",
    "option": {}
  },
  "S509": {
    "description": "[Status: stable] [Name: snmp-weak-cryptography] [Message: You should not use SNMPv3 without encryption. <code>noAuthNoPriv</code> & <code>authNoPriv</code> is insecure.] What it does Checks for uses of the SNMPv3 protocol without encryption. Why is this bad? Unencrypted SNMPv3 communication can be intercepted and read by unauthorized parties. Instead, enable encryption when using SNMPv3. Example from pysnmp.hlapi import UsmUserData UsmUserData(\"user\") Use instead: from pysnmp.hlapi import UsmUserData UsmUserData(\"user\", \"authkey\", \"privkey\")",
    "option": {}
  },
  "S601": {
    "description": "[Status: stable] [Name: paramiko-call] [Message: Possible shell injection via Paramiko call; check inputs are properly sanitized] What it does Checks for paramiko calls. Why is this bad? paramiko calls allow users to execute arbitrary shell commands on a remote machine. If the inputs to these calls are not properly sanitized, they can be vulnerable to shell injection attacks. Example import paramiko client = paramiko.SSHClient() client.exec_command(\"echo $HOME\")",
    "option": {}
  },
  "S602": {
    "description": "[Status: stable] [Name: subprocess-popen-with-shell-equals-true] [Message: <code>subprocess</code> call with <code>shell=True</code> seems safe, but may be changed in the future; consider rewriting without <code>shell</code>] What it does Check for method calls that initiate a subprocess with a shell. Why is this bad? Starting a subprocess with a shell can allow attackers to execute arbitrary shell commands. Consider starting the process without a shell call and sanitize the input to mitigate the risk of shell injection. Example import subprocess subprocess.run(\"ls -l\", shell=True) Use instead: import subprocess subprocess.run([\"ls\", \"-l\"])",
    "option": {}
  },
  "S603": {
    "description": "[Status: stable] [Name: subprocess-without-shell-equals-true] [Message: <code>subprocess</code> call: check for execution of untrusted input] What it does Check for method calls that initiate a subprocess without a shell. Why is this bad? Starting a subprocess without a shell can prevent attackers from executing arbitrary shell commands; however, it is still error-prone. Consider validating the input. Known problems Prone to false positives as it is difficult to determine whether the passed arguments have been validated (#4045). Example import subprocess cmd = input(\"Enter a command: \").split() subprocess.run(cmd)",
    "option": {}
  },
  "S604": {
    "description": "[Status: stable] [Name: call-with-shell-equals-true] [Message: Function call with <code>shell=True</code> parameter identified, security issue] What it does Checks for method calls that set the shell parameter to true or another truthy value when invoking a subprocess. Why is this bad? Setting the shell parameter to true or another truthy value when invoking a subprocess can introduce security vulnerabilities, as it allows shell metacharacters and whitespace to be passed to child processes, potentially leading to shell injection attacks. It is recommended to avoid using shell=True unless absolutely necessary and, when used, to ensure that all inputs are properly sanitized and quoted to prevent such vulnerabilities. Known problems Prone to false positives as it is triggered on any function call with a shell=True parameter. Example import my_custom_subprocess user_input = input(\"Enter a command: \") my_custom_subprocess.run(user_input, shell=True)",
    "option": {}
  },
  "S605": {
    "description": "[Status: stable] [Name: start-process-with-a-shell] [Message: Starting a process with a shell: seems safe, but may be changed in the future; consider rewriting without <code>shell</code>] What it does Checks for calls that start a process with a shell, providing guidance on whether the usage is safe or not. Why is this bad? Starting a process with a shell can introduce security risks, such as code injection vulnerabilities. It's important to be aware of whether the usage of the shell is safe or not. This rule triggers on functions like os.system, popen, etc., which start processes with a shell. It evaluates whether the provided command is a literal string or an expression. If the command is a literal string, it's considered safe. If the command is an expression, it's considered (potentially) unsafe. Example import os # Safe usage (literal string) command = \"ls -l\" os.system(command) # Potentially unsafe usage (expression) cmd = get_user_input() os.system(cmd) Note The subprocess module provides more powerful facilities for spawning new processes and retrieving their results, and using that module is preferable to using os.system or similar functions. Consider replacing such usages with subprocess.call or related functions.",
    "option": {}
  },
  "S606": {
    "description": "[Status: stable] [Name: start-process-with-no-shell] [Message: Starting a process without a shell] What it does Checks for functions that start a process without a shell. Why is this bad? Invoking any kind of external executable via a function call can pose security risks if arbitrary variables are passed to the executable, or if the input is otherwise unsanitised or unvalidated. This rule specifically flags functions in the os module that spawn subprocesses without the use of a shell. Note that these typically pose a much smaller security risk than subprocesses that are started with a shell, which are flagged by start-process-with-a-shell (S605). This gives you the option of enabling one rule while disabling the other if you decide that the security risk from these functions is acceptable for your use case. Example import os def insecure_function(arbitrary_user_input: str): os.spawnlp(os.P_NOWAIT, \"/bin/mycmd\", \"mycmd\", arbitrary_user_input)",
    "option": {}
  },
  "S607": {
    "description": "[Status: stable] [Name: start-process-with-partial-path] [Message: Starting a process with a partial executable path] What it does Checks for the starting of a process with a partial executable path. Why is this bad? Starting a process with a partial executable path can allow attackers to execute an arbitrary executable by adjusting the PATH environment variable. Consider using a full path to the executable instead. Example import subprocess subprocess.Popen([\"ruff\", \"check\", \"file.py\"]) Use instead: import subprocess subprocess.Popen([\"/usr/bin/ruff\", \"check\", \"file.py\"])",
    "option": {}
  },
  "S608": {
    "description": "[Status: stable] [Name: hardcoded-sql-expression] [Message: Possible SQL injection vector through string-based query construction] What it does Checks for strings that resemble SQL statements involved in some form string building operation. Why is this bad? SQL injection is a common attack vector for web applications. Directly interpolating user input into SQL statements should always be avoided. Instead, favor parameterized queries, in which the SQL statement is provided separately from its parameters, as supported by psycopg3 and other database drivers and ORMs. Example query = \"DELETE FROM foo WHERE id = '%s'\" % identifier",
    "option": {}
  },
  "S609": {
    "description": "[Status: stable] [Name: unix-command-wildcard-injection] [Message: Possible wildcard injection in call due to <code>*</code> usage] What it does Checks for possible wildcard injections in calls to subprocess.Popen(). Why is this bad? Wildcard injections can lead to unexpected behavior if unintended files are matched by the wildcard. Consider using a more specific path instead. Example import subprocess subprocess.Popen([\"chmod\", \"777\", \"*.py\"], shell=True) Use instead: import subprocess subprocess.Popen([\"chmod\", \"777\", \"main.py\"], shell=True)",
    "option": {}
  },
  "S610": {
    "description": "[Status: stable] [Name: django-extra] [Message: Use of Django <code>extra</code> can lead to SQL injection vulnerabilities] What it does Checks for uses of Django's extra function where one or more arguments passed are not literal expressions. Why is this bad? Django's extra function can be used to execute arbitrary SQL queries, which can in turn lead to SQL injection vulnerabilities. Example from django.contrib.auth.models import User # String interpolation creates a security loophole that could be used # for SQL injection: User.objects.all().extra(select={\"test\": \"%secure\" % \"nos\"}) Use instead: from django.contrib.auth.models import User # SQL injection is impossible if all arguments are literal expressions: User.objects.all().extra(select={\"test\": \"secure\"})",
    "option": {}
  },
  "S611": {
    "description": "[Status: stable] [Name: django-raw-sql] [Message: Use of <code>RawSQL</code> can lead to SQL injection vulnerabilities] What it does Checks for uses of Django's RawSQL function. Why is this bad? Django's RawSQL function can be used to execute arbitrary SQL queries, which can in turn lead to SQL injection vulnerabilities. Example from django.db.models.expressions import RawSQL from django.contrib.auth.models import User User.objects.annotate(val=RawSQL(\"%s\" % input_param, []))",
    "option": {}
  },
  "S612": {
    "description": "[Status: stable] [Name: logging-config-insecure-listen] [Message: Use of insecure <code>logging.config.listen</code> detected] What it does Checks for insecure logging.config.listen calls. Why is this bad? logging.config.listen starts a server that listens for logging configuration requests. This is insecure, as parts of the configuration are passed to the built-in eval function, which can be used to execute arbitrary code. Example import logging logging.config.listen(9999)",
    "option": {}
  },
  "S701": {
    "description": "[Status: stable] [Name: jinja2-autoescape-false] [Message: Using jinja2 templates with <code>autoescape=False</code> is dangerous and can lead to XSS. Ensure <code>autoescape=True</code> or use the <code>select_autoescape</code> function.] What it does Checks for jinja2 templates that use autoescape=False. Why is this bad? jinja2 templates that use autoescape=False are vulnerable to cross-site scripting (XSS) attacks that allow attackers to execute arbitrary JavaScript. By default, jinja2 sets autoescape to False, so it is important to set autoescape=True or use the select_autoescape function to mitigate XSS vulnerabilities. Example import jinja2 jinja2.Environment(loader=jinja2.FileSystemLoader(\".\")) Use instead: import jinja2 jinja2.Environment(loader=jinja2.FileSystemLoader(\".\"), autoescape=True)",
    "option": {}
  },
  "S702": {
    "description": "[Status: stable] [Name: mako-templates] [Message: Mako templates allow HTML and JavaScript rendering by default and are inherently open to XSS attacks] What it does Checks for uses of the mako templates. Why is this bad? Mako templates allow HTML and JavaScript rendering by default, and are inherently open to XSS attacks. Ensure variables in all templates are properly sanitized via the n, h or x flags (depending on context). For example, to HTML escape the variable data, use ${ data |h }. Example from mako.template import Template Template(\"hello\") Use instead: from mako.template import Template Template(\"hello |h\")",
    "option": {}
  },
  "S704": {
    "description": "[Status: stable] [Name: unsafe-markup-use] [Message: Unsafe use of <code>{name}</code> detected] What it does Checks for non-literal strings being passed to markupsafe.Markup. Why is this bad? markupsafe.Markup does not perform any escaping, so passing dynamic content, like f-strings, variables or interpolated strings will potentially lead to XSS vulnerabilities. Instead you should interpolate the Markup object. Using lint.flake8-bandit.extend-markup-names additional objects can be treated like Markup. This rule was originally inspired by flake8-markupsafe but doesn't carve out any exceptions for i18n related calls by default. You can use lint.flake8-bandit.allowed-markup-calls to specify exceptions. Example Given: from markupsafe import Markup content = \"<script>alert('Hello, world!')</script>\" html = Markup(f\"<b>{content}</b>\") # XSS Use instead: from markupsafe import Markup content = \"<script>alert('Hello, world!')</script>\" html = Markup(\"<b>{}</b>\").format(content) # Safe Given: from markupsafe import Markup lines = [ Markup(\"<b>heading</b>\"), \"<script>alert('XSS attempt')</script>\", ] html = Markup(\"<br>\".join(lines)) # XSS Use instead: from markupsafe import Markup lines = [ Markup(\"<b>heading</b>\"), \"<script>alert('XSS attempt')</script>\", ] html = Markup(\"<br>\").join(lines) # Safe",
    "option": {
      "lint.flake8-bandit.extend-markup-names": {
        "option_description": "A list of additional callable names that behave like\nmarkupsafe.Markup.\n\nExpects to receive a list of fully-qualified names (e.g., webhelpers.html.literal, rather than\nliteral).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-bandit]\nextend-markup-names = [\"webhelpers.html.literal\", \"my_package.Markup\"]\n\n\n\n[lint.flake8-bandit]\nextend-markup-names = [\"webhelpers.html.literal\", \"my_package.Markup\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.flake8-bandit.allowed-markup-calls": {
        "option_description": "A list of callable names, whose result may be safely passed into\nmarkupsafe.Markup.\n\nExpects to receive a list of fully-qualified names (e.g., bleach.clean, rather than clean).\n\nThis setting helps you avoid false positives in code like:\n\nfrom bleach import clean\nfrom markupsafe import Markup\n\ncleaned_markup = Markup(clean(some_user_input))\n\nWhere the use of bleach.clean\nusually ensures that there's no XSS vulnerability.\n\nAlthough it is not recommended, you may also use this setting to whitelist other\nkinds of calls, e.g. calls to i18n translation functions, where how safe that is\nwill depend on the implementation and how well the translations are audited.\n\nAnother common use-case is to wrap the output of functions that generate markup\nlike xml.etree.ElementTree.tostring\nor template rendering engines where sanitization of potential user input is either\nalready baked in or has to happen before rendering.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-bandit]\nallowed-markup-calls = [\"bleach.clean\", \"my_package.sanitize\"]\n\n\n\n[lint.flake8-bandit]\nallowed-markup-calls = [\"bleach.clean\", \"my_package.sanitize\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "BLE001": {
    "description": "[Status: stable] [Name: blind-except] [Message: Do not catch blind exception: <code>{name}</code>] What it does Checks for except clauses that catch all exceptions. This includes except BaseException and except Exception. Why is this bad? Overly broad except clauses can lead to unexpected behavior, such as catching KeyboardInterrupt or SystemExit exceptions that prevent the user from exiting the program. Instead of catching all exceptions, catch only those that are expected to be raised in the try block. Example try: foo() except BaseException: ... Use instead: try: foo() except FileNotFoundError: ... Exceptions that are re-raised will not be flagged, as they're expected to be caught elsewhere: try: foo() except BaseException: raise Exceptions that are logged with exc_info enabled will not be flagged, as this is a common pattern for propagating exception traces: try: foo() except BaseException: logging.exception(\"Something went wrong\")",
    "option": {
      "lint.logger-objects": {
        "option_description": "A list of objects that should be treated equivalently to a\nlogging.Logger object.\n\nThis is useful for ensuring proper diagnostics (e.g., to identify\nlogging deprecations and other best-practices) for projects that\nre-export a logging.Logger object from a common module.\n\nFor example, if you have a module logging_setup.py with the following\ncontents:\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nAdding \"logging_setup.logger\" to logger-objects will ensure that\nlogging_setup.logger is treated as a logging.Logger object when\nimported from other modules (e.g., from logging_setup import logger).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\nlogger-objects = [\"logging_setup.logger\"]\n\n\n\n[lint]\nlogger-objects = [\"logging_setup.logger\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "FBT001": {
    "description": "[Status: stable] [Name: boolean-type-hint-positional-argument] [Message: Boolean-typed positional argument in function definition] What it does Checks for the use of boolean positional arguments in function definitions, as determined by the presence of a type hint containing bool as an evident subtype - e.g. bool, bool | int, typing.Optional[bool], etc. Why is this bad? Calling a function with boolean positional arguments is confusing as the meaning of the boolean value is not clear to the caller and to future readers of the code. The use of a boolean will also limit the function to only two possible behaviors, which makes the function difficult to extend in the future. Instead, consider refactoring into separate implementations for the True and False cases, using an Enum, or making the argument a keyword-only argument, to force callers to be explicit when providing the argument. Dunder methods that define operators are exempt from this rule, as are setters and @override definitions. Example from math import ceil, floor def round_number(number: float, up: bool) -> int: return ceil(number) if up else floor(number) round_number(1.5, True) # What does `True` mean? round_number(1.5, False) # What does `False` mean? Instead, refactor into separate implementations: from math import ceil, floor def round_up(number: float) -> int: return ceil(number) def round_down(number: float) -> int: return floor(number) round_up(1.5) round_down(1.5) Or, refactor to use an Enum: from enum import Enum class RoundingMethod(Enum): UP = 1 DOWN = 2 def round_number(value: float, method: RoundingMethod) -> float: ... Or, make the argument a keyword-only argument: from math import ceil, floor def round_number(number: float, *, up: bool) -> int: return ceil(number) if up else floor(number) round_number(1.5, up=True) round_number(1.5, up=False)",
    "option": {}
  },
  "FBT002": {
    "description": "[Status: stable] [Name: boolean-default-value-positional-argument] [Message: Boolean default positional argument in function definition] What it does Checks for the use of boolean positional arguments in function definitions, as determined by the presence of a boolean default value. Why is this bad? Calling a function with boolean positional arguments is confusing as the meaning of the boolean value is not clear to the caller and to future readers of the code. The use of a boolean will also limit the function to only two possible behaviors, which makes the function difficult to extend in the future. Instead, consider refactoring into separate implementations for the True and False cases, using an Enum, or making the argument a keyword-only argument, to force callers to be explicit when providing the argument. This rule exempts methods decorated with @typing.override, since changing the signature of a subclass method that overrides a superclass method may cause type checkers to complain about a violation of the Liskov Substitution Principle. Example from math import ceil, floor def round_number(number, up=True): return ceil(number) if up else floor(number) round_number(1.5, True) # What does `True` mean? round_number(1.5, False) # What does `False` mean? Instead, refactor into separate implementations: from math import ceil, floor def round_up(number): return ceil(number) def round_down(number): return floor(number) round_up(1.5) round_down(1.5) Or, refactor to use an Enum: from enum import Enum class RoundingMethod(Enum): UP = 1 DOWN = 2 def round_number(value, method): return ceil(number) if method is RoundingMethod.UP else floor(number) round_number(1.5, RoundingMethod.UP) round_number(1.5, RoundingMethod.DOWN) Or, make the argument a keyword-only argument: from math import ceil, floor def round_number(number, *, up=True): return ceil(number) if up else floor(number) round_number(1.5, up=True) round_number(1.5, up=False)",
    "option": {}
  },
  "FBT003": {
    "description": "[Status: stable] [Name: boolean-positional-value-in-call] [Message: Boolean positional value in function call] What it does Checks for boolean positional arguments in function calls. Some functions are whitelisted by default. To extend the list of allowed calls configure the lint.flake8-boolean-trap.extend-allowed-calls option. Why is this bad? Calling a function with boolean positional arguments is confusing as the meaning of the boolean value is not clear to the caller, and to future readers of the code. Example def func(flag: bool) -> None: ... func(True) Use instead: def func(flag: bool) -> None: ... func(flag=True)",
    "option": {
      "lint.flake8-boolean-trap.extend-allowed-calls": {
        "option_description": "Additional callable functions with which to allow boolean traps.\n\nExpects to receive a list of fully-qualified names (e.g., pydantic.Field, rather than\nField).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-boolean-trap]\nextend-allowed-calls = [\"pydantic.Field\", \"django.db.models.Value\"]\n\n\n\n[lint.flake8-boolean-trap]\nextend-allowed-calls = [\"pydantic.Field\", \"django.db.models.Value\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "B002": {
    "description": "[Status: stable] [Name: unary-prefix-increment-decrement] [Message: Python does not support the unary prefix increment operator (<code>++</code>)] What it does Checks for the attempted use of the unary prefix increment (++) or decrement operator (--). Why is this bad? Python does not support the unary prefix increment or decrement operator. Writing ++n is equivalent to +(+(n)) and writing --n is equivalent to -(-(n)). In both cases, it is equivalent to n. Example ++x --y Use instead: x += 1 y -= 1",
    "option": {}
  },
  "B003": {
    "description": "[Status: stable] [Name: assignment-to-os-environ] [Message: Assigning to <code>os.environ</code> doesn't clear the environment] What it does Checks for assignments to os.environ. Why is this bad? In Python, os.environ is a mapping that represents the environment of the current process. However, reassigning to os.environ does not clear the environment. Instead, it merely updates the os.environ for the current process. This can lead to unexpected behavior, especially when running the program in a subprocess. Instead, use os.environ.clear() to clear the environment, or use the env argument of subprocess.Popen to pass a custom environment to a subprocess. Example import os os.environ = {\"foo\": \"bar\"} Use instead: import os os.environ.clear() os.environ[\"foo\"] = \"bar\"",
    "option": {}
  },
  "B004": {
    "description": "[Status: stable] [Name: unreliable-callable-check] [Message: Using <code>hasattr(x, \"__call__\")</code> to test if x is callable is unreliable. Use <code>callable(x)</code> for consistent results.] What it does Checks for uses of hasattr to test if an object is callable (e.g., hasattr(obj, \"__call__\")). Why is this bad? Using hasattr is an unreliable mechanism for testing if an object is callable. If obj implements a custom __getattr__, or if its __call__ is itself not callable, you may get misleading results. Instead, use callable(obj) to test if obj is callable. Example hasattr(obj, \"__call__\") Use instead: callable(obj) Fix safety This rule's fix is marked as unsafe because the replacement may not be semantically equivalent to the original expression, potentially changing the behavior of the code. For example, an imported module may have a __call__ attribute but is not considered a callable object: import operator assert hasattr(operator, \"__call__\") assert callable(operator) is False Additionally, __call__ may be defined only as an instance method: class A: def __init__(self): self.__call__ = None assert hasattr(A(), \"__call__\") assert callable(A()) is False Additionally, if there are comments in the hasattr call expression, they may be removed: hasattr( # comment 1 obj, # comment 2 # comment 3 \"__call__\", # comment 4 # comment 5 )",
    "option": {}
  },
  "B005": {
    "description": "[Status: stable] [Name: strip-with-multi-characters] [Message: Using <code>.strip()</code> with multi-character strings is misleading] What it does Checks for uses of multi-character strings in .strip(), .lstrip(), and .rstrip() calls. Why is this bad? All characters in the call to .strip(), .lstrip(), or .rstrip() are removed from the leading and trailing ends of the string. If the string contains multiple characters, the reader may be misled into thinking that a prefix or suffix is being removed, rather than a set of characters. In Python 3.9 and later, you can use str.removeprefix and str.removesuffix to remove an exact prefix or suffix from a string, respectively, which should be preferred when possible. Known problems As a heuristic, this rule only flags multi-character strings that contain duplicate characters. This allows usages like .strip(\"xyz\"), which removes all occurrences of the characters x, y, and z from the leading and trailing ends of the string, but not .strip(\"foo\"). The use of unique, multi-character strings may be intentional and consistent with the intent of .strip(), .lstrip(), or .rstrip(), while the use of duplicate-character strings is very likely to be a mistake. Example \"text.txt\".strip(\".txt\") # \"e\" Use instead: \"text.txt\".removesuffix(\".txt\") # \"text\"",
    "option": {}
  },
  "B006": {
    "description": "[Status: stable] [Name: mutable-argument-default] [Message: Do not use mutable data structures for argument defaults] What it does Checks for uses of mutable objects as function argument defaults. Why is this bad? Function defaults are evaluated once, when the function is defined. The same mutable object is then shared across all calls to the function. If the object is modified, those modifications will persist across calls, which can lead to unexpected behavior. Instead, prefer to use immutable data structures, or take None as a default, and initialize a new mutable object inside the function body for each call. Arguments with immutable type annotations will be ignored by this rule. Types outside of the standard library can be marked as immutable with the lint.flake8-bugbear.extend-immutable-calls configuration option. Known problems Mutable argument defaults can be used intentionally to cache computation results. Replacing the default with None or an immutable data structure does not work for such usages. Instead, prefer the @functools.lru_cache decorator from the standard library. Example def add_to_list(item, some_list=[]): some_list.append(item) return some_list l1 = add_to_list(0) # [0] l2 = add_to_list(1) # [0, 1] Use instead: def add_to_list(item, some_list=None): if some_list is None: some_list = [] some_list.append(item) return some_list l1 = add_to_list(0) # [0] l2 = add_to_list(1) # [1] Fix safety This fix is marked as unsafe because it replaces the mutable default with None and initializes it in the function body, which may not be what the user intended, as described above.",
    "option": {
      "lint.flake8-bugbear.extend-immutable-calls": {
        "option_description": "Additional callable functions to consider \"immutable\" when evaluating, e.g., the\nfunction-call-in-default-argument rule (B008) or function-call-in-dataclass-defaults\nrule (RUF009).\n\nExpects to receive a list of fully-qualified names (e.g., fastapi.Query, rather than\nQuery).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-bugbear]\n# Allow default arguments like, e.g., `data: List[str] = fastapi.Query(None)`.\nextend-immutable-calls = [\"fastapi.Depends\", \"fastapi.Query\"]\n\n\n\n[lint.flake8-bugbear]\n# Allow default arguments like, e.g., `data: List[str] = fastapi.Query(None)`.\nextend-immutable-calls = [\"fastapi.Depends\", \"fastapi.Query\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "B007": {
    "description": "[Status: stable] [Name: unused-loop-control-variable] [Message: Loop control variable <code>{name}</code> not used within loop body] What it does Checks for unused variables in loops (e.g., for and while statements). Why is this bad? Defining a variable in a loop statement that is never used can confuse readers. If the variable is intended to be unused (e.g., to facilitate destructuring of a tuple or other object), prefix it with an underscore to indicate the intent. Otherwise, remove the variable entirely. Example for i, j in foo: bar(i) Use instead: for i, _j in foo: bar(i)",
    "option": {
      "lint.dummy-variable-rgx": {
        "option_description": "A regular expression used to identify \"dummy\" variables, or those which\nshould be ignored when enforcing (e.g.) unused-variable rules. The\ndefault expression matches _, __, and _var, but not _var_.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\n# Only ignore variables named \"_\".\ndummy-variable-rgx = \"^_$\"\n\n\n\n[lint]\n# Only ignore variables named \"_\".\ndummy-variable-rgx = \"^_$\"",
        "option_data_type": "str",
        "default_value": "\"^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "B008": {
    "description": "[Status: stable] [Name: function-call-in-default-argument] [Message: Do not perform function call <code>{name}</code> in argument defaults; instead, perform the call within the function, or read the default from a module-level singleton variable] What it does Checks for function calls in default function arguments. Why is this bad? Any function call that's used in a default argument will only be performed once, at definition time. The returned value will then be reused by all calls to the function, which can lead to unexpected behaviour. Parameters with immutable type annotations will be ignored by this rule. Those whose default arguments are NewType calls where the original type is immutable are also ignored. Calls and types outside of the standard library can be marked as an exception to this rule with the lint.flake8-bugbear.extend-immutable-calls configuration option. Example def create_list() -> list[int]: return [1, 2, 3] def mutable_default(arg: list[int] = create_list()) -> list[int]: arg.append(4) return arg Use instead: def better(arg: list[int] | None = None) -> list[int]: if arg is None: arg = create_list() arg.append(4) return arg If the use of a singleton is intentional, assign the result call to a module-level variable, and use that variable in the default argument: ERROR = ValueError(\"Hosts weren't successfully added\") def add_host(error: Exception = ERROR) -> None: ...",
    "option": {
      "lint.flake8-bugbear.extend-immutable-calls": {
        "option_description": "Additional callable functions to consider \"immutable\" when evaluating, e.g., the\nfunction-call-in-default-argument rule (B008) or function-call-in-dataclass-defaults\nrule (RUF009).\n\nExpects to receive a list of fully-qualified names (e.g., fastapi.Query, rather than\nQuery).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-bugbear]\n# Allow default arguments like, e.g., `data: List[str] = fastapi.Query(None)`.\nextend-immutable-calls = [\"fastapi.Depends\", \"fastapi.Query\"]\n\n\n\n[lint.flake8-bugbear]\n# Allow default arguments like, e.g., `data: List[str] = fastapi.Query(None)`.\nextend-immutable-calls = [\"fastapi.Depends\", \"fastapi.Query\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "B009": {
    "description": "[Status: stable] [Name: get-attr-with-constant] [Message: Do not call <code>getattr</code> with a constant attribute value. It is not any safer than normal property access.] What it does Checks for uses of getattr that take a constant attribute value as an argument (e.g., getattr(obj, \"foo\")). Why is this bad? getattr is used to access attributes dynamically. If the attribute is defined as a constant, it is no safer than a typical property access. When possible, prefer property access over getattr calls, as the former is more concise and idiomatic. Example getattr(obj, \"foo\") Use instead: obj.foo Fix safety The fix is marked as unsafe for attribute names that are not in NFKC (Normalization Form KC) normalization. Python normalizes identifiers using NFKC when using attribute access syntax (e.g., obj.attr), but does not normalize string arguments passed to getattr. Rewriting getattr(obj, \"\") to obj. would be interpreted as obj.s at runtime, changing behavior. Additionally, the fix is marked as unsafe if the expression contains comments, as the replacement may remove comments attached to the original getattr call. For example, the long s character \"\" normalizes to \"s\" under NFKC, so: # This accesses an attribute with the exact name \"\" (if it exists) value = getattr(obj, \"\") # But this would normalize to \"s\" and access a different attribute obj. # This is interpreted as obj.s, not obj.",
    "option": {}
  },
  "B010": {
    "description": "[Status: stable] [Name: set-attr-with-constant] [Message: Do not call <code>setattr</code> with a constant attribute value. It is not any safer than normal property access.] What it does Checks for uses of setattr that take a constant attribute value as an argument (e.g., setattr(obj, \"foo\", 42)). Why is this bad? setattr is used to set attributes dynamically. If the attribute is defined as a constant, it is no safer than a typical property access. When possible, prefer property access over setattr calls, as the former is more concise and idiomatic. Example setattr(obj, \"foo\", 42) Use instead: obj.foo = 42 Fix safety The fix is marked as unsafe for attribute names that are not in NFKC (Normalization Form KC) normalization. Python normalizes identifiers using NFKC when using attribute access syntax (e.g., obj.attr = value), but does not normalize string arguments passed to setattr. Rewriting setattr(obj, \"\", 1) to obj. = 1 would be interpreted as obj.s = 1 at runtime, changing behavior. Additionally, the fix is marked as unsafe if the expression contains comments, as the replacement may remove comments attached to the original setattr call. For example, the long s character \"\" normalizes to \"s\" under NFKC, so: # This creates an attribute with the exact name \"\" setattr(obj, \"\", 1) getattr(obj, \"\") # Returns 1 # But this would normalize to \"s\" and set a different attribute obj. = 1 # This is interpreted as obj.s = 1, not obj. = 1",
    "option": {}
  },
  "B011": {
    "description": "[Status: stable] [Name: assert-false] [Message: Do not <code>assert False</code> (<code>python -O</code> removes these calls), raise <code>AssertionError()</code>] What it does Checks for uses of assert False. Why is this bad? Python removes assert statements when running in optimized mode (python -O), making assert False an unreliable means of raising an AssertionError. Instead, raise an AssertionError directly. Example assert False Use instead: raise AssertionError Fix safety This rule's fix is marked as unsafe, as changing an assert to a raise will change the behavior of your program when running in optimized mode (python -O).",
    "option": {}
  },
  "B012": {
    "description": "[Status: stable] [Name: jump-statement-in-finally] [Message: <code>{name}</code> inside <code>finally</code> blocks cause exceptions to be silenced] What it does Checks for break, continue, and return statements in finally blocks. Why is this bad? The use of break, continue, and return statements in finally blocks can cause exceptions to be silenced. finally blocks execute regardless of whether an exception is raised. If a break, continue, or return statement is reached in a finally block, any exception raised in the try or except blocks will be silenced. Example def speed(distance, time): try: return distance / time except ZeroDivisionError: raise ValueError(\"Time cannot be zero\") finally: return 299792458 # `ValueError` is silenced Use instead: def speed(distance, time): try: return distance / time except ZeroDivisionError: raise ValueError(\"Time cannot be zero\")",
    "option": {}
  },
  "B013": {
    "description": "[Status: stable] [Name: redundant-tuple-in-exception-handler] [Message: A length-one tuple literal is redundant in exception handlers] What it does Checks for single-element tuples in exception handlers (e.g., except (ValueError,):). Note: Single-element tuples consisting of a starred expression are allowed. Why is this bad? A tuple with a single element can be more concisely and idiomatically expressed as a single value. Example try: ... except (ValueError,): ... Use instead: try: ... except ValueError: ... Fix safety This rule's fix is marked as safe, unless the exception handler contains comments.",
    "option": {}
  },
  "B014": {
    "description": "[Status: stable] [Name: duplicate-handler-exception] [Message: Exception handler with duplicate exception: <code>{name}</code>] What it does Checks for exception handlers that catch duplicate exceptions. Why is this bad? Including the same exception multiple times in the same handler is redundant, as the first exception will catch the exception, making the second exception unreachable. The same applies to exception hierarchies, as a handler for a parent exception (like Exception) will also catch child exceptions (like ValueError). Example try: ... except (Exception, ValueError): # `Exception` includes `ValueError`. ... Use instead: try: ... except Exception: ...",
    "option": {}
  },
  "B015": {
    "description": "[Status: stable] [Name: useless-comparison] [Message: Pointless comparison. Did you mean to assign a value? Otherwise, prepend <code>assert</code> or remove it.] What it does Checks for useless comparisons. Why is this bad? Useless comparisons have no effect on the program, and are often included by mistake. If the comparison is intended to enforce an invariant, prepend the comparison with an assert. Otherwise, remove it entirely. Example foo == bar Use instead: assert foo == bar, \"`foo` and `bar` should be equal.\" Notebook behavior For Jupyter Notebooks, this rule is not applied to the last top-level expression in a cell. This is because it's common to have a notebook cell that ends with an expression, which will result in the repr of the evaluated expression being printed as the cell's output.",
    "option": {}
  },
  "B016": {
    "description": "[Status: stable] [Name: raise-literal] [Message: Cannot raise a literal. Did you intend to return it or raise an Exception?] What it does Checks for raise statements that raise a literal value. Why is this bad? raise must be followed by an exception instance or an exception class, and exceptions must be instances of BaseException or a subclass thereof. Raising a literal will raise a TypeError at runtime. Example raise \"foo\" Use instead: raise Exception(\"foo\")",
    "option": {}
  },
  "B017": {
    "description": "[Status: stable] [Name: assert-raises-exception] [Message: Do not assert blind exception: <code>{exception}</code>] What it does Checks for assertRaises and pytest.raises context managers that catch Exception or BaseException. Why is this bad? These forms catch every Exception, which can lead to tests passing even if, e.g., the code under consideration raises a SyntaxError or IndentationError. Either assert for a more specific exception (builtin or custom), or use assertRaisesRegex or pytest.raises(..., match=<REGEX>) respectively. Example self.assertRaises(Exception, foo) Use instead: self.assertRaises(SomeSpecificException, foo)",
    "option": {}
  },
  "B018": {
    "description": "[Status: stable] [Name: useless-expression] [Message: Found useless expression. Either assign it to a variable or remove it.] What it does Checks for useless expressions. Why is this bad? Useless expressions have no effect on the program, and are often included by mistake. Assign a useless expression to a variable, or remove it entirely. Example 1 + 1 Use instead: foo = 1 + 1 Notebook behavior For Jupyter Notebooks, this rule is not applied to the last top-level expression in a cell. This is because it's common to have a notebook cell that ends with an expression, which will result in the repr of the evaluated expression being printed as the cell's output. Known problems This rule ignores expression types that are commonly used for their side effects, such as function calls. However, if a seemingly useless expression (like an attribute access) is needed to trigger a side effect, consider assigning it to an anonymous variable, to indicate that the return value is intentionally ignored. For example, given: with errors.ExceptionRaisedContext(): obj.attribute Use instead: with errors.ExceptionRaisedContext(): _ = obj.attribute",
    "option": {}
  },
  "B019": {
    "description": "[Status: stable] [Name: cached-instance-method] [Message: Use of <code>functools.lru_cache</code> or <code>functools.cache</code> on methods can lead to memory leaks] What it does Checks for uses of the functools.lru_cache and functools.cache decorators on methods. Why is this bad? Using the functools.lru_cache and functools.cache decorators on methods can lead to memory leaks, as the global cache will retain a reference to the instance, preventing it from being garbage collected. Instead, refactor the method to depend only on its arguments and not on the instance of the class, or use the @lru_cache decorator on a function outside of the class. This rule ignores instance methods on enumeration classes, as enum members are singletons. Example from functools import lru_cache def square(x: int) -> int: return x * x class Number: value: int @lru_cache def squared(self): return square(self.value) Use instead: from functools import lru_cache @lru_cache def square(x: int) -> int: return x * x class Number: value: int def squared(self): return square(self.value)",
    "option": {
      "lint.pep8-naming.classmethod-decorators": {
        "option_description": "A list of decorators that, when applied to a method, indicate that the\nmethod should be treated as a class method (in addition to the builtin\n@classmethod).\n\nFor example, Ruff will expect that any method decorated by a decorator\nin this list takes a cls argument as its first argument.\n\nExpects to receive a list of fully-qualified names (e.g., pydantic.validator,\nrather than validator) or alternatively a plain name which is then matched against\nthe last segment in case the decorator itself consists of a dotted name.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nclassmethod-decorators = [\n    # Allow Pydantic's `@validator` decorator to trigger class method treatment.\n    \"pydantic.validator\",\n    # Allow SQLAlchemy's dynamic decorators, like `@field.expression`, to trigger class method treatment.\n    \"declared_attr\",\n    \"expression\",\n    \"comparator\",\n]\n\n\n\n[lint.pep8-naming]\nclassmethod-decorators = [\n    # Allow Pydantic's `@validator` decorator to trigger class method treatment.\n    \"pydantic.validator\",\n    # Allow SQLAlchemy's dynamic decorators, like `@field.expression`, to trigger class method treatment.\n    \"declared_attr\",\n    \"expression\",\n    \"comparator\",\n]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pep8-naming.staticmethod-decorators": {
        "option_description": "A list of decorators that, when applied to a method, indicate that the\nmethod should be treated as a static method (in addition to the builtin\n@staticmethod).\n\nFor example, Ruff will expect that any method decorated by a decorator\nin this list has no self or cls argument.\n\nExpects to receive a list of fully-qualified names (e.g., belay.Device.teardown,\nrather than teardown) or alternatively a plain name which is then matched against\nthe last segment in case the decorator itself consists of a dotted name.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\n# Allow Belay's `@Device.teardown` decorator to trigger static method treatment.\nstaticmethod-decorators = [\"belay.Device.teardown\"]\n\n\n\n[lint.pep8-naming]\n# Allow Belay's `@Device.teardown` decorator to trigger static method treatment.\nstaticmethod-decorators = [\"belay.Device.teardown\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "B020": {
    "description": "[Status: stable] [Name: loop-variable-overrides-iterator] [Message: Loop control variable <code>{name}</code> overrides iterable it iterates] What it does Checks for loop control variables that override the loop iterable. Why is this bad? Loop control variables should not override the loop iterable, as this can lead to confusing behavior. Instead, use a distinct variable name for any loop control variables. Example items = [1, 2, 3] for items in items: print(items) Use instead: items = [1, 2, 3] for item in items: print(item)",
    "option": {}
  },
  "B021": {
    "description": "[Status: stable] [Name: f-string-docstring] [Message: f-string used as docstring. Python will interpret this as a joined string, rather than a docstring.] What it does Checks for docstrings that are written via f-strings. Why is this bad? Python will interpret the f-string as a joined string, rather than as a docstring. As such, the \"docstring\" will not be accessible via the __doc__ attribute, nor will it be picked up by any automated documentation tooling. Example def foo(): f\"\"\"Not a docstring.\"\"\" Use instead: def foo(): \"\"\"A docstring.\"\"\"",
    "option": {}
  },
  "B022": {
    "description": "[Status: stable] [Name: useless-contextlib-suppress] [Message: No arguments passed to <code>contextlib.suppress</code>. No exceptions will be suppressed and therefore this context manager is redundant] What it does Checks for contextlib.suppress without arguments. Why is this bad? contextlib.suppress is a context manager that suppresses exceptions. It takes, as arguments, the exceptions to suppress within the enclosed block. If no exceptions are specified, then the context manager won't suppress any exceptions, and is thus redundant. Consider adding exceptions to the contextlib.suppress call, or removing the context manager entirely. Example import contextlib with contextlib.suppress(): foo() Use instead: import contextlib with contextlib.suppress(Exception): foo()",
    "option": {}
  },
  "B023": {
    "description": "[Status: stable] [Name: function-uses-loop-variable] [Message: Function definition does not bind loop variable <code>{name}</code>] What it does Checks for function definitions that use a loop variable. Why is this bad? The loop variable is not bound in the function definition, so it will always have the value it had in the last iteration when the function is called. Instead, consider using a default argument to bind the loop variable at function definition time. Or, use functools.partial. Example adders = [lambda x: x + i for i in range(3)] values = [adder(1) for adder in adders] # [3, 3, 3] Use instead: adders = [lambda x, i=i: x + i for i in range(3)] values = [adder(1) for adder in adders] # [1, 2, 3] Or: from functools import partial adders = [partial(lambda x, i: x + i, i=i) for i in range(3)] values = [adder(1) for adder in adders] # [1, 2, 3]",
    "option": {}
  },
  "B024": {
    "description": "[Status: stable] [Name: abstract-base-class-without-abstract-method] [Message: <code>{name}</code> is an abstract base class, but it has no abstract methods or properties] What it does Checks for abstract classes without abstract methods or properties. Annotated but unassigned class variables are regarded as abstract. Why is this bad? Abstract base classes are used to define interfaces. If an abstract base class has no abstract methods or properties, you may have forgotten to add an abstract method or property to the class, or omitted an @abstractmethod decorator. If the class is not meant to be used as an interface, consider removing the ABC base class from the class definition. Example from abc import ABC from typing import ClassVar class Foo(ABC): class_var: ClassVar[str] = \"assigned\" def method(self): bar() Use instead: from abc import ABC, abstractmethod from typing import ClassVar class Foo(ABC): class_var: ClassVar[str] # unassigned @abstractmethod def method(self): bar()",
    "option": {}
  },
  "B025": {
    "description": "[Status: stable] [Name: duplicate-try-block-exception] [Message: try-except* block with duplicate exception <code>{name}</code>] What it does Checks for try-except blocks with duplicate exception handlers. Why is this bad? Duplicate exception handlers are redundant, as the first handler will catch the exception, making the second handler unreachable. Example try: ... except ValueError: ... except ValueError: ... Use instead: try: ... except ValueError: ... Fix safety This rule's fix is marked as safe, unless the exception handler contains comments.",
    "option": {}
  },
  "B026": {
    "description": "[Status: stable] [Name: star-arg-unpacking-after-keyword-arg] [Message: Star-arg unpacking after a keyword argument is strongly discouraged] What it does Checks for function calls that use star-argument unpacking after providing a keyword argument Why is this bad? In Python, you can use star-argument unpacking to pass a list or tuple of arguments to a function. Providing a star-argument after a keyword argument can lead to confusing behavior, and is only supported for backwards compatibility. Example def foo(x, y, z): return x, y, z foo(1, 2, 3) # (1, 2, 3) foo(1, *[2, 3]) # (1, 2, 3) # foo(x=1, *[2, 3]) # TypeError # foo(y=2, *[1, 3]) # TypeError foo(z=3, *[1, 2]) # (1, 2, 3) # No error, but confusing! Use instead: def foo(x, y, z): return x, y, z foo(1, 2, 3) # (1, 2, 3) foo(x=1, y=2, z=3) # (1, 2, 3) foo(*[1, 2, 3]) # (1, 2, 3) foo(*[1, 2], 3) # (1, 2, 3)",
    "option": {}
  },
  "B027": {
    "description": "[Status: stable] [Name: empty-method-without-abstract-decorator] [Message: <code>{name}</code> is an empty method in an abstract base class, but has no abstract decorator] What it does Checks for empty methods in abstract base classes without an abstract decorator. Why is this bad? Empty methods in abstract base classes without an abstract decorator may be be indicative of a mistake. If the method is meant to be abstract, add an @abstractmethod decorator to the method. Example from abc import ABC class Foo(ABC): def method(self): ... Use instead: from abc import ABC, abstractmethod class Foo(ABC): @abstractmethod def method(self): ...",
    "option": {}
  },
  "B028": {
    "description": "[Status: stable] [Name: no-explicit-stacklevel] [Message: No explicit <code>stacklevel</code> keyword argument found] What it does Checks for warnings.warn calls without an explicit stacklevel keyword argument. Why is this bad? The warnings.warn method uses a stacklevel of 1 by default, which will output a stack frame of the line on which the \"warn\" method is called. Setting it to a higher number will output a stack frame from higher up the stack. It's recommended to use a stacklevel of 2 or higher, to give the caller more context about the warning. Example import warnings warnings.warn(\"This is a warning\") Use instead: import warnings warnings.warn(\"This is a warning\", stacklevel=2) Fix safety This rule's fix is marked as unsafe because it changes the behavior of the code. Moreover, the fix will assign a stacklevel of 2, while the user may wish to assign a higher stacklevel to address the diagnostic.",
    "option": {}
  },
  "B029": {
    "description": "[Status: stable] [Name: except-with-empty-tuple] [Message: Using <code>except* ():</code> with an empty tuple does not catch anything; add exceptions to handle] What it does Checks for exception handlers that catch an empty tuple. Why is this bad? An exception handler that catches an empty tuple will not catch anything, and is indicative of a mistake. Instead, add exceptions to the except clause. Example try: 1 / 0 except (): ... Use instead: try: 1 / 0 except ZeroDivisionError: ...",
    "option": {}
  },
  "B030": {
    "description": "[Status: stable] [Name: except-with-non-exception-classes] [Message: <code>except*</code> handlers should only be exception classes or tuples of exception classes] What it does Checks for exception handlers that catch non-exception classes. Why is this bad? Catching classes that do not inherit from BaseException will raise a TypeError. Example try: 1 / 0 except 1: ... Use instead: try: 1 / 0 except ZeroDivisionError: ...",
    "option": {}
  },
  "B031": {
    "description": "[Status: stable] [Name: reuse-of-groupby-generator] [Message: Using the generator returned from <code>itertools.groupby()</code> more than once will do nothing on the second usage] What it does Checks for multiple usage of the generator returned from itertools.groupby(). Why is this bad? Using the generator more than once will do nothing on the second usage. If that data is needed later, it should be stored as a list. Example: import itertools for name, group in itertools.groupby(data): for _ in range(5): do_something_with_the_group(group) Use instead: import itertools for name, group in itertools.groupby(data): values = list(group) for _ in range(5): do_something_with_the_group(values)",
    "option": {}
  },
  "B032": {
    "description": "[Status: stable] [Name: unintentional-type-annotation] [Message: Possible unintentional type annotation (using <code>:</code>). Did you mean to assign (using <code>=</code>)?] What it does Checks for the unintentional use of type annotations. Why is this bad? The use of a colon (:) in lieu of an assignment (=) can be syntactically valid, but is almost certainly a mistake when used in a subscript or attribute assignment. Example a[\"b\"]: 1 Use instead: a[\"b\"] = 1",
    "option": {}
  },
  "B033": {
    "description": "[Status: stable] [Name: duplicate-value] [Message: Sets should not contain duplicate item <code>{value}</code>] What it does Checks for set literals that contain duplicate items. Why is this bad? In Python, sets are unordered collections of unique elements. Including a duplicate item in a set literal is redundant, as the duplicate item will be replaced with a single item at runtime. Example {1, 2, 3, 1} Use instead: {1, 2, 3} Fix Safety This rule's fix is marked as unsafe if the replacement would remove comments attached to the original expression, potentially losing important context or documentation. For example: { 1, 2, # Comment 1, }",
    "option": {}
  },
  "B034": {
    "description": "[Status: stable] [Name: re-sub-positional-args] [Message: <code>{method}</code> should pass <code>{param_name}</code> and <code>flags</code> as keyword arguments to avoid confusion due to unintuitive argument positions] What it does Checks for calls to re.sub, re.subn, and re.split that pass count, maxsplit, or flags as positional arguments. Why is this bad? Passing count, maxsplit, or flags as positional arguments to re.sub, re.subn, or re.split can lead to confusion, as most methods in the re module accept flags as the third positional argument, while re.sub, re.subn, and re.split have different signatures. Instead, pass count, maxsplit, and flags as keyword arguments. Example import re re.split(\"pattern\", \"replacement\", 1) Use instead: import re re.split(\"pattern\", \"replacement\", maxsplit=1)",
    "option": {}
  },
  "B035": {
    "description": "[Status: stable] [Name: static-key-dict-comprehension] [Message: Dictionary comprehension uses static key: <code>{key}</code>] What it does Checks for dictionary comprehensions that use a static key, like a string literal or a variable defined outside the comprehension. Why is this bad? Using a static key (like a string literal) in a dictionary comprehension is usually a mistake, as it will result in a dictionary with only one key, despite the comprehension iterating over multiple values. Example data = [\"some\", \"Data\"] {\"key\": value.upper() for value in data} Use instead: data = [\"some\", \"Data\"] {value: value.upper() for value in data}",
    "option": {}
  },
  "B039": {
    "description": "[Status: stable] [Name: mutable-contextvar-default] [Message: Do not use mutable data structures for <code>ContextVar</code> defaults] What it does Checks for uses of mutable objects as ContextVar defaults. Why is this bad? The ContextVar default is evaluated once, when the ContextVar is defined. The same mutable object is then shared across all .get() method calls to the ContextVar. If the object is modified, those modifications will persist across calls, which can lead to unexpected behavior. Instead, prefer to use immutable data structures. Alternatively, take None as a default, and initialize a new mutable object inside for each call using the .set() method. Types outside the standard library can be marked as immutable with the lint.flake8-bugbear.extend-immutable-calls configuration option. Example from contextvars import ContextVar cv: ContextVar[list] = ContextVar(\"cv\", default=[]) Use instead: from contextvars import ContextVar cv: ContextVar[list | None] = ContextVar(\"cv\", default=None) ... if cv.get() is None: cv.set([])",
    "option": {
      "lint.flake8-bugbear.extend-immutable-calls": {
        "option_description": "Additional callable functions to consider \"immutable\" when evaluating, e.g., the\nfunction-call-in-default-argument rule (B008) or function-call-in-dataclass-defaults\nrule (RUF009).\n\nExpects to receive a list of fully-qualified names (e.g., fastapi.Query, rather than\nQuery).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-bugbear]\n# Allow default arguments like, e.g., `data: List[str] = fastapi.Query(None)`.\nextend-immutable-calls = [\"fastapi.Depends\", \"fastapi.Query\"]\n\n\n\n[lint.flake8-bugbear]\n# Allow default arguments like, e.g., `data: List[str] = fastapi.Query(None)`.\nextend-immutable-calls = [\"fastapi.Depends\", \"fastapi.Query\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "B901": {
    "description": "[Status: preview] [Name: return-in-generator] [Message: Using <code>yield</code> and <code>return {value}</code> in a generator function can lead to confusing behavior] What it does Checks for return {value} statements in functions that also contain yield or yield from statements. Why is this bad? Using return {value} in a generator function was syntactically invalid in Python 2. In Python 3 return {value} can be used in a generator; however, the combination of yield and return can lead to confusing behavior, as the return statement will cause the generator to raise StopIteration with the value provided, rather than returning the value to the caller. For example, given: from collections.abc import Iterable from pathlib import Path def get_file_paths(file_types: Iterable[str] | None = None) -> Iterable[Path]: dir_path = Path(\".\") if file_types is None: return dir_path.glob(\"*\") for file_type in file_types: yield from dir_path.glob(f\"*.{file_type}\") Readers might assume that get_file_paths() would return an iterable of Path objects in the directory; in reality, though, list(get_file_paths()) evaluates to [], since the return statement causes the generator to raise StopIteration with the value dir_path.glob(\"*\"): >>> list(get_file_paths(file_types=[\"cfg\", \"toml\"])) [PosixPath('setup.cfg'), PosixPath('pyproject.toml')] >>> list(get_file_paths()) [] For intentional uses of return in a generator, consider suppressing this diagnostic. Example from collections.abc import Iterable from pathlib import Path def get_file_paths(file_types: Iterable[str] | None = None) -> Iterable[Path]: dir_path = Path(\".\") if file_types is None: return dir_path.glob(\"*\") for file_type in file_types: yield from dir_path.glob(f\"*.{file_type}\") Use instead: from collections.abc import Iterable from pathlib import Path def get_file_paths(file_types: Iterable[str] | None = None) -> Iterable[Path]: dir_path = Path(\".\") if file_types is None: yield from dir_path.glob(\"*\") else: for file_type in file_types: yield from dir_path.glob(f\"*.{file_type}\")",
    "option": {}
  },
  "B903": {
    "description": "[Status: preview] [Name: class-as-data-structure] [Message: Class could be dataclass or namedtuple] What it does Checks for classes that only have a public __init__ method, without base classes and decorators. Why is this bad? Classes with just an __init__ are possibly better off being a dataclass or a namedtuple, which have less boilerplate. Example class Point: def __init__(self, x: float, y: float): self.x = x self.y = y Use instead: from dataclasses import dataclass @dataclass class Point: x: float y: float",
    "option": {}
  },
  "B904": {
    "description": "[Status: stable] [Name: raise-without-from-inside-except] [Message: Within an <code>except*</code> clause, raise exceptions with <code>raise ... from err</code> or <code>raise ... from None</code> to distinguish them from errors in exception handling] What it does Checks for raise statements in exception handlers that lack a from clause. Why is this bad? In Python, raise can be used with or without an exception from which the current exception is derived. This is known as exception chaining. When printing the stack trace, chained exceptions are displayed in such a way so as make it easier to trace the exception back to its root cause. When raising an exception from within an except clause, always include a from clause to facilitate exception chaining. If the exception is not chained, it will be difficult to trace the exception back to its root cause. Example try: ... except FileNotFoundError: if ...: raise RuntimeError(\"...\") else: raise UserWarning(\"...\") Use instead: try: ... except FileNotFoundError as exc: if ...: raise RuntimeError(\"...\") from None else: raise UserWarning(\"...\") from exc",
    "option": {}
  },
  "B905": {
    "description": "[Status: stable] [Name: zip-without-explicit-strict] [Message: <code>zip()</code> without an explicit <code>strict=</code> parameter] What it does Checks for zip calls without an explicit strict parameter when called with two or more iterables, or any starred argument. Why is this bad? By default, if the iterables passed to zip are of different lengths, the resulting iterator will be silently truncated to the length of the shortest iterable. This can lead to subtle bugs. Pass strict=True to raise a ValueError if the iterables are of non-uniform length. Alternatively, if the iterables are deliberately of different lengths, pass strict=False to make the intention explicit. Example zip(a, b) Use instead: zip(a, b, strict=True) Fix safety This rule's fix is marked as unsafe. While adding strict=False preserves the runtime behavior, it can obscure situations where the iterables are of unequal length. Ruff prefers to alert users so they can choose the intended behavior themselves.",
    "option": {}
  },
  "B909": {
    "description": "[Status: preview] [Name: loop-iterator-mutation] [Message: Mutation to loop iterable <code>{name}</code> during iteration] What it does Checks for mutations to an iterable during a loop iteration. Why is this bad? When iterating over an iterable, mutating the iterable can lead to unexpected behavior, like skipping elements or infinite loops. Example items = [1, 2, 3] for item in items: print(item) # Create an infinite loop by appending to the list. items.append(item)",
    "option": {}
  },
  "B911": {
    "description": "[Status: stable] [Name: batched-without-explicit-strict] [Message: <code>itertools.batched()</code> without an explicit <code>strict</code> parameter] What it does Checks for itertools.batched calls without an explicit strict parameter. Why is this bad? By default, if the length of the iterable is not divisible by the second argument to itertools.batched, the last batch will be shorter than the rest. In Python 3.13, a strict parameter was added which allows controlling if the batches must be of uniform length. Pass strict=True to raise a ValueError if the batches are of non-uniform length. Otherwise, pass strict=False to make the intention explicit. Example import itertools itertools.batched(iterable, n) Use instead if the batches must be of uniform length: import itertools itertools.batched(iterable, n, strict=True) Or if the batches can be of non-uniform length: import itertools itertools.batched(iterable, n, strict=False) Known deviations Unlike the upstream B911, this rule will not report infinite iterators (e.g., itertools.cycle(...)).",
    "option": {
      "target-version": {
        "option_description": "The minimum Python version to target, e.g., when considering automatic\ncode upgrades, like rewriting type annotations. Ruff will not propose\nchanges using features that are not available in the given version.\n\nFor example, to represent supporting Python >=3.11 or ==3.11\nspecify target-version = \"py311\".\n\nIf you're already using a pyproject.toml file, we recommend\nproject.requires-python instead, as it's based on Python packaging\nstandards, and will be respected by other tools. For example, Ruff\ntreats the following as identical to target-version = \"py38\":\n\n[project]\nrequires-python = \">=3.8\"\n\nIf both are specified, target-version takes precedence over\nrequires-python. See Inferring the Python version\nfor a complete description of how the target-version is determined\nwhen left unspecified.\n\nNote that a stub file can sometimes make use of a typing feature\nbefore it is available at runtime, as long as the stub does not make\nuse of new syntax. For example, a type checker will understand\nint | str in a stub as being a Union type annotation, even if the\ntype checker is run using Python 3.9, despite the fact that the |\noperator can only be used to create union types at runtime on Python\n3.10+. As such, Ruff will often recommend newer features in a stub\nfile than it would for an equivalent runtime file with the same target\nversion.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"\n\n\n\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"",
        "option_data_type": "\"py37\" | \"py38\" | \"py39\" | \"py310\" | \"py311\" | \"py312\" | \"py313\" | \"py314\"",
        "default_value": "\"py310\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "B912": {
    "description": "[Status: preview] [Name: map-without-explicit-strict] [Message: <code>map()</code> without an explicit <code>strict=</code> parameter] What it does Checks for map calls without an explicit strict parameter when called with two or more iterables, or any starred argument. This rule applies to Python 3.14 and later, where map accepts a strict keyword argument. For details, see: Whats New in Python 3.14. Why is this bad? By default, if the iterables passed to map are of different lengths, the resulting iterator will be silently truncated to the length of the shortest iterable. This can lead to subtle bugs. Pass strict=True to raise a ValueError if the iterables are of non-uniform length. Alternatively, if the iterables are deliberately of different lengths, pass strict=False to make the intention explicit. Example map(f, a, b) Use instead: map(f, a, b, strict=True) Fix safety This rule's fix is marked as unsafe. While adding strict=False preserves the runtime behavior, it can obscure situations where the iterables are of unequal length. Ruff prefers to alert users so they can choose the intended behavior themselves.",
    "option": {}
  },
  "A001": {
    "description": "[Status: stable] [Name: builtin-variable-shadowing] [Message: Variable <code>{name}</code> is shadowing a Python builtin] What it does Checks for variable (and function) assignments that use the same names as builtins. Why is this bad? Reusing a builtin name for the name of a variable increases the difficulty of reading and maintaining the code, and can cause non-obvious errors, as readers may mistake the variable for the builtin and vice versa. Builtins can be marked as exceptions to this rule via the lint.flake8-builtins.ignorelist configuration option. Example def find_max(list_of_lists): max = 0 for flat_list in list_of_lists: for value in flat_list: max = max(max, value) # TypeError: 'int' object is not callable return max Use instead: def find_max(list_of_lists): result = 0 for flat_list in list_of_lists: for value in flat_list: result = max(result, value) return result",
    "option": {
      "lint.flake8-builtins.ignorelist": {
        "option_description": "Ignore list of builtins.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-builtins]\nignorelist = [\"id\"]\n\n\n\n[lint.flake8-builtins]\nignorelist = [\"id\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "A002": {
    "description": "[Status: stable] [Name: builtin-argument-shadowing] [Message: Function argument <code>{name}</code> is shadowing a Python builtin] What it does Checks for function arguments that use the same names as builtins. Why is this bad? Reusing a builtin name for the name of an argument increases the difficulty of reading and maintaining the code, and can cause non-obvious errors, as readers may mistake the argument for the builtin and vice versa. Function definitions decorated with @override or @overload are exempt from this rule by default. Builtins can be marked as exceptions to this rule via the lint.flake8-builtins.ignorelist configuration option. Example def remove_duplicates(list, list2): result = set() for value in list: result.add(value) for value in list2: result.add(value) return list(result) # TypeError: 'list' object is not callable Use instead: def remove_duplicates(list1, list2): result = set() for value in list1: result.add(value) for value in list2: result.add(value) return list(result)",
    "option": {
      "lint.flake8-builtins.ignorelist": {
        "option_description": "Ignore list of builtins.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-builtins]\nignorelist = [\"id\"]\n\n\n\n[lint.flake8-builtins]\nignorelist = [\"id\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "A003": {
    "description": "[Status: stable] [Name: builtin-attribute-shadowing] [Message: Python builtin is shadowed by class attribute <code>{name}</code> from {row}] What it does Checks for class attributes and methods that use the same names as Python builtins. Why is this bad? Reusing a builtin name for the name of an attribute increases the difficulty of reading and maintaining the code, and can cause non-obvious errors, as readers may mistake the attribute for the builtin and vice versa. Since methods and class attributes typically cannot be referenced directly from outside the class scope, this rule only applies to those methods and attributes that both shadow a builtin and are referenced from within the class scope, as in the following example, where the list[int] return type annotation resolves to the list method, rather than the builtin: class Class: @staticmethod def list() -> None: pass @staticmethod def repeat(value: int, times: int) -> list[int]: return [value] * times Builtins can be marked as exceptions to this rule via the lint.flake8-builtins.ignorelist configuration option, or converted to the appropriate dunder method. Methods decorated with @typing.override or @typing_extensions.override are also ignored. Example class Class: @staticmethod def list() -> None: pass @staticmethod def repeat(value: int, times: int) -> list[int]: return [value] * times",
    "option": {
      "lint.flake8-builtins.ignorelist": {
        "option_description": "Ignore list of builtins.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-builtins]\nignorelist = [\"id\"]\n\n\n\n[lint.flake8-builtins]\nignorelist = [\"id\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "A004": {
    "description": "[Status: stable] [Name: builtin-import-shadowing] [Message: Import <code>{name}</code> is shadowing a Python builtin] What it does Checks for imports that use the same names as builtins. Why is this bad? Reusing a builtin for the name of an import increases the difficulty of reading and maintaining the code, and can cause non-obvious errors, as readers may mistake the variable for the builtin and vice versa. Builtins can be marked as exceptions to this rule via the lint.flake8-builtins.ignorelist configuration option. Example from rich import print print(\"Some message\") Use instead: from rich import print as rich_print rich_print(\"Some message\") or: import rich rich.print(\"Some message\")",
    "option": {
      "lint.flake8-builtins.ignorelist": {
        "option_description": "Ignore list of builtins.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-builtins]\nignorelist = [\"id\"]\n\n\n\n[lint.flake8-builtins]\nignorelist = [\"id\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      },
      "target-version": {
        "option_description": "The minimum Python version to target, e.g., when considering automatic\ncode upgrades, like rewriting type annotations. Ruff will not propose\nchanges using features that are not available in the given version.\n\nFor example, to represent supporting Python >=3.11 or ==3.11\nspecify target-version = \"py311\".\n\nIf you're already using a pyproject.toml file, we recommend\nproject.requires-python instead, as it's based on Python packaging\nstandards, and will be respected by other tools. For example, Ruff\ntreats the following as identical to target-version = \"py38\":\n\n[project]\nrequires-python = \">=3.8\"\n\nIf both are specified, target-version takes precedence over\nrequires-python. See Inferring the Python version\nfor a complete description of how the target-version is determined\nwhen left unspecified.\n\nNote that a stub file can sometimes make use of a typing feature\nbefore it is available at runtime, as long as the stub does not make\nuse of new syntax. For example, a type checker will understand\nint | str in a stub as being a Union type annotation, even if the\ntype checker is run using Python 3.9, despite the fact that the |\noperator can only be used to create union types at runtime on Python\n3.10+. As such, Ruff will often recommend newer features in a stub\nfile than it would for an equivalent runtime file with the same target\nversion.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"\n\n\n\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"",
        "option_data_type": "\"py37\" | \"py38\" | \"py39\" | \"py310\" | \"py311\" | \"py312\" | \"py313\" | \"py314\"",
        "default_value": "\"py310\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "A005": {
    "description": "[Status: stable] [Name: stdlib-module-shadowing] [Message: Module <code>{name}</code> shadows a Python standard-library module] What it does Checks for modules that use the same names as Python standard-library modules. Why is this bad? Reusing a standard-library module name for the name of a module increases the difficulty of reading and maintaining the code, and can cause non-obvious errors. Readers may mistake the first-party module for the standard-library module and vice versa. Standard-library modules can be marked as exceptions to this rule via the lint.flake8-builtins.allowed-modules configuration option. By default, the module path relative to the project root or src directories is considered, so a top-level logging.py or logging/__init__.py will clash with the builtin logging module, but utils/logging.py, for example, will not. With the lint.flake8-builtins.strict-checking option set to true, only the last component of the module name is considered, so logging.py, utils/logging.py, and utils/logging/__init__.py will all trigger the rule. This rule is not applied to stub files, as the name of a stub module is out of the control of the author of the stub file. Instead, a stub should aim to faithfully emulate the runtime module it is stubbing. As of Python 3.13, errors from modules that use the same name as standard-library modules now display a custom message. Example $ touch random.py $ python3 -c 'from random import choice' Traceback (most recent call last): File \"<string>\", line 1, in <module> from random import choice ImportError: cannot import name 'choice' from 'random' (consider renaming '/random.py' since it has the same name as the standard library module named 'random' and prevents importing that standard library module)",
    "option": {
      "lint.flake8-builtins.allowed-modules": {
        "option_description": "List of builtin module names to allow.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-builtins]\nallowed-modules = [\"secrets\"]\n\n\n\n[lint.flake8-builtins]\nallowed-modules = [\"secrets\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.flake8-builtins.strict-checking": {
        "option_description": "Compare module names instead of full module paths.\n\nUsed by A005 - stdlib-module-shadowing.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-builtins]\nstrict-checking = true\n\n\n\n[lint.flake8-builtins]\nstrict-checking = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "A006": {
    "description": "[Status: stable] [Name: builtin-lambda-argument-shadowing] [Message: Lambda argument <code>{name}</code> is shadowing a Python builtin] What it does Checks for lambda arguments that use the same names as Python builtins. Why is this bad? Reusing a builtin name for the name of a lambda argument increases the difficulty of reading and maintaining the code and can cause non-obvious errors. Readers may mistake the variable for the builtin, and vice versa. Builtins can be marked as exceptions to this rule via the lint.flake8-builtins.ignorelist configuration option.",
    "option": {
      "lint.flake8-builtins.ignorelist": {
        "option_description": "Ignore list of builtins.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-builtins]\nignorelist = [\"id\"]\n\n\n\n[lint.flake8-builtins]\nignorelist = [\"id\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "COM812": {
    "description": "[Status: stable] [Name: missing-trailing-comma] [Message: Trailing comma missing] What it does Checks for the absence of trailing commas. Why is this bad? The presence of a trailing comma can reduce diff size when parameters or elements are added or removed from function calls, function definitions, literals, etc. Example foo = { \"bar\": 1, \"baz\": 2 } Use instead: foo = { \"bar\": 1, \"baz\": 2, } Formatter compatibility We recommend against using this rule alongside the formatter. The formatter enforces consistent use of trailing commas, making the rule redundant.",
    "option": {}
  },
  "COM818": {
    "description": "[Status: stable] [Name: trailing-comma-on-bare-tuple] [Message: Trailing comma on bare tuple prohibited] What it does Checks for the presence of trailing commas on bare (i.e., unparenthesized) tuples. Why is this bad? The presence of a misplaced comma will cause Python to interpret the value as a tuple, which can lead to unexpected behaviour. Example import json foo = json.dumps({\"bar\": 1}), Use instead: import json foo = json.dumps({\"bar\": 1}) In the event that a tuple is intended, then use instead: import json foo = (json.dumps({\"bar\": 1}),)",
    "option": {}
  },
  "COM819": {
    "description": "[Status: stable] [Name: prohibited-trailing-comma] [Message: Trailing comma prohibited] What it does Checks for the presence of prohibited trailing commas. Why is this bad? Trailing commas are not essential in some cases and can therefore be viewed as unnecessary. Example foo = (1, 2, 3,) Use instead: foo = (1, 2, 3) Formatter compatibility We recommend against using this rule alongside the formatter. The formatter enforces consistent use of trailing commas, making the rule redundant.",
    "option": {}
  },
  "C400": {
    "description": "[Status: stable] [Name: unnecessary-generator-list] [Message: Unnecessary generator (rewrite using <code>list()</code>)] What it does Checks for unnecessary generators that can be rewritten as list comprehensions (or with list() directly). Why is this bad? It is unnecessary to use list() around a generator expression, since there are equivalent comprehensions for these types. Using a comprehension is clearer and more idiomatic. Further, if the comprehension can be removed entirely, as in the case of list(x for x in foo), it's better to use list(foo) directly, since it's even more direct. Example list(f(x) for x in foo) list(x for x in foo) list((x for x in foo)) Use instead: [f(x) for x in foo] list(foo) list(foo) Fix safety This rule's fix is marked as unsafe, as it may occasionally drop comments when rewriting the call. In most cases, though, comments will be preserved.",
    "option": {}
  },
  "C401": {
    "description": "[Status: stable] [Name: unnecessary-generator-set] [Message: Unnecessary generator (rewrite using <code>set()</code>)] What it does Checks for unnecessary generators that can be rewritten as set comprehensions (or with set() directly). Why is this bad? It is unnecessary to use set around a generator expression, since there are equivalent comprehensions for these types. Using a comprehension is clearer and more idiomatic. Further, if the comprehension can be removed entirely, as in the case of set(x for x in foo), it's better to use set(foo) directly, since it's even more direct. Example set(f(x) for x in foo) set(x for x in foo) set((x for x in foo)) Use instead: {f(x) for x in foo} set(foo) set(foo) Fix safety This rule's fix is marked as unsafe, as it may occasionally drop comments when rewriting the call. In most cases, though, comments will be preserved.",
    "option": {}
  },
  "C402": {
    "description": "[Status: stable] [Name: unnecessary-generator-dict] [Message: Unnecessary generator (rewrite as a dict comprehension)] What it does Checks for unnecessary generators that can be rewritten as dict comprehensions. Why is this bad? It is unnecessary to use dict() around a generator expression, since there are equivalent comprehensions for these types. Using a comprehension is clearer and more idiomatic. Example dict((x, f(x)) for x in foo) Use instead: {x: f(x) for x in foo} Fix safety This rule's fix is marked as unsafe, as it may occasionally drop comments when rewriting the call. In most cases, though, comments will be preserved.",
    "option": {}
  },
  "C403": {
    "description": "[Status: stable] [Name: unnecessary-list-comprehension-set] [Message: Unnecessary list comprehension (rewrite as a set comprehension)] What it does Checks for unnecessary list comprehensions. Why is this bad? It's unnecessary to use a list comprehension inside a call to set(), since there is an equivalent comprehension for this type. Example set([f(x) for x in foo]) Use instead: {f(x) for x in foo} Fix safety This rule's fix is marked as unsafe, as it may occasionally drop comments when rewriting the call. In most cases, though, comments will be preserved.",
    "option": {}
  },
  "C404": {
    "description": "[Status: stable] [Name: unnecessary-list-comprehension-dict] [Message: Unnecessary list comprehension (rewrite as a dict comprehension)] What it does Checks for unnecessary list comprehensions. Why is this bad? It's unnecessary to use a list comprehension inside a call to dict(), since there is an equivalent comprehension for this type. Example dict([(x, f(x)) for x in foo]) Use instead: {x: f(x) for x in foo} Fix safety This rule's fix is marked as unsafe, as it may occasionally drop comments when rewriting the call. In most cases, though, comments will be preserved.",
    "option": {}
  },
  "C405": {
    "description": "[Status: stable] [Name: unnecessary-literal-set] [Message: Unnecessary {kind} literal (rewrite as a set literal)] What it does Checks for set() calls that take unnecessary list or tuple literals as arguments. Why is this bad? It's unnecessary to use a list or tuple literal within a call to set(). Instead, the expression can be rewritten as a set literal. Example set([1, 2]) set((1, 2)) set([]) Use instead: {1, 2} {1, 2} set() Fix safety This rule's fix is marked as unsafe, as it may occasionally drop comments when rewriting the call. In most cases, though, comments will be preserved.",
    "option": {}
  },
  "C406": {
    "description": "[Status: stable] [Name: unnecessary-literal-dict] [Message: Unnecessary {obj_type} literal (rewrite as a dict literal)] What it does Checks for unnecessary list or tuple literals. Why is this bad? It's unnecessary to use a list or tuple literal within a call to dict(). It can be rewritten as a dict literal ({}). Example dict([(1, 2), (3, 4)]) dict(((1, 2), (3, 4))) dict([]) Use instead: {1: 2, 3: 4} {1: 2, 3: 4} {} Fix safety This rule's fix is marked as unsafe, as it may occasionally drop comments when rewriting the call. In most cases, though, comments will be preserved.",
    "option": {}
  },
  "C408": {
    "description": "[Status: stable] [Name: unnecessary-collection-call] [Message: Unnecessary <code>{kind}()</code> call (rewrite as a literal)] What it does Checks for unnecessary dict(), list() or tuple() calls that can be rewritten as empty literals. Why is this bad? It's unnecessary to call, e.g., dict() as opposed to using an empty literal ({}). The former is slower because the name dict must be looked up in the global scope in case it has been rebound. Example dict() dict(a=1, b=2) list() tuple() Use instead: {} {\"a\": 1, \"b\": 2} [] () Fix safety This rule's fix is marked as unsafe, as it may occasionally drop comments when rewriting the call. In most cases, though, comments will be preserved.",
    "option": {
      "lint.flake8-comprehensions.allow-dict-calls-with-keyword-arguments": {
        "option_description": "Allow dict calls that make use of keyword arguments (e.g., dict(a=1, b=2)).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-comprehensions]\nallow-dict-calls-with-keyword-arguments = true\n\n\n\n[lint.flake8-comprehensions]\nallow-dict-calls-with-keyword-arguments = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "C409": {
    "description": "[Status: stable] [Name: unnecessary-literal-within-tuple-call] [Message: Unnecessary list literal passed to <code>tuple()</code> (rewrite as a tuple literal)] What it does Checks for tuple calls that take unnecessary list or tuple literals as arguments. In preview, this also includes unnecessary list comprehensions within tuple calls. Why is this bad? It's unnecessary to use a list or tuple literal within a tuple() call, since there is a literal syntax for these types. If a list literal was passed, then it should be rewritten as a tuple literal. Otherwise, if a tuple literal was passed, then the outer call to tuple() should be removed. In preview, this rule also checks for list comprehensions within tuple() calls. If a list comprehension is found, it should be rewritten as a generator expression. Example tuple([1, 2]) tuple((1, 2)) tuple([x for x in range(10)]) Use instead: (1, 2) (1, 2) tuple(x for x in range(10)) Fix safety This rule's fix is marked as unsafe, as it may occasionally drop comments when rewriting the call. In most cases, though, comments will be preserved.",
    "option": {}
  },
  "C410": {
    "description": "[Status: stable] [Name: unnecessary-literal-within-list-call] [Message: Unnecessary list literal passed to <code>list()</code> (remove the outer call to <code>list()</code>)] What it does Checks for list() calls that take unnecessary list or tuple literals as arguments. Why is this bad? It's unnecessary to use a list or tuple literal within a list() call, since there is a literal syntax for these types. If a list literal is passed in, then the outer call to list() should be removed. Otherwise, if a tuple literal is passed in, then it should be rewritten as a list literal. Example list([1, 2]) list((1, 2)) Use instead: [1, 2] [1, 2] Fix safety This rule's fix is marked as unsafe, as it may occasionally drop comments when rewriting the call. In most cases, though, comments will be preserved.",
    "option": {}
  },
  "C411": {
    "description": "[Status: stable] [Name: unnecessary-list-call] [Message: Unnecessary <code>list()</code> call (remove the outer call to <code>list()</code>)] What it does Checks for unnecessary list() calls around list comprehensions. Why is this bad? It is redundant to use a list() call around a list comprehension. Example list([f(x) for x in foo]) Use instead [f(x) for x in foo] Fix safety This rule's fix is marked as unsafe, as it may occasionally drop comments when rewriting the call. In most cases, though, comments will be preserved.",
    "option": {}
  },
  "C413": {
    "description": "[Status: stable] [Name: unnecessary-call-around-sorted] [Message: Unnecessary <code>{func}()</code> call around <code>sorted()</code>] What it does Checks for unnecessary list() or reversed() calls around sorted() calls. Why is this bad? It is unnecessary to use list() around sorted(), as the latter already returns a list. It is also unnecessary to use reversed() around sorted(), as the latter has a reverse argument that can be used in lieu of an additional reversed() call. In both cases, it's clearer and more efficient to avoid the redundant call. Example reversed(sorted(iterable)) Use instead: sorted(iterable, reverse=True) Fix safety This rule's fix is marked as unsafe for reversed() cases, as reversed() and reverse=True will yield different results in the event of custom sort keys or equality functions. Specifically, reversed() will reverse the order of the collection, while sorted() with reverse=True will perform a stable reverse sort, which will preserve the order of elements that compare as equal. The fix is marked as safe for list() cases, as removing list() around sorted() does not change the behavior.",
    "option": {}
  },
  "C414": {
    "description": "[Status: stable] [Name: unnecessary-double-cast-or-process] [Message: Unnecessary <code>{inner}()</code> call within <code>{outer}()</code>] What it does Checks for unnecessary list(), reversed(), set(), sorted(), and tuple() call within list(), set(), sorted(), and tuple() calls. Why is this bad? It's unnecessary to double-cast or double-process iterables by wrapping the listed functions within an additional list(), set(), sorted(), or tuple() call. Doing so is redundant and can be confusing for readers. Example list(tuple(iterable)) Use instead: list(iterable) This rule applies to a variety of functions, including list(), reversed(), set(), sorted(), and tuple(). For example: Instead of list(list(iterable)), use list(iterable). Instead of list(tuple(iterable)), use list(iterable). Instead of tuple(list(iterable)), use tuple(iterable). Instead of tuple(tuple(iterable)), use tuple(iterable). Instead of set(set(iterable)), use set(iterable). Instead of set(list(iterable)), use set(iterable). Instead of set(tuple(iterable)), use set(iterable). Instead of set(sorted(iterable)), use set(iterable). Instead of set(reversed(iterable)), use set(iterable). Instead of sorted(list(iterable)), use sorted(iterable). Instead of sorted(tuple(iterable)), use sorted(iterable). Instead of sorted(sorted(iterable)), use sorted(iterable). Instead of sorted(reversed(iterable)), use sorted(iterable). Fix safety This rule's fix is marked as unsafe, as it may occasionally drop comments when rewriting the call. In most cases, though, comments will be preserved.",
    "option": {}
  },
  "C415": {
    "description": "[Status: stable] [Name: unnecessary-subscript-reversal] [Message: Unnecessary subscript reversal of iterable within <code>{func}()</code>] What it does Checks for unnecessary subscript reversal of iterable. Why is this bad? It's unnecessary to reverse the order of an iterable when passing it into reversed(), set() or sorted() functions as they will change the order of the elements again. Example sorted(iterable[::-1]) set(iterable[::-1]) reversed(iterable[::-1]) Use instead: sorted(iterable) set(iterable) iterable",
    "option": {}
  },
  "C416": {
    "description": "[Status: stable] [Name: unnecessary-comprehension] [Message: Unnecessary {kind} comprehension (rewrite using <code>{kind}()</code>)] What it does Checks for unnecessary dict, list, and set comprehension. Why is this bad? It's unnecessary to use a dict/list/set comprehension to build a data structure if the elements are unchanged. Wrap the iterable with dict(), list(), or set() instead. Example {a: b for a, b in iterable} [x for x in iterable] {x for x in iterable} Use instead: dict(iterable) list(iterable) set(iterable) Known problems This rule may produce false positives for dictionary comprehensions that iterate over a mapping. The dict constructor behaves differently depending on if it receives a sequence (e.g., a list) or a mapping (e.g., a dict). When a comprehension iterates over the keys of a mapping, replacing it with a dict() constructor call will give a different result. For example: >>> d1 = {(1, 2): 3, (4, 5): 6} >>> {x: y for x, y in d1} # Iterates over the keys of a mapping {1: 2, 4: 5} >>> dict(d1) # Ruff's incorrect suggested fix {(1, 2): 3, (4, 5): 6} >>> dict(d1.keys()) # Correct fix {1: 2, 4: 5} When the comprehension iterates over a sequence, Ruff's suggested fix is correct. However, Ruff cannot consistently infer if the iterable type is a sequence or a mapping and cannot suggest the correct fix for mappings. Fix safety Due to the known problem with dictionary comprehensions, this fix is marked as unsafe. Additionally, this fix may drop comments when rewriting the comprehension.",
    "option": {}
  },
  "C417": {
    "description": "[Status: stable] [Name: unnecessary-map] [Message: Unnecessary <code>map()</code> usage (rewrite using a {object_type})] What it does Checks for unnecessary map() calls with lambda functions. Why is this bad? Using map(func, iterable) when func is a lambda is slower than using a generator expression or a comprehension, as the latter approach avoids the function call overhead, in addition to being more readable. This rule also applies to map() calls within list(), set(), and dict() calls. For example: Instead of list(map(lambda num: num * 2, nums)), use [num * 2 for num in nums]. Instead of set(map(lambda num: num % 2 == 0, nums)), use {num % 2 == 0 for num in nums}. Instead of dict(map(lambda v: (v, v ** 2), values)), use {v: v ** 2 for v in values}. Example map(lambda x: x + 1, iterable) Use instead: (x + 1 for x in iterable) Fix safety This rule's fix is marked as unsafe, as it may occasionally drop comments when rewriting the call. In most cases, though, comments will be preserved.",
    "option": {}
  },
  "C418": {
    "description": "[Status: stable] [Name: unnecessary-literal-within-dict-call] [Message: Unnecessary dict {kind} passed to <code>dict()</code> (remove the outer call to <code>dict()</code>)] What it does Checks for dict() calls that take unnecessary dict literals or dict comprehensions as arguments. Why is this bad? It's unnecessary to wrap a dict literal or comprehension within a dict() call, since the literal or comprehension syntax already returns a dictionary. Example dict({}) dict({\"a\": 1}) Use instead: {} {\"a\": 1} Fix safety This rule's fix is marked as unsafe, as it may occasionally drop comments when rewriting the call. In most cases, though, comments will be preserved.",
    "option": {}
  },
  "C419": {
    "description": "[Status: stable] [Name: unnecessary-comprehension-in-call] [Message: Unnecessary list comprehension] What it does Checks for unnecessary list or set comprehensions passed to builtin functions that take an iterable. Set comprehensions are only a violation in the case where the builtin function does not care about duplication of elements in the passed iterable. Why is this bad? Many builtin functions (this rule currently covers any and all in stable, along with min, max, and sum in preview) accept any iterable, including a generator. Constructing a temporary list via list comprehension is unnecessary and wastes memory for large iterables. any and all can also short-circuit iteration, saving a lot of time. The unnecessary comprehension forces a full iteration of the input iterable, giving up the benefits of short-circuiting. For example, compare the performance of all with a list comprehension against that of a generator in a case where an early short-circuit is possible (almost 40x faster): In [1]: %timeit all([i for i in range(1000)]) 8.14 s  25.4 ns per loop (mean  std. dev. of 7 runs, 100,000 loops each) In [2]: %timeit all(i for i in range(1000)) 212 ns  0.892 ns per loop (mean  std. dev. of 7 runs, 1,000,000 loops each) This performance improvement is due to short-circuiting. If the entire iterable has to be traversed, the comprehension version may even be a bit faster: list allocation overhead is not necessarily greater than generator overhead. Applying this rule simplifies the code and will usually save memory, but in the absence of short-circuiting it may not improve performance. (It may even slightly regress performance, though the difference will usually be small.) Example any([x.id for x in bar]) all([x.id for x in bar]) sum([x.val for x in bar]) min([x.val for x in bar]) max([x.val for x in bar]) Use instead: any(x.id for x in bar) all(x.id for x in bar) sum(x.val for x in bar) min(x.val for x in bar) max(x.val for x in bar) Fix safety This rule's fix is marked as unsafe, as it can change the behavior of the code if the iteration has side effects (due to laziness and short-circuiting). The fix may also drop comments when rewriting some comprehensions.",
    "option": {}
  },
  "C420": {
    "description": "[Status: stable] [Name: unnecessary-dict-comprehension-for-iterable] [Message: Unnecessary dict comprehension for iterable; use <code>dict.fromkeys</code> instead] What it does Checks for unnecessary dict comprehension when creating a dictionary from an iterable. Why is this bad? It's unnecessary to use a dict comprehension to build a dictionary from an iterable when the value is static. Prefer dict.fromkeys(iterable) over {value: None for value in iterable}, as dict.fromkeys is more readable and efficient. Example {a: None for a in iterable} {a: 1 for a in iterable} Use instead: dict.fromkeys(iterable) dict.fromkeys(iterable, 1) Fix safety This rule's fix is marked as unsafe if there's comments inside the dict comprehension, as comments may be removed. For example, the fix would be marked as unsafe in the following case: { # comment 1 a: # comment 2 None # comment 3 for a in iterable # comment 4 }",
    "option": {}
  },
  "CPY001": {
    "description": "[Status: preview] [Name: missing-copyright-notice] [Message: Missing copyright notice at top of file] What it does Checks for the absence of copyright notices within Python files. Note that this check only searches within the first 4096 bytes of the file. Why is this bad? In some codebases, it's common to have a license header at the top of every file. This rule ensures that the license header is present.",
    "option": {
      "lint.flake8-copyright.author": {
        "option_description": "Author to enforce within the copyright notice. If provided, the\nauthor must be present immediately following the copyright notice.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-copyright]\nauthor = \"Ruff\"\n\n\n\n[lint.flake8-copyright]\nauthor = \"Ruff\"",
        "option_data_type": "str",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.flake8-copyright.min-file-size": {
        "option_description": "A minimum file size (in bytes) required for a copyright notice to\nbe enforced. By default, all files are validated.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-copyright]\n# Avoid enforcing a header on files smaller than 1024 bytes.\nmin-file-size = 1024\n\n\n\n[lint.flake8-copyright]\n# Avoid enforcing a header on files smaller than 1024 bytes.\nmin-file-size = 1024",
        "option_data_type": "int",
        "default_value": "0",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.flake8-copyright.notice-rgx": {
        "option_description": "The regular expression used to match the copyright notice, compiled\nwith the regex crate.\nDefaults to (?i)Copyright\\s+((?:\\(C\\)|)\\s+)?\\d{4}((-|,\\s)\\d{4})*, which matches\nthe following:\n\nCopyright 2023\nCopyright (C) 2023\nCopyright 2021-2023\nCopyright (C) 2021-2023\nCopyright (C) 2021, 2023\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-copyright]\nnotice-rgx = \"(?i)Copyright \\\\(C\\\\) \\\\d{4}\"\n\n\n\n[lint.flake8-copyright]\nnotice-rgx = \"(?i)Copyright \\\\(C\\\\) \\\\d{4}\"",
        "option_data_type": "str",
        "default_value": "\"(?i)Copyright\\s+((?:\\(C\\)|)\\s+)?\\d{4}((-|,\\s)\\d{4})*\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "DTZ001": {
    "description": "[Status: stable] [Name: call-datetime-without-tzinfo] [Message: <code>datetime.datetime()</code> called without a <code>tzinfo</code> argument] What it does Checks for datetime instantiations that do not specify a timezone. Why is this bad? datetime objects are \"naive\" by default, in that they do not include timezone information. \"Naive\" objects are easy to understand, but ignore some aspects of reality, which can lead to subtle bugs. Timezone-aware datetime objects are preferred, as they represent a specific moment in time, unlike \"naive\" objects. By providing a non-None value for tzinfo, a datetime can be made timezone-aware. Example import datetime datetime.datetime(2000, 1, 1, 0, 0, 0) Use instead: import datetime datetime.datetime(2000, 1, 1, 0, 0, 0, tzinfo=datetime.timezone.utc) Or, on Python 3.11 and later: import datetime datetime.datetime(2000, 1, 1, 0, 0, 0, tzinfo=datetime.UTC)",
    "option": {}
  },
  "DTZ002": {
    "description": "[Status: stable] [Name: call-datetime-today] [Message: <code>datetime.datetime.today()</code> used] What it does Checks for usage of datetime.datetime.today(). Why is this bad? datetime objects are \"naive\" by default, in that they do not include timezone information. \"Naive\" objects are easy to understand, but ignore some aspects of reality, which can lead to subtle bugs. Timezone-aware datetime objects are preferred, as they represent a specific moment in time, unlike \"naive\" objects. datetime.datetime.today() creates a \"naive\" object; instead, use datetime.datetime.now(tz=...) to create a timezone-aware object. Example import datetime datetime.datetime.today() Use instead: import datetime datetime.datetime.now(tz=datetime.timezone.utc) Or, for Python 3.11 and later: import datetime datetime.datetime.now(tz=datetime.UTC)",
    "option": {}
  },
  "DTZ003": {
    "description": "[Status: stable] [Name: call-datetime-utcnow] [Message: <code>datetime.datetime.utcnow()</code> used] What it does Checks for usage of datetime.datetime.utcnow(). Why is this bad? Python datetime objects can be naive or timezone-aware. While an aware object represents a specific moment in time, a naive object does not contain enough information to unambiguously locate itself relative to other datetime objects. Since this can lead to errors, it is recommended to always use timezone-aware objects. datetime.datetime.utcnow() returns a naive datetime object; instead, use datetime.datetime.now(tz=...) to create a timezone-aware object. Example import datetime datetime.datetime.utcnow() Use instead: import datetime datetime.datetime.now(tz=datetime.timezone.utc) Or, for Python 3.11 and later: import datetime datetime.datetime.now(tz=datetime.UTC)",
    "option": {}
  },
  "DTZ004": {
    "description": "[Status: stable] [Name: call-datetime-utcfromtimestamp] [Message: <code>datetime.datetime.utcfromtimestamp()</code> used] What it does Checks for usage of datetime.datetime.utcfromtimestamp(). Why is this bad? Python datetime objects can be naive or timezone-aware. While an aware object represents a specific moment in time, a naive object does not contain enough information to unambiguously locate itself relative to other datetime objects. Since this can lead to errors, it is recommended to always use timezone-aware objects. datetime.datetime.utcfromtimestamp() returns a naive datetime object; instead, use datetime.datetime.fromtimestamp(ts, tz=...) to create a timezone-aware object. Example import datetime datetime.datetime.utcfromtimestamp(946684800) Use instead: import datetime datetime.datetime.fromtimestamp(946684800, tz=datetime.timezone.utc) Or, on Python 3.11 and later: import datetime datetime.datetime.fromtimestamp(946684800, tz=datetime.UTC)",
    "option": {}
  },
  "DTZ005": {
    "description": "[Status: stable] [Name: call-datetime-now-without-tzinfo] [Message: <code>datetime.datetime.now()</code> called without a <code>tz</code> argument] What it does Checks for usages of datetime.datetime.now() that do not specify a timezone. Why is this bad? Python datetime objects can be naive or timezone-aware. While an aware object represents a specific moment in time, a naive object does not contain enough information to unambiguously locate itself relative to other datetime objects. Since this can lead to errors, it is recommended to always use timezone-aware objects. datetime.datetime.now() or datetime.datetime.now(tz=None) returns a naive datetime object. Instead, use datetime.datetime.now(tz=<timezone>) to create a timezone-aware object. Example import datetime datetime.datetime.now() Use instead: import datetime datetime.datetime.now(tz=datetime.timezone.utc) Or, for Python 3.11 and later: import datetime datetime.datetime.now(tz=datetime.UTC)",
    "option": {}
  },
  "DTZ006": {
    "description": "[Status: stable] [Name: call-datetime-fromtimestamp] [Message: <code>datetime.datetime.fromtimestamp()</code> called without a <code>tz</code> argument] What it does Checks for usage of datetime.datetime.fromtimestamp() that do not specify a timezone. Why is this bad? Python datetime objects can be naive or timezone-aware. While an aware object represents a specific moment in time, a naive object does not contain enough information to unambiguously locate itself relative to other datetime objects. Since this can lead to errors, it is recommended to always use timezone-aware objects. datetime.datetime.fromtimestamp(ts) or datetime.datetime.fromtimestampe(ts, tz=None) returns a naive datetime object. Instead, use datetime.datetime.fromtimestamp(ts, tz=<timezone>) to create a timezone-aware object. Example import datetime datetime.datetime.fromtimestamp(946684800) Use instead: import datetime datetime.datetime.fromtimestamp(946684800, tz=datetime.timezone.utc) Or, on Python 3.11 and later: import datetime datetime.datetime.fromtimestamp(946684800, tz=datetime.UTC)",
    "option": {}
  },
  "DTZ007": {
    "description": "[Status: stable] [Name: call-datetime-strptime-without-zone] [Message: Naive datetime constructed using <code>datetime.datetime.strptime()</code> without %z] What it does Checks for uses of datetime.datetime.strptime() that lead to naive datetime objects. Why is this bad? Python datetime objects can be naive or timezone-aware. While an aware object represents a specific moment in time, a naive object does not contain enough information to unambiguously locate itself relative to other datetime objects. Since this can lead to errors, it is recommended to always use timezone-aware objects. datetime.datetime.strptime() without %z returns a naive datetime object. Follow it with .replace(tzinfo=<timezone>) or .astimezone(). Example import datetime datetime.datetime.strptime(\"2022/01/31\", \"%Y/%m/%d\") Instead, use .replace(tzinfo=<timezone>): import datetime datetime.datetime.strptime(\"2022/01/31\", \"%Y/%m/%d\").replace( tzinfo=datetime.timezone.utc ) Or, use .astimezone(): import datetime datetime.datetime.strptime(\"2022/01/31\", \"%Y/%m/%d\").astimezone(datetime.timezone.utc) On Python 3.11 and later, datetime.timezone.utc can be replaced with datetime.UTC.",
    "option": {}
  },
  "DTZ011": {
    "description": "[Status: stable] [Name: call-date-today] [Message: <code>datetime.date.today()</code> used] What it does Checks for usage of datetime.date.today(). Why is this bad? Python date objects are naive, that is, not timezone-aware. While an aware object represents a specific moment in time, a naive object does not contain enough information to unambiguously locate itself relative to other datetime objects. Since this can lead to errors, it is recommended to always use timezone-aware objects. datetime.date.today returns a naive date object without taking timezones into account. Instead, use datetime.datetime.now(tz=...).date() to create a timezone-aware object and retrieve its date component. Example import datetime datetime.date.today() Use instead: import datetime datetime.datetime.now(tz=datetime.timezone.utc).date() Or, for Python 3.11 and later: import datetime datetime.datetime.now(tz=datetime.UTC).date()",
    "option": {}
  },
  "DTZ012": {
    "description": "[Status: stable] [Name: call-date-fromtimestamp] [Message: <code>datetime.date.fromtimestamp()</code> used] What it does Checks for usage of datetime.date.fromtimestamp(). Why is this bad? Python date objects are naive, that is, not timezone-aware. While an aware object represents a specific moment in time, a naive object does not contain enough information to unambiguously locate itself relative to other datetime objects. Since this can lead to errors, it is recommended to always use timezone-aware objects. datetime.date.fromtimestamp(ts) returns a naive date object. Instead, use datetime.datetime.fromtimestamp(ts, tz=...).date() to create a timezone-aware datetime object and retrieve its date component. Example import datetime datetime.date.fromtimestamp(946684800) Use instead: import datetime datetime.datetime.fromtimestamp(946684800, tz=datetime.timezone.utc).date() Or, for Python 3.11 and later: import datetime datetime.datetime.fromtimestamp(946684800, tz=datetime.UTC).date()",
    "option": {}
  },
  "DTZ901": {
    "description": "[Status: stable] [Name: datetime-min-max] [Message: Use of <code>datetime.datetime.{min_max}</code> without timezone information] What it does Checks for uses of datetime.datetime.min and datetime.datetime.max. Why is this bad? datetime.min and datetime.max are non-timezone-aware datetime objects. As such, operations on datetime.min and datetime.max may behave unexpectedly, as in: import datetime # Timezone: UTC-14 datetime.datetime.min.timestamp() # ValueError: year 0 is out of range datetime.datetime.max.timestamp() # ValueError: year 10000 is out of range Example import datetime datetime.datetime.max Use instead: import datetime datetime.datetime.max.replace(tzinfo=datetime.UTC)",
    "option": {}
  },
  "T100": {
    "description": "[Status: stable] [Name: debugger] [Message: Trace found: <code>{name}</code> used] What it does Checks for the presence of debugger calls and imports. Why is this bad? Debugger calls and imports should be used for debugging purposes only. The presence of a debugger call or import in production code is likely a mistake and may cause unintended behavior, such as exposing sensitive information or causing the program to hang. Instead, consider using a logging library to log information about the program's state, and writing tests to verify that the program behaves as expected. Example def foo(): breakpoint()",
    "option": {}
  },
  "DJ001": {
    "description": "[Status: stable] [Name: django-nullable-model-string-field] [Message: Avoid using <code>null=True</code> on string-based fields such as <code>{field_name}</code>] What it does Checks nullable string-based fields (like CharField and TextField) in Django models. Why is this bad? If a string-based field is nullable, then your model will have two possible representations for \"no data\": None and the empty string. This can lead to confusion, as clients of the API have to check for both None and the empty string when trying to determine if the field has data. The Django convention is to use the empty string in lieu of None for string-based fields. Example from django.db import models class MyModel(models.Model): field = models.CharField(max_length=255, null=True) Use instead: from django.db import models class MyModel(models.Model): field = models.CharField(max_length=255, default=\"\")",
    "option": {}
  },
  "DJ003": {
    "description": "[Status: stable] [Name: django-locals-in-render-function] [Message: Avoid passing <code>locals()</code> as context to a <code>render</code> function] What it does Checks for the use of locals() in render functions. Why is this bad? Using locals() can expose internal variables or other unintentional data to the rendered template. Example from django.shortcuts import render def index(request): posts = Post.objects.all() return render(request, \"app/index.html\", locals()) Use instead: from django.shortcuts import render def index(request): posts = Post.objects.all() context = {\"posts\": posts} return render(request, \"app/index.html\", context)",
    "option": {}
  },
  "DJ006": {
    "description": "[Status: stable] [Name: django-exclude-with-model-form] [Message: Do not use <code>exclude</code> with <code>ModelForm</code>, use <code>fields</code> instead] What it does Checks for the use of exclude in Django ModelForm classes. Why is this bad? If a ModelForm includes the exclude attribute, any new field that is added to the model will automatically be exposed for modification. Example from django.forms import ModelForm class PostForm(ModelForm): class Meta: model = Post exclude = [\"author\"] Use instead: from django.forms import ModelForm class PostForm(ModelForm): class Meta: model = Post fields = [\"title\", \"content\"]",
    "option": {}
  },
  "DJ007": {
    "description": "[Status: stable] [Name: django-all-with-model-form] [Message: Do not use <code>__all__</code> with <code>ModelForm</code>, use <code>fields</code> instead] What it does Checks for the use of fields = \"__all__\" in Django ModelForm classes. Why is this bad? If a ModelForm includes the fields = \"__all__\" attribute, any new field that is added to the model will automatically be exposed for modification. Example from django.forms import ModelForm class PostForm(ModelForm): class Meta: model = Post fields = \"__all__\" Use instead: from django.forms import ModelForm class PostForm(ModelForm): class Meta: model = Post fields = [\"title\", \"content\"]",
    "option": {}
  },
  "DJ008": {
    "description": "[Status: stable] [Name: django-model-without-dunder-str] [Message: Model does not define <code>__str__</code> method] What it does Checks that a __str__ method is defined in Django models. Why is this bad? Django models should define a __str__ method to return a string representation of the model instance, as Django calls this method to display the object in the Django Admin and elsewhere. Models without a __str__ method will display a non-meaningful representation of the object in the Django Admin. Example from django.db import models class MyModel(models.Model): field = models.CharField(max_length=255) Use instead: from django.db import models class MyModel(models.Model): field = models.CharField(max_length=255) def __str__(self): return f\"{self.field}\"",
    "option": {}
  },
  "DJ012": {
    "description": "[Status: stable] [Name: django-unordered-body-content-in-model] [Message: Order of model's inner classes, methods, and fields does not follow the Django Style Guide: {element_type} should come before {prev_element_type}] What it does Checks for the order of Model's inner classes, methods, and fields as per the Django Style Guide. Why is this bad? The Django Style Guide specifies that the order of Model inner classes, attributes and methods should be as follows: All database fields Custom manager attributes class Meta def __str__() def save() def get_absolute_url() Any custom methods Example from django.db import models class StrBeforeFieldModel(models.Model): class Meta: verbose_name = \"test\" verbose_name_plural = \"tests\" def __str__(self): return \"foobar\" first_name = models.CharField(max_length=32) last_name = models.CharField(max_length=40) Use instead: from django.db import models class StrBeforeFieldModel(models.Model): first_name = models.CharField(max_length=32) last_name = models.CharField(max_length=40) class Meta: verbose_name = \"test\" verbose_name_plural = \"tests\" def __str__(self): return \"foobar\"",
    "option": {}
  },
  "DJ013": {
    "description": "[Status: stable] [Name: django-non-leading-receiver-decorator] [Message: <code>@receiver</code> decorator must be on top of all the other decorators] What it does Checks that Django's @receiver decorator is listed first, prior to any other decorators. Why is this bad? Django's @receiver decorator is special in that it does not return a wrapped function. Rather, @receiver connects the decorated function to a signal. If any other decorators are listed before @receiver, the decorated function will not be connected to the signal. Example from django.dispatch import receiver from django.db.models.signals import post_save @transaction.atomic @receiver(post_save, sender=MyModel) def my_handler(sender, instance, created, **kwargs): pass Use instead: from django.dispatch import receiver from django.db.models.signals import post_save @receiver(post_save, sender=MyModel) @transaction.atomic def my_handler(sender, instance, created, **kwargs): pass",
    "option": {}
  },
  "EM101": {
    "description": "[Status: stable] [Name: raw-string-in-exception] [Message: Exception must not use a string literal, assign to variable first] What it does Checks for the use of string literals in exception constructors. Why is this bad? Python includes the raise in the default traceback (and formatters like Rich and IPython do too). By using a string literal, the error message will be duplicated in the traceback, which can make the traceback less readable. Example Given: raise RuntimeError(\"'Some value' is incorrect\") Python will produce a traceback like: Traceback (most recent call last): File \"tmp.py\", line 2, in <module> raise RuntimeError(\"'Some value' is incorrect\") RuntimeError: 'Some value' is incorrect Instead, assign the string to a variable: msg = \"'Some value' is incorrect\" raise RuntimeError(msg) Which will produce a traceback like: Traceback (most recent call last): File \"tmp.py\", line 3, in <module> raise RuntimeError(msg) RuntimeError: 'Some value' is incorrect",
    "option": {
      "lint.flake8-errmsg.max-string-length": {
        "option_description": "Maximum string length for string literals in exception messages.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-errmsg]\nmax-string-length = 20\n\n\n\n[lint.flake8-errmsg]\nmax-string-length = 20",
        "option_data_type": "int",
        "default_value": "0",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "EM102": {
    "description": "[Status: stable] [Name: f-string-in-exception] [Message: Exception must not use an f-string literal, assign to variable first] What it does Checks for the use of f-strings in exception constructors. Why is this bad? Python includes the raise in the default traceback (and formatters like Rich and IPython do too). By using an f-string, the error message will be duplicated in the traceback, which can make the traceback less readable. Example Given: sub = \"Some value\" raise RuntimeError(f\"{sub!r} is incorrect\") Python will produce a traceback like: Traceback (most recent call last): File \"tmp.py\", line 2, in <module> raise RuntimeError(f\"{sub!r} is incorrect\") RuntimeError: 'Some value' is incorrect Instead, assign the string to a variable: sub = \"Some value\" msg = f\"{sub!r} is incorrect\" raise RuntimeError(msg) Which will produce a traceback like: Traceback (most recent call last): File \"tmp.py\", line 3, in <module> raise RuntimeError(msg) RuntimeError: 'Some value' is incorrect",
    "option": {}
  },
  "EM103": {
    "description": "[Status: stable] [Name: dot-format-in-exception] [Message: Exception must not use a <code>.format()</code> string directly, assign to variable first] What it does Checks for the use of .format calls on string literals in exception constructors. Why is this bad? Python includes the raise in the default traceback (and formatters like Rich and IPython do too). By using a .format call, the error message will be duplicated in the traceback, which can make the traceback less readable. Example Given: sub = \"Some value\" raise RuntimeError(\"'{}' is incorrect\".format(sub)) Python will produce a traceback like: Traceback (most recent call last): File \"tmp.py\", line 2, in <module> raise RuntimeError(\"'{}' is incorrect\".format(sub)) RuntimeError: 'Some value' is incorrect Instead, assign the string to a variable: sub = \"Some value\" msg = \"'{}' is incorrect\".format(sub) raise RuntimeError(msg) Which will produce a traceback like: Traceback (most recent call last): File \"tmp.py\", line 3, in <module> raise RuntimeError(msg) RuntimeError: 'Some value' is incorrect",
    "option": {}
  },
  "EXE001": {
    "description": "[Status: stable] [Name: shebang-not-executable] [Message: Shebang is present but file is not executable] What it does Checks for a shebang directive in a file that is not executable. Why is this bad? In Python, a shebang (also known as a hashbang) is the first line of a script, which specifies the interpreter that should be used to run the script. The presence of a shebang suggests that a file is intended to be executable. If a file contains a shebang but is not executable, then the shebang is misleading, or the file is missing the executable bit. If the file is meant to be executable, add the executable bit to the file (e.g., chmod +x __main__.py or git update-index --chmod=+x __main__.py). Otherwise, remove the shebang. A file is considered executable if it has the executable bit set (i.e., its permissions mode intersects with 0o111). As such, this rule is only available on Unix-like systems, and is not enforced on Windows or WSL. Example #!/usr/bin/env python",
    "option": {}
  },
  "EXE002": {
    "description": "[Status: stable] [Name: shebang-missing-executable-file] [Message: The file is executable but no shebang is present] What it does Checks for executable .py files that do not have a shebang. Why is this bad? In Python, a shebang (also known as a hashbang) is the first line of a script, which specifies the interpreter that should be used to run the script. If a .py file is executable, but does not have a shebang, it may be run with the wrong interpreter, or fail to run at all. If the file is meant to be executable, add a shebang, as in: #!/usr/bin/env python Otherwise, remove the executable bit from the file (e.g., chmod -x __main__.py or git update-index --chmod=-x __main__.py). A file is considered executable if it has the executable bit set (i.e., its permissions mode intersects with 0o111). As such, this rule is only available on Unix-like systems, and is not enforced on Windows or WSL.",
    "option": {}
  },
  "EXE003": {
    "description": "[Status: stable] [Name: shebang-missing-python] [Message: Shebang should contain <code>python</code>, <code>pytest</code>, or <code>uv run</code>] What it does Checks for a shebang directive in .py files that does not contain python, pytest, or uv run. Why is this bad? In Python, a shebang (also known as a hashbang) is the first line of a script, which specifies the command that should be used to run the script. For Python scripts, if the shebang does not include a command that explicitly or implicitly specifies an interpreter, then the file will be executed with the default interpreter, which is likely a mistake. Example #!/usr/bin/env bash Use instead: #!/usr/bin/env python3",
    "option": {}
  },
  "EXE004": {
    "description": "[Status: stable] [Name: shebang-leading-whitespace] [Message: Avoid whitespace before shebang] What it does Checks for whitespace before a shebang directive. Why is this bad? In Python, a shebang (also known as a hashbang) is the first line of a script, which specifies the interpreter that should be used to run the script. The shebang's #! prefix must be the first two characters of a file. The presence of whitespace before the shebang will cause the shebang to be ignored, which is likely a mistake. Example #!/usr/bin/env python3 Use instead: #!/usr/bin/env python3",
    "option": {}
  },
  "EXE005": {
    "description": "[Status: stable] [Name: shebang-not-first-line] [Message: Shebang should be at the beginning of the file] What it does Checks for a shebang directive that is not at the beginning of the file. Why is this bad? In Python, a shebang (also known as a hashbang) is the first line of a script, which specifies the interpreter that should be used to run the script. The shebang's #! prefix must be the first two characters of a file. If the shebang is not at the beginning of the file, it will be ignored, which is likely a mistake. Example foo = 1 #!/usr/bin/env python3 Use instead: #!/usr/bin/env python3 foo = 1",
    "option": {}
  },
  "FIX001": {
    "description": "[Status: stable] [Name: line-contains-fixme] [Message: Line contains FIXME, consider resolving the issue] What it does Checks for \"FIXME\" comments. Why is this bad? \"FIXME\" comments are used to describe an issue that should be resolved (usually, a bug or unexpected behavior). Consider resolving the issue before deploying the code. Note that if you use \"FIXME\" comments as a form of documentation, this rule may not be appropriate for your project. Example def speed(distance, time): return distance / time # FIXME: Raises ZeroDivisionError for time = 0.",
    "option": {}
  },
  "FIX002": {
    "description": "[Status: stable] [Name: line-contains-todo] [Message: Line contains TODO, consider resolving the issue] What it does Checks for \"TODO\" comments. Why is this bad? \"TODO\" comments are used to describe an issue that should be resolved (usually, a missing feature, optimization, or refactoring opportunity). Consider resolving the issue before deploying the code. Note that if you use \"TODO\" comments as a form of documentation (e.g., to provide context for future work), this rule may not be appropriate for your project. Example def greet(name): return f\"Hello, {name}!\" # TODO: Add support for custom greetings.",
    "option": {}
  },
  "FIX003": {
    "description": "[Status: stable] [Name: line-contains-xxx] [Message: Line contains XXX, consider resolving the issue] What it does Checks for \"XXX\" comments. Why is this bad? \"XXX\" comments are used to describe an issue that should be resolved. Consider resolving the issue before deploying the code, or, at minimum, using a more descriptive comment tag (e.g, \"TODO\"). Example def speed(distance, time): return distance / time # XXX: Raises ZeroDivisionError for time = 0.",
    "option": {}
  },
  "FIX004": {
    "description": "[Status: stable] [Name: line-contains-hack] [Message: Line contains HACK, consider resolving the issue] What it does Checks for \"HACK\" comments. Why is this bad? \"HACK\" comments are used to describe an issue that should be resolved (usually, a suboptimal solution or temporary workaround). Consider resolving the issue before deploying the code. Note that if you use \"HACK\" comments as a form of documentation, this rule may not be appropriate for your project. Example import os def running_windows(): # HACK: Use platform module instead. try: os.mkdir(\"C:\\\\Windows\\\\System32\\\\\") except FileExistsError: return True else: os.rmdir(\"C:\\\\Windows\\\\System32\\\\\") return False",
    "option": {}
  },
  "FA100": {
    "description": "[Status: stable] [Name: future-rewritable-type-annotation] [Message: Add <code>from __future__ import annotations</code> to simplify <code>{name}</code>] What it does Checks for missing from __future__ import annotations imports upon detecting type annotations that can be written more succinctly under PEP 563. Why is this bad? PEP 585 enabled the use of a number of convenient type annotations, such as list[str] instead of List[str]. However, these annotations are only available on Python 3.9 and higher, unless the from __future__ import annotations import is present. Similarly, PEP 604 enabled the use of the | operator for unions, such as str | None instead of Optional[str]. However, these annotations are only available on Python 3.10 and higher, unless the from __future__ import annotations import is present. By adding the __future__ import, the pyupgrade rules can automatically migrate existing code to use the new syntax, even for older Python versions. This rule thus pairs well with pyupgrade and with Ruff's pyupgrade rules. This rule respects the target-version setting. For example, if your project targets Python 3.10 and above, adding from __future__ import annotations does not impact your ability to leverage PEP 604-style unions (e.g., to convert Optional[str] to str | None). As such, this rule will only flag such usages if your project targets Python 3.9 or below. Example from typing import List, Dict, Optional def func(obj: Dict[str, Optional[int]]) -> None: ... Use instead: from __future__ import annotations from typing import List, Dict, Optional def func(obj: Dict[str, Optional[int]]) -> None: ... After running the additional pyupgrade rules: from __future__ import annotations def func(obj: dict[str, int | None]) -> None: ... Fix safety This rule's fix is marked as unsafe, as adding from __future__ import annotations may change the semantics of the program.",
    "option": {
      "target-version": {
        "option_description": "The minimum Python version to target, e.g., when considering automatic\ncode upgrades, like rewriting type annotations. Ruff will not propose\nchanges using features that are not available in the given version.\n\nFor example, to represent supporting Python >=3.11 or ==3.11\nspecify target-version = \"py311\".\n\nIf you're already using a pyproject.toml file, we recommend\nproject.requires-python instead, as it's based on Python packaging\nstandards, and will be respected by other tools. For example, Ruff\ntreats the following as identical to target-version = \"py38\":\n\n[project]\nrequires-python = \">=3.8\"\n\nIf both are specified, target-version takes precedence over\nrequires-python. See Inferring the Python version\nfor a complete description of how the target-version is determined\nwhen left unspecified.\n\nNote that a stub file can sometimes make use of a typing feature\nbefore it is available at runtime, as long as the stub does not make\nuse of new syntax. For example, a type checker will understand\nint | str in a stub as being a Union type annotation, even if the\ntype checker is run using Python 3.9, despite the fact that the |\noperator can only be used to create union types at runtime on Python\n3.10+. As such, Ruff will often recommend newer features in a stub\nfile than it would for an equivalent runtime file with the same target\nversion.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"\n\n\n\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"",
        "option_data_type": "\"py37\" | \"py38\" | \"py39\" | \"py310\" | \"py311\" | \"py312\" | \"py313\" | \"py314\"",
        "default_value": "\"py310\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "FA102": {
    "description": "[Status: stable] [Name: future-required-type-annotation] [Message: Missing <code>from __future__ import annotations</code>, but uses {reason}] What it does Checks for uses of PEP 585- and PEP 604-style type annotations in Python modules that lack the required from __future__ import annotations import for compatibility with older Python versions. Why is this bad? Using PEP 585 and PEP 604 style annotations without a from __future__ import annotations import will cause runtime errors on Python versions prior to 3.9 and 3.10, respectively. By adding the __future__ import, the interpreter will no longer interpret annotations at evaluation time, making the code compatible with both past and future Python versions. This rule respects the target-version setting. For example, if your project targets Python 3.10 and above, adding from __future__ import annotations does not impact your ability to leverage PEP 604-style unions (e.g., to convert Optional[str] to str | None). As such, this rule will only flag such usages if your project targets Python 3.9 or below. Example def func(obj: dict[str, int | None]) -> None: ... Use instead: from __future__ import annotations def func(obj: dict[str, int | None]) -> None: ... Fix safety This rule's fix is marked as unsafe, as adding from __future__ import annotations may change the semantics of the program.",
    "option": {
      "target-version": {
        "option_description": "The minimum Python version to target, e.g., when considering automatic\ncode upgrades, like rewriting type annotations. Ruff will not propose\nchanges using features that are not available in the given version.\n\nFor example, to represent supporting Python >=3.11 or ==3.11\nspecify target-version = \"py311\".\n\nIf you're already using a pyproject.toml file, we recommend\nproject.requires-python instead, as it's based on Python packaging\nstandards, and will be respected by other tools. For example, Ruff\ntreats the following as identical to target-version = \"py38\":\n\n[project]\nrequires-python = \">=3.8\"\n\nIf both are specified, target-version takes precedence over\nrequires-python. See Inferring the Python version\nfor a complete description of how the target-version is determined\nwhen left unspecified.\n\nNote that a stub file can sometimes make use of a typing feature\nbefore it is available at runtime, as long as the stub does not make\nuse of new syntax. For example, a type checker will understand\nint | str in a stub as being a Union type annotation, even if the\ntype checker is run using Python 3.9, despite the fact that the |\noperator can only be used to create union types at runtime on Python\n3.10+. As such, Ruff will often recommend newer features in a stub\nfile than it would for an equivalent runtime file with the same target\nversion.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"\n\n\n\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"",
        "option_data_type": "\"py37\" | \"py38\" | \"py39\" | \"py310\" | \"py311\" | \"py312\" | \"py313\" | \"py314\"",
        "default_value": "\"py310\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "INT001": {
    "description": "[Status: stable] [Name: f-string-in-get-text-func-call] [Message: f-string is resolved before function call; consider <code>_(\"string %s\") % arg</code>] What it does Checks for f-strings in gettext function calls. Why is this bad? In the gettext API, the gettext function (often aliased to _) returns a translation of its input argument by looking it up in a translation catalog. Calling gettext with an f-string as its argument can cause unexpected behavior. Since the f-string is resolved before the function call, the translation catalog will look up the formatted string, rather than the f-string template. Instead, format the value returned by the function call, rather than its argument. Example from gettext import gettext as _ name = \"Maria\" _(f\"Hello, {name}!\") # Looks for \"Hello, Maria!\". Use instead: from gettext import gettext as _ name = \"Maria\" _(\"Hello, %s!\") % name # Looks for \"Hello, %s!\".",
    "option": {
      "lint.flake8-gettext.function-names": {
        "option_description": "The function names to consider as internationalization calls.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-gettext]\nfunction-names = [\"_\", \"gettext\", \"ngettext\", \"ugettetxt\"]\n\n\n\n[lint.flake8-gettext]\nfunction-names = [\"_\", \"gettext\", \"ngettext\", \"ugettetxt\"]",
        "option_data_type": "list[str]",
        "default_value": "[\"_\", \"gettext\", \"ngettext\"]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "INT002": {
    "description": "[Status: stable] [Name: format-in-get-text-func-call] [Message: <code>format</code> method argument is resolved before function call; consider <code>_(\"string %s\") % arg</code>] What it does Checks for str.format calls in gettext function calls. Why is this bad? In the gettext API, the gettext function (often aliased to _) returns a translation of its input argument by looking it up in a translation catalog. Calling gettext with a formatted string as its argument can cause unexpected behavior. Since the formatted string is resolved before the function call, the translation catalog will look up the formatted string, rather than the str.format-style template. Instead, format the value returned by the function call, rather than its argument. Example from gettext import gettext as _ name = \"Maria\" _(\"Hello, {}!\".format(name)) # Looks for \"Hello, Maria!\". Use instead: from gettext import gettext as _ name = \"Maria\" _(\"Hello, %s!\") % name # Looks for \"Hello, %s!\".",
    "option": {
      "lint.flake8-gettext.function-names": {
        "option_description": "The function names to consider as internationalization calls.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-gettext]\nfunction-names = [\"_\", \"gettext\", \"ngettext\", \"ugettetxt\"]\n\n\n\n[lint.flake8-gettext]\nfunction-names = [\"_\", \"gettext\", \"ngettext\", \"ugettetxt\"]",
        "option_data_type": "list[str]",
        "default_value": "[\"_\", \"gettext\", \"ngettext\"]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "INT003": {
    "description": "[Status: stable] [Name: printf-in-get-text-func-call] [Message: printf-style format is resolved before function call; consider <code>_(\"string %s\") % arg</code>] What it does Checks for printf-style formatted strings in gettext function calls. Why is this bad? In the gettext API, the gettext function (often aliased to _) returns a translation of its input argument by looking it up in a translation catalog. Calling gettext with a formatted string as its argument can cause unexpected behavior. Since the formatted string is resolved before the function call, the translation catalog will look up the formatted string, rather than the printf-style template. Instead, format the value returned by the function call, rather than its argument. Example from gettext import gettext as _ name = \"Maria\" _(\"Hello, %s!\" % name) # Looks for \"Hello, Maria!\". Use instead: from gettext import gettext as _ name = \"Maria\" _(\"Hello, %s!\") % name # Looks for \"Hello, %s!\".",
    "option": {
      "lint.flake8-gettext.function-names": {
        "option_description": "The function names to consider as internationalization calls.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-gettext]\nfunction-names = [\"_\", \"gettext\", \"ngettext\", \"ugettetxt\"]\n\n\n\n[lint.flake8-gettext]\nfunction-names = [\"_\", \"gettext\", \"ngettext\", \"ugettetxt\"]",
        "option_data_type": "list[str]",
        "default_value": "[\"_\", \"gettext\", \"ngettext\"]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "ISC001": {
    "description": "[Status: stable] [Name: single-line-implicit-string-concatenation] [Message: Implicitly concatenated string literals on one line] What it does Checks for implicitly concatenated strings on a single line. Why is this bad? While it is valid Python syntax to concatenate multiple string or byte literals implicitly (via whitespace delimiters), it is unnecessary and negatively affects code readability. In some cases, the implicit concatenation may also be unintentional, as code formatters are capable of introducing single-line implicit concatenations when collapsing long lines. Example z = \"The quick \" \"brown fox.\" Use instead: z = \"The quick brown fox.\"",
    "option": {}
  },
  "ISC002": {
    "description": "[Status: stable] [Name: multi-line-implicit-string-concatenation] [Message: Implicitly concatenated string literals over multiple lines] What it does Checks for implicitly concatenated strings that span multiple lines. Why is this bad? For string literals that wrap across multiple lines, PEP 8 recommends the use of implicit string concatenation within parentheses instead of using a backslash for line continuation, as the former is more readable than the latter. By default, this rule will only trigger if the string literal is concatenated via a backslash. To disallow implicit string concatenation altogether, set the lint.flake8-implicit-str-concat.allow-multiline option to false. Example z = \"The quick brown fox jumps over the lazy \"\\ \"dog.\" Use instead: z = ( \"The quick brown fox jumps over the lazy \" \"dog.\" ) Formatter compatibility Using this rule with allow-multiline = false can be incompatible with the formatter because the formatter can introduce new multi-line implicitly concatenated strings. We recommend to either: Enable ISC001 to disallow all implicit concatenated strings Setting allow-multiline = true",
    "option": {
      "lint.flake8-implicit-str-concat.allow-multiline": {
        "option_description": "Whether to allow implicit string concatenations for multiline strings.\nBy default, implicit concatenations of multiline strings are\nallowed (but continuation lines, delimited with a backslash, are\nprohibited).\n\nSetting allow-multiline = false will automatically disable the\nexplicit-string-concatenation (ISC003) rule. Otherwise, both\nimplicit and explicit multiline string concatenations would be seen\nas violations, making it impossible to write a linter-compliant multiline\nstring.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-implicit-str-concat]\nallow-multiline = false\n\n\n\n[lint.flake8-implicit-str-concat]\nallow-multiline = false",
        "option_data_type": "bool",
        "default_value": "true",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "ISC003": {
    "description": "[Status: stable] [Name: explicit-string-concatenation] [Message: Explicitly concatenated string should be implicitly concatenated] What it does Checks for string literals that are explicitly concatenated (using the + operator). Why is this bad? For string literals that wrap across multiple lines, implicit string concatenation within parentheses is preferred over explicit concatenation using the + operator, as the former is more readable. Example z = ( \"The quick brown fox jumps over the lazy \" + \"dog\" ) Use instead: z = ( \"The quick brown fox jumps over the lazy \" \"dog\" )",
    "option": {
      "lint.flake8-implicit-str-concat.allow-multiline": {
        "option_description": "Whether to allow implicit string concatenations for multiline strings.\nBy default, implicit concatenations of multiline strings are\nallowed (but continuation lines, delimited with a backslash, are\nprohibited).\n\nSetting allow-multiline = false will automatically disable the\nexplicit-string-concatenation (ISC003) rule. Otherwise, both\nimplicit and explicit multiline string concatenations would be seen\nas violations, making it impossible to write a linter-compliant multiline\nstring.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-implicit-str-concat]\nallow-multiline = false\n\n\n\n[lint.flake8-implicit-str-concat]\nallow-multiline = false",
        "option_data_type": "bool",
        "default_value": "true",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "ISC004": {
    "description": "[Status: preview] [Name: implicit-string-concatenation-in-collection-literal] [Message: Unparenthesized implicit string concatenation in collection] What it does Checks for implicitly concatenated strings inside list, tuple, and set literals. Why is this bad? In collection literals, implicit string concatenation is often the result of a missing comma between elements, which can silently merge items together. Example facts = ( \"Lobsters have blue blood.\", \"The liver is the only human organ that can fully regenerate itself.\", \"Clarinets are made almost entirely out of wood from the mpingo tree.\" \"In 1971, astronaut Alan Shepard played golf on the moon.\", ) Instead, you likely intended: facts = ( \"Lobsters have blue blood.\", \"The liver is the only human organ that can fully regenerate itself.\", \"Clarinets are made almost entirely out of wood from the mpingo tree.\", \"In 1971, astronaut Alan Shepard played golf on the moon.\", ) If the concatenation is intentional, wrap it in parentheses to make it explicit: facts = ( \"Lobsters have blue blood.\", \"The liver is the only human organ that can fully regenerate itself.\", ( \"Clarinets are made almost entirely out of wood from the mpingo tree.\" \"In 1971, astronaut Alan Shepard played golf on the moon.\" ), ) Fix safety The fix is safe in that it does not change the semantics of your code. However, the issue is that you may often want to change semantics by adding a missing comma.",
    "option": {}
  },
  "ICN001": {
    "description": "[Status: stable] [Name: unconventional-import-alias] [Message: <code>{name}</code> should be imported as <code>{asname}</code>] What it does Checks for imports that are typically imported using a common convention, like import pandas as pd, and enforces that convention. Why is this bad? Consistency is good. Use a common convention for imports to make your code more readable and idiomatic. For example, import pandas as pd is a common convention for importing the pandas library, and users typically expect Pandas to be aliased as pd. Example import pandas Use instead: import pandas as pd",
    "option": {
      "lint.flake8-import-conventions.aliases": {
        "option_description": "The conventional aliases for imports. These aliases can be extended by\nthe extend-aliases option.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-import-conventions.aliases]\n# Declare the default aliases.\naltair = \"alt\"\n\"matplotlib.pyplot\" = \"plt\"\nnumpy = \"np\"\npandas = \"pd\"\nseaborn = \"sns\"\nscipy = \"sp\"\n\n\n\n[lint.flake8-import-conventions.aliases]\n# Declare the default aliases.\naltair = \"alt\"\n\"matplotlib.pyplot\" = \"plt\"\nnumpy = \"np\"\npandas = \"pd\"\nseaborn = \"sns\"\nscipy = \"sp\"",
        "option_data_type": "dict[str, str]",
        "default_value": "{\"altair\": \"alt\", \"matplotlib\": \"mpl\", \"matplotlib.pyplot\": \"plt\", \"numpy\": \"np\", \"numpy.typing\": \"npt\", \"pandas\": \"pd\", \"seaborn\": \"sns\", \"tensorflow\": \"tf\", \"tkinter\":  \"tk\", \"holoviews\": \"hv\", \"panel\": \"pn\", \"plotly.express\": \"px\", \"polars\": \"pl\", \"pyarrow\": \"pa\", \"xml.etree.ElementTree\": \"ET\"}",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.flake8-import-conventions.extend-aliases": {
        "option_description": "A mapping from module to conventional import alias. These aliases will\nbe added to the aliases mapping.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-import-conventions.extend-aliases]\n# Declare a custom alias for the `dask` module.\n\"dask.dataframe\" = \"dd\"\n\n\n\n[lint.flake8-import-conventions.extend-aliases]\n# Declare a custom alias for the `dask` module.\n\"dask.dataframe\" = \"dd\"",
        "option_data_type": "dict[str, str]",
        "default_value": "{}",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "ICN002": {
    "description": "[Status: stable] [Name: banned-import-alias] [Message: <code>{name}</code> should not be imported as <code>{asname}</code>] What it does Checks for imports that use non-standard naming conventions, like import tensorflow.keras.backend as K. Why is this bad? Consistency is good. Avoid using a non-standard naming convention for imports, and, in particular, choosing import aliases that violate PEP 8. For example, aliasing via import tensorflow.keras.backend as K violates the guidance of PEP 8, and is thus avoided in some projects. Example import tensorflow.keras.backend as K Use instead: import tensorflow as tf tf.keras.backend",
    "option": {
      "lint.flake8-import-conventions.banned-aliases": {
        "option_description": "A mapping from module to its banned import aliases.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-import-conventions.banned-aliases]\n# Declare the banned aliases.\n\"tensorflow.keras.backend\" = [\"K\"]\n\n\n\n[lint.flake8-import-conventions.banned-aliases]\n# Declare the banned aliases.\n\"tensorflow.keras.backend\" = [\"K\"]",
        "option_data_type": "dict[str, list[str]]",
        "default_value": "{}",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "ICN003": {
    "description": "[Status: stable] [Name: banned-import-from] [Message: Members of <code>{name}</code> should not be imported explicitly] What it does Checks for member imports that should instead be accessed by importing the module. Why is this bad? Consistency is good. Use a common convention for imports to make your code more readable and idiomatic. For example, it's common to import pandas as pd, and then access members like Series via pd.Series, rather than importing Series directly. Example from pandas import Series Use instead: import pandas as pd pd.Series",
    "option": {
      "lint.flake8-import-conventions.banned-from": {
        "option_description": "A list of modules that should not be imported from using the\nfrom ... import ... syntax.\n\nFor example, given banned-from = [\"pandas\"], from pandas import DataFrame\nwould be disallowed, while import pandas would be allowed.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-import-conventions]\n# Declare the banned `from` imports.\nbanned-from = [\"typing\"]\n\n\n\n[lint.flake8-import-conventions]\n# Declare the banned `from` imports.\nbanned-from = [\"typing\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "LOG001": {
    "description": "[Status: stable] [Name: direct-logger-instantiation] [Message: Use <code>logging.getLogger()</code> to instantiate loggers] What it does Checks for direct instantiation of logging.Logger, as opposed to using logging.getLogger(). Why is this bad? The Logger Objects documentation states that: Note that Loggers should NEVER be instantiated directly, but always through the module-level function logging.getLogger(name). If a logger is directly instantiated, it won't be added to the logger tree, and will bypass all configuration. Messages logged to it will only be sent to the \"handler of last resort\", skipping any filtering or formatting. Example import logging logger = logging.Logger(__name__) Use instead: import logging logger = logging.getLogger(__name__) Fix safety This fix is always unsafe, as changing from Logger to getLogger changes program behavior by will adding the logger to the logging tree.",
    "option": {}
  },
  "LOG002": {
    "description": "[Status: stable] [Name: invalid-get-logger-argument] [Message: Use <code>__name__</code> with <code>logging.getLogger()</code>] What it does Checks for any usage of __cached__ and __file__ as an argument to logging.getLogger(). Why is this bad? The logging documentation recommends this pattern: logging.getLogger(__name__) Here, __name__ is the fully qualified module name, such as foo.bar, which is the intended format for logger names. This rule detects probably-mistaken usage of similar module-level dunder constants: __cached__ - the pathname of the module's compiled version, such as foo/__pycache__/bar.cpython-311.pyc. __file__ - the pathname of the module, such as foo/bar.py. Example import logging logger = logging.getLogger(__file__) Use instead: import logging logger = logging.getLogger(__name__) Fix safety This fix is always unsafe, as changing the arguments to getLogger can change the received logger object, and thus program behavior.",
    "option": {}
  },
  "LOG004": {
    "description": "[Status: preview] [Name: log-exception-outside-except-handler] [Message: <code>.exception()</code> call outside exception handlers] What it does Checks for .exception() logging calls outside of exception handlers. Why is this bad? The documentation states: This function should only be called from an exception handler. Calling .exception() outside of an exception handler attaches None as exception information, leading to confusing messages: >>> logging.exception(\"example\") ERROR:root:example NoneType: None Example import logging logging.exception(\"Foobar\") Use instead: import logging logging.error(\"Foobar\") Fix safety The fix, if available, will always be marked as unsafe, as it changes runtime behavior.",
    "option": {
      "lint.logger-objects": {
        "option_description": "A list of objects that should be treated equivalently to a\nlogging.Logger object.\n\nThis is useful for ensuring proper diagnostics (e.g., to identify\nlogging deprecations and other best-practices) for projects that\nre-export a logging.Logger object from a common module.\n\nFor example, if you have a module logging_setup.py with the following\ncontents:\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nAdding \"logging_setup.logger\" to logger-objects will ensure that\nlogging_setup.logger is treated as a logging.Logger object when\nimported from other modules (e.g., from logging_setup import logger).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\nlogger-objects = [\"logging_setup.logger\"]\n\n\n\n[lint]\nlogger-objects = [\"logging_setup.logger\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "LOG007": {
    "description": "[Status: stable] [Name: exception-without-exc-info] [Message: Use of <code>logging.exception</code> with falsy <code>exc_info</code>] What it does Checks for uses of logging.exception() with exc_info set to False. Why is this bad? The logging.exception() method captures the exception automatically, but accepts an optional exc_info argument to override this behavior. Setting exc_info to False disables the automatic capture of the exception and stack trace. Instead of setting exc_info to False, prefer logging.error(), which has equivalent behavior to logging.exception() with exc_info set to False, but is clearer in intent. Example logging.exception(\"...\", exc_info=False) Use instead: logging.error(\"...\")",
    "option": {
      "lint.logger-objects": {
        "option_description": "A list of objects that should be treated equivalently to a\nlogging.Logger object.\n\nThis is useful for ensuring proper diagnostics (e.g., to identify\nlogging deprecations and other best-practices) for projects that\nre-export a logging.Logger object from a common module.\n\nFor example, if you have a module logging_setup.py with the following\ncontents:\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nAdding \"logging_setup.logger\" to logger-objects will ensure that\nlogging_setup.logger is treated as a logging.Logger object when\nimported from other modules (e.g., from logging_setup import logger).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\nlogger-objects = [\"logging_setup.logger\"]\n\n\n\n[lint]\nlogger-objects = [\"logging_setup.logger\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "LOG009": {
    "description": "[Status: stable] [Name: undocumented-warn] [Message: Use of undocumented <code>logging.WARN</code> constant] What it does Checks for uses of logging.WARN. Why is this bad? The logging.WARN constant is an undocumented alias for logging.WARNING. Although its not explicitly deprecated, logging.WARN is not mentioned in the logging documentation. Prefer logging.WARNING instead. Example import logging logging.basicConfig(level=logging.WARN) Use instead: import logging logging.basicConfig(level=logging.WARNING)",
    "option": {}
  },
  "LOG014": {
    "description": "[Status: stable] [Name: exc-info-outside-except-handler] [Message: <code>exc_info=</code> outside exception handlers] What it does Checks for logging calls with exc_info= outside exception handlers. Why is this bad? Using exc_info=True outside of an exception handler attaches None as the exception information, leading to confusing messages: >>> logging.warning(\"Uh oh\", exc_info=True) WARNING:root:Uh oh NoneType: None Example import logging logging.warning(\"Foobar\", exc_info=True) Use instead: import logging logging.warning(\"Foobar\") Fix safety The fix is always marked as unsafe, as it changes runtime behavior.",
    "option": {
      "lint.logger-objects": {
        "option_description": "A list of objects that should be treated equivalently to a\nlogging.Logger object.\n\nThis is useful for ensuring proper diagnostics (e.g., to identify\nlogging deprecations and other best-practices) for projects that\nre-export a logging.Logger object from a common module.\n\nFor example, if you have a module logging_setup.py with the following\ncontents:\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nAdding \"logging_setup.logger\" to logger-objects will ensure that\nlogging_setup.logger is treated as a logging.Logger object when\nimported from other modules (e.g., from logging_setup import logger).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\nlogger-objects = [\"logging_setup.logger\"]\n\n\n\n[lint]\nlogger-objects = [\"logging_setup.logger\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "LOG015": {
    "description": "[Status: stable] [Name: root-logger-call] [Message: <code>{}()</code> call on root logger] What it does Checks for usages of the following logging top-level functions: debug, info, warn, warning, error, critical, log, exception. Why is this bad? Using the root logger causes the messages to have no source information, making them less useful for debugging. Example import logging logging.info(\"Foobar\") Use instead: import logging logger = logging.getLogger(__name__) logger.info(\"Foobar\")",
    "option": {}
  },
  "G001": {
    "description": "[Status: stable] [Name: logging-string-format] [Message: Logging statement uses <code>str.format</code>] What it does Checks for uses of str.format to format logging messages. Why is this bad? The logging module provides a mechanism for passing additional values to be logged using the extra keyword argument. This is more consistent, more efficient, and less error-prone than formatting the string directly. Using str.format to format a logging message requires that Python eagerly format the string, even if the logging statement is never executed (e.g., if the log level is above the level of the logging statement), whereas using the extra keyword argument defers formatting until required. Additionally, the use of extra will ensure that the values are made available to all handlers, which can then be configured to log the values in a consistent manner. As an alternative to extra, passing values as arguments to the logging method can also be used to defer string formatting until required. Known problems This rule detects uses of the logging module via a heuristic. Specifically, it matches against: Uses of the logging module itself (e.g., import logging; logging.info(...)). Uses of flask.current_app.logger (e.g., from flask import current_app; current_app.logger.info(...)). Objects whose name starts with log or ends with logger or logging, when used in the same file in which they are defined (e.g., logger = logging.getLogger(); logger.info(...)). Imported objects marked as loggers via the lint.logger-objects setting, which can be used to enforce these rules against shared logger objects (e.g., from module import logger; logger.info(...), when lint.logger-objects is set to [\"module.logger\"]). Example import logging logging.basicConfig(format=\"%(message)s\", level=logging.INFO) user = \"Maria\" logging.info(\"{} - Something happened\".format(user)) Use instead: import logging logging.basicConfig(format=\"%(user_id)s - %(message)s\", level=logging.INFO) user = \"Maria\" logging.info(\"Something happened\", extra={\"user_id\": user}) Or: import logging logging.basicConfig(format=\"%(message)s\", level=logging.INFO) user = \"Maria\" logging.info(\"%s - Something happened\", user)",
    "option": {
      "lint.logger-objects": {
        "option_description": "A list of objects that should be treated equivalently to a\nlogging.Logger object.\n\nThis is useful for ensuring proper diagnostics (e.g., to identify\nlogging deprecations and other best-practices) for projects that\nre-export a logging.Logger object from a common module.\n\nFor example, if you have a module logging_setup.py with the following\ncontents:\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nAdding \"logging_setup.logger\" to logger-objects will ensure that\nlogging_setup.logger is treated as a logging.Logger object when\nimported from other modules (e.g., from logging_setup import logger).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\nlogger-objects = [\"logging_setup.logger\"]\n\n\n\n[lint]\nlogger-objects = [\"logging_setup.logger\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "G002": {
    "description": "[Status: stable] [Name: logging-percent-format] [Message: Logging statement uses <code>%</code>] What it does Checks for uses of printf-style format strings to format logging messages. Why is this bad? The logging module provides a mechanism for passing additional values to be logged using the extra keyword argument. This is more consistent, more efficient, and less error-prone than formatting the string directly. Using printf-style format strings to format a logging message requires that Python eagerly format the string, even if the logging statement is never executed (e.g., if the log level is above the level of the logging statement), whereas using the extra keyword argument defers formatting until required. Additionally, the use of extra will ensure that the values are made available to all handlers, which can then be configured to log the values in a consistent manner. As an alternative to extra, passing values as arguments to the logging method can also be used to defer string formatting until required. Known problems This rule detects uses of the logging module via a heuristic. Specifically, it matches against: Uses of the logging module itself (e.g., import logging; logging.info(...)). Uses of flask.current_app.logger (e.g., from flask import current_app; current_app.logger.info(...)). Objects whose name starts with log or ends with logger or logging, when used in the same file in which they are defined (e.g., logger = logging.getLogger(); logger.info(...)). Imported objects marked as loggers via the lint.logger-objects setting, which can be used to enforce these rules against shared logger objects (e.g., from module import logger; logger.info(...), when lint.logger-objects is set to [\"module.logger\"]). Example import logging logging.basicConfig(format=\"%(message)s\", level=logging.INFO) user = \"Maria\" logging.info(\"%s - Something happened\" % user) Use instead: import logging logging.basicConfig(format=\"%(user_id)s - %(message)s\", level=logging.INFO) user = \"Maria\" logging.info(\"Something happened\", extra=dict(user_id=user)) Or: import logging logging.basicConfig(format=\"%(message)s\", level=logging.INFO) user = \"Maria\" logging.info(\"%s - Something happened\", user)",
    "option": {
      "lint.logger-objects": {
        "option_description": "A list of objects that should be treated equivalently to a\nlogging.Logger object.\n\nThis is useful for ensuring proper diagnostics (e.g., to identify\nlogging deprecations and other best-practices) for projects that\nre-export a logging.Logger object from a common module.\n\nFor example, if you have a module logging_setup.py with the following\ncontents:\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nAdding \"logging_setup.logger\" to logger-objects will ensure that\nlogging_setup.logger is treated as a logging.Logger object when\nimported from other modules (e.g., from logging_setup import logger).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\nlogger-objects = [\"logging_setup.logger\"]\n\n\n\n[lint]\nlogger-objects = [\"logging_setup.logger\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "G003": {
    "description": "[Status: stable] [Name: logging-string-concat] [Message: Logging statement uses <code>+</code>] What it does Checks for uses string concatenation via the + operator to format logging messages. Why is this bad? The logging module provides a mechanism for passing additional values to be logged using the extra keyword argument. This is more consistent, more efficient, and less error-prone than formatting the string directly. Using concatenation to format a logging message requires that Python eagerly format the string, even if the logging statement is never executed (e.g., if the log level is above the level of the logging statement), whereas using the extra keyword argument defers formatting until required. Additionally, the use of extra will ensure that the values are made available to all handlers, which can then be configured to log the values in a consistent manner. As an alternative to extra, passing values as arguments to the logging method can also be used to defer string formatting until required. Known problems This rule detects uses of the logging module via a heuristic. Specifically, it matches against: Uses of the logging module itself (e.g., import logging; logging.info(...)). Uses of flask.current_app.logger (e.g., from flask import current_app; current_app.logger.info(...)). Objects whose name starts with log or ends with logger or logging, when used in the same file in which they are defined (e.g., logger = logging.getLogger(); logger.info(...)). Imported objects marked as loggers via the lint.logger-objects setting, which can be used to enforce these rules against shared logger objects (e.g., from module import logger; logger.info(...), when lint.logger-objects is set to [\"module.logger\"]). Example import logging logging.basicConfig(format=\"%(message)s\", level=logging.INFO) user = \"Maria\" logging.info(user + \" - Something happened\") Use instead: import logging logging.basicConfig(format=\"%(user_id)s - %(message)s\", level=logging.INFO) user = \"Maria\" logging.info(\"Something happened\", extra=dict(user_id=user)) Or: import logging logging.basicConfig(format=\"%(message)s\", level=logging.INFO) user = \"Maria\" logging.info(\"%s - Something happened\", user)",
    "option": {
      "lint.logger-objects": {
        "option_description": "A list of objects that should be treated equivalently to a\nlogging.Logger object.\n\nThis is useful for ensuring proper diagnostics (e.g., to identify\nlogging deprecations and other best-practices) for projects that\nre-export a logging.Logger object from a common module.\n\nFor example, if you have a module logging_setup.py with the following\ncontents:\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nAdding \"logging_setup.logger\" to logger-objects will ensure that\nlogging_setup.logger is treated as a logging.Logger object when\nimported from other modules (e.g., from logging_setup import logger).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\nlogger-objects = [\"logging_setup.logger\"]\n\n\n\n[lint]\nlogger-objects = [\"logging_setup.logger\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "G004": {
    "description": "[Status: stable] [Name: logging-f-string] [Message: Logging statement uses f-string] What it does Checks for uses of f-strings to format logging messages. Why is this bad? The logging module provides a mechanism for passing additional values to be logged using the extra keyword argument. This is more consistent, more efficient, and less error-prone than formatting the string directly. Using f-strings to format a logging message requires that Python eagerly format the string, even if the logging statement is never executed (e.g., if the log level is above the level of the logging statement), whereas using the extra keyword argument defers formatting until required. Additionally, the use of extra will ensure that the values are made available to all handlers, which can then be configured to log the values in a consistent manner. As an alternative to extra, passing values as arguments to the logging method can also be used to defer string formatting until required. Known problems This rule detects uses of the logging module via a heuristic. Specifically, it matches against: Uses of the logging module itself (e.g., import logging; logging.info(...)). Uses of flask.current_app.logger (e.g., from flask import current_app; current_app.logger.info(...)). Objects whose name starts with log or ends with logger or logging, when used in the same file in which they are defined (e.g., logger = logging.getLogger(); logger.info(...)). Imported objects marked as loggers via the lint.logger-objects setting, which can be used to enforce these rules against shared logger objects (e.g., from module import logger; logger.info(...), when lint.logger-objects is set to [\"module.logger\"]). Example import logging logging.basicConfig(format=\"%(message)s\", level=logging.INFO) user = \"Maria\" logging.info(f\"{user} - Something happened\") Use instead: import logging logging.basicConfig(format=\"%(user_id)s - %(message)s\", level=logging.INFO) user = \"Maria\" logging.info(\"Something happened\", extra=dict(user_id=user)) Or: import logging logging.basicConfig(format=\"%(message)s\", level=logging.INFO) user = \"Maria\" logging.info(\"%s - Something happened\", user)",
    "option": {
      "lint.logger-objects": {
        "option_description": "A list of objects that should be treated equivalently to a\nlogging.Logger object.\n\nThis is useful for ensuring proper diagnostics (e.g., to identify\nlogging deprecations and other best-practices) for projects that\nre-export a logging.Logger object from a common module.\n\nFor example, if you have a module logging_setup.py with the following\ncontents:\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nAdding \"logging_setup.logger\" to logger-objects will ensure that\nlogging_setup.logger is treated as a logging.Logger object when\nimported from other modules (e.g., from logging_setup import logger).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\nlogger-objects = [\"logging_setup.logger\"]\n\n\n\n[lint]\nlogger-objects = [\"logging_setup.logger\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "G010": {
    "description": "[Status: stable] [Name: logging-warn] [Message: Logging statement uses <code>warn</code> instead of <code>warning</code>] What it does Checks for uses of logging.warn and logging.Logger.warn. Why is this bad? logging.warn and logging.Logger.warn are deprecated in favor of logging.warning and logging.Logger.warning, which are functionally equivalent. Known problems This rule detects uses of the logging module via a heuristic. Specifically, it matches against: Uses of the logging module itself (e.g., import logging; logging.info(...)). Uses of flask.current_app.logger (e.g., from flask import current_app; current_app.logger.info(...)). Objects whose name starts with log or ends with logger or logging, when used in the same file in which they are defined (e.g., logger = logging.getLogger(); logger.info(...)). Imported objects marked as loggers via the lint.logger-objects setting, which can be used to enforce these rules against shared logger objects (e.g., from module import logger; logger.info(...), when lint.logger-objects is set to [\"module.logger\"]). Example import logging logging.warn(\"Something happened\") Use instead: import logging logging.warning(\"Something happened\")",
    "option": {
      "lint.logger-objects": {
        "option_description": "A list of objects that should be treated equivalently to a\nlogging.Logger object.\n\nThis is useful for ensuring proper diagnostics (e.g., to identify\nlogging deprecations and other best-practices) for projects that\nre-export a logging.Logger object from a common module.\n\nFor example, if you have a module logging_setup.py with the following\ncontents:\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nAdding \"logging_setup.logger\" to logger-objects will ensure that\nlogging_setup.logger is treated as a logging.Logger object when\nimported from other modules (e.g., from logging_setup import logger).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\nlogger-objects = [\"logging_setup.logger\"]\n\n\n\n[lint]\nlogger-objects = [\"logging_setup.logger\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "G101": {
    "description": "[Status: stable] [Name: logging-extra-attr-clash] [Message: Logging statement uses an <code>extra</code> field that clashes with a <code>LogRecord</code> field: <code>{key}</code>] What it does Checks for extra keywords in logging statements that clash with LogRecord attributes. Why is this bad? The logging module provides a mechanism for passing additional values to be logged using the extra keyword argument. These values are then passed to the LogRecord constructor. Providing a value via extra that clashes with one of the attributes of the LogRecord constructor will raise a KeyError when the LogRecord is constructed. Known problems This rule detects uses of the logging module via a heuristic. Specifically, it matches against: Uses of the logging module itself (e.g., import logging; logging.info(...)). Uses of flask.current_app.logger (e.g., from flask import current_app; current_app.logger.info(...)). Objects whose name starts with log or ends with logger or logging, when used in the same file in which they are defined (e.g., logger = logging.getLogger(); logger.info(...)). Imported objects marked as loggers via the lint.logger-objects setting, which can be used to enforce these rules against shared logger objects (e.g., from module import logger; logger.info(...), when lint.logger-objects is set to [\"module.logger\"]). Example import logging logging.basicConfig(format=\"%(name) - %(message)s\", level=logging.INFO) username = \"Maria\" logging.info(\"Something happened\", extra=dict(name=username)) Use instead: import logging logging.basicConfig(format=\"%(user_id)s - %(message)s\", level=logging.INFO) username = \"Maria\" logging.info(\"Something happened\", extra=dict(user_id=username))",
    "option": {
      "lint.logger-objects": {
        "option_description": "A list of objects that should be treated equivalently to a\nlogging.Logger object.\n\nThis is useful for ensuring proper diagnostics (e.g., to identify\nlogging deprecations and other best-practices) for projects that\nre-export a logging.Logger object from a common module.\n\nFor example, if you have a module logging_setup.py with the following\ncontents:\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nAdding \"logging_setup.logger\" to logger-objects will ensure that\nlogging_setup.logger is treated as a logging.Logger object when\nimported from other modules (e.g., from logging_setup import logger).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\nlogger-objects = [\"logging_setup.logger\"]\n\n\n\n[lint]\nlogger-objects = [\"logging_setup.logger\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "G201": {
    "description": "[Status: stable] [Name: logging-exc-info] [Message: Logging <code>.exception(...)</code> should be used instead of <code>.error(..., exc_info=True)</code>] What it does Checks for uses of logging.error that pass exc_info=True. Why is this bad? Calling logging.error with exc_info=True is equivalent to calling logging.exception. Using logging.exception is more concise, more readable, and conveys the intent of the logging statement more clearly. Known problems This rule detects uses of the logging module via a heuristic. Specifically, it matches against: Uses of the logging module itself (e.g., import logging; logging.info(...)). Uses of flask.current_app.logger (e.g., from flask import current_app; current_app.logger.info(...)). Objects whose name starts with log or ends with logger or logging, when used in the same file in which they are defined (e.g., logger = logging.getLogger(); logger.info(...)). Imported objects marked as loggers via the lint.logger-objects setting, which can be used to enforce these rules against shared logger objects (e.g., from module import logger; logger.info(...), when lint.logger-objects is set to [\"module.logger\"]). Example import logging try: ... except ValueError: logging.error(\"Exception occurred\", exc_info=True) Use instead: import logging try: ... except ValueError: logging.exception(\"Exception occurred\")",
    "option": {
      "lint.logger-objects": {
        "option_description": "A list of objects that should be treated equivalently to a\nlogging.Logger object.\n\nThis is useful for ensuring proper diagnostics (e.g., to identify\nlogging deprecations and other best-practices) for projects that\nre-export a logging.Logger object from a common module.\n\nFor example, if you have a module logging_setup.py with the following\ncontents:\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nAdding \"logging_setup.logger\" to logger-objects will ensure that\nlogging_setup.logger is treated as a logging.Logger object when\nimported from other modules (e.g., from logging_setup import logger).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\nlogger-objects = [\"logging_setup.logger\"]\n\n\n\n[lint]\nlogger-objects = [\"logging_setup.logger\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "G202": {
    "description": "[Status: stable] [Name: logging-redundant-exc-info] [Message: Logging statement has redundant <code>exc_info</code>] What it does Checks for redundant exc_info keyword arguments in logging statements. Why is this bad? exc_info is True by default for logging.exception, and False by default for logging.error. Passing exc_info=True to logging.exception calls is redundant, as is passing exc_info=False to logging.error calls. Known problems This rule detects uses of the logging module via a heuristic. Specifically, it matches against: Uses of the logging module itself (e.g., import logging; logging.info(...)). Uses of flask.current_app.logger (e.g., from flask import current_app; current_app.logger.info(...)). Objects whose name starts with log or ends with logger or logging, when used in the same file in which they are defined (e.g., logger = logging.getLogger(); logger.info(...)). Imported objects marked as loggers via the lint.logger-objects setting, which can be used to enforce these rules against shared logger objects (e.g., from module import logger; logger.info(...), when lint.logger-objects is set to [\"module.logger\"]). Example import logging try: ... except ValueError: logging.exception(\"Exception occurred\", exc_info=True) Use instead: import logging try: ... except ValueError: logging.exception(\"Exception occurred\")",
    "option": {
      "lint.logger-objects": {
        "option_description": "A list of objects that should be treated equivalently to a\nlogging.Logger object.\n\nThis is useful for ensuring proper diagnostics (e.g., to identify\nlogging deprecations and other best-practices) for projects that\nre-export a logging.Logger object from a common module.\n\nFor example, if you have a module logging_setup.py with the following\ncontents:\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nAdding \"logging_setup.logger\" to logger-objects will ensure that\nlogging_setup.logger is treated as a logging.Logger object when\nimported from other modules (e.g., from logging_setup import logger).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\nlogger-objects = [\"logging_setup.logger\"]\n\n\n\n[lint]\nlogger-objects = [\"logging_setup.logger\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "INP001": {
    "description": "[Status: stable] [Name: implicit-namespace-package] [Message: File <code>{filename}</code> is part of an implicit namespace package. Add an <code>__init__.py</code>.] What it does Checks for packages that are missing an __init__.py file. Why is this bad? Python packages are directories that contain a file named __init__.py. The existence of this file indicates that the directory is a Python package, and so it can be imported the same way a module can be imported. Directories that lack an __init__.py file can still be imported, but they're indicative of a special kind of package, known as a \"namespace package\" (see: PEP 420). Namespace packages are less widely used, so a package that lacks an __init__.py file is typically meant to be a regular package, and the absence of the __init__.py file is probably an oversight.",
    "option": {
      "namespace-packages": {
        "option_description": "Mark the specified directories as namespace packages. For the purpose of\nmodule resolution, Ruff will treat those directories and all their subdirectories\nas if they contained an __init__.py file.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\nnamespace-packages = [\"airflow/providers\"]\n\n\n\nnamespace-packages = [\"airflow/providers\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PIE790": {
    "description": "[Status: stable] [Name: unnecessary-placeholder] [Message: Unnecessary <code>pass</code> statement] What it does Checks for unnecessary pass statements and ellipsis (...) literals in functions, classes, and other blocks. Why is this bad? In Python, the pass statement and ellipsis (...) literal serve as placeholders, allowing for syntactically correct empty code blocks. The primary purpose of these nodes is to avoid syntax errors in situations where a statement or expression is syntactically required, but no code needs to be executed. If a pass or ellipsis is present in a code block that includes at least one other statement (even, e.g., a docstring), it is unnecessary and should be removed. Example def func(): \"\"\"Placeholder docstring.\"\"\" pass Use instead: def func(): \"\"\"Placeholder docstring.\"\"\" Or, given: def func(): \"\"\"Placeholder docstring.\"\"\" ... Use instead: def func(): \"\"\"Placeholder docstring.\"\"\" Fix safety This rule's fix is marked as unsafe in the rare case that the pass or ellipsis is followed by a string literal, since removal of the placeholder would convert the subsequent string literal into a docstring.",
    "option": {}
  },
  "PIE794": {
    "description": "[Status: stable] [Name: duplicate-class-field-definition] [Message: Class field <code>{name}</code> is defined multiple times] What it does Checks for duplicate field definitions in classes. Why is this bad? Defining a field multiple times in a class body is redundant and likely a mistake. Example class Person: name = Tom ... name = Ben Use instead: class Person: name = Tom ... Fix safety This fix is always marked as unsafe since we cannot know for certain which assignment was intended.",
    "option": {}
  },
  "PIE796": {
    "description": "[Status: stable] [Name: non-unique-enums] [Message: Enum contains duplicate value: <code>{value}</code>] What it does Checks for enums that contain duplicate values. Why is this bad? Enum values should be unique. Non-unique values are redundant and likely a mistake. Example from enum import Enum class Foo(Enum): A = 1 B = 2 C = 1 Use instead: from enum import Enum class Foo(Enum): A = 1 B = 2 C = 3",
    "option": {}
  },
  "PIE800": {
    "description": "[Status: stable] [Name: unnecessary-spread] [Message: Unnecessary spread <code>**</code>] What it does Checks for unnecessary dictionary unpacking operators (**). Why is this bad? Unpacking a dictionary into another dictionary is redundant. The unpacking operator can be removed, making the code more readable. Example foo = {\"A\": 1, \"B\": 2} bar = {**foo, **{\"C\": 3}} Use instead: foo = {\"A\": 1, \"B\": 2} bar = {**foo, \"C\": 3}",
    "option": {}
  },
  "PIE804": {
    "description": "[Status: stable] [Name: unnecessary-dict-kwargs] [Message: Unnecessary <code>dict</code> kwargs] What it does Checks for unnecessary dict kwargs. Why is this bad? If the dict keys are valid identifiers, they can be passed as keyword arguments directly, without constructing unnecessary dictionary. This also makes code more type-safe as type checkers often cannot precisely verify dynamic keyword arguments. Example def foo(bar): return bar + 1 print(foo(**{\"bar\": 2})) # prints 3 # No typing errors, but results in an exception at runtime. print(foo(**{\"bar\": 2, \"baz\": 3})) Use instead: def foo(bar): return bar + 1 print(foo(bar=2)) # prints 3 # Typing error detected: No parameter named \"baz\". print(foo(bar=2, baz=3)) Fix safety This rule's fix is marked as unsafe for dictionaries with comments interleaved between the items, as comments may be removed. For example, the fix would be marked as unsafe in the following case: foo( **{ # comment \"x\": 1.0, # comment \"y\": 2.0, } ) as this is converted to foo(x=1.0, y=2.0) without any of the comments.",
    "option": {}
  },
  "PIE807": {
    "description": "[Status: stable] [Name: reimplemented-container-builtin] [Message: Prefer <code>{container}</code> over useless lambda] What it does Checks for lambdas that can be replaced with the list or dict builtins. Why is this bad? Using container builtins are more succinct and idiomatic than wrapping the literal in a lambda. Example from dataclasses import dataclass, field @dataclass class Foo: bar: list[int] = field(default_factory=lambda: []) Use instead: from dataclasses import dataclass, field @dataclass class Foo: bar: list[int] = field(default_factory=list) baz: dict[str, int] = field(default_factory=dict)",
    "option": {}
  },
  "PIE808": {
    "description": "[Status: stable] [Name: unnecessary-range-start] [Message: Unnecessary <code>start</code> argument in <code>range</code>] What it does Checks for range calls with an unnecessary start argument. Why is this bad? range(0, x) is equivalent to range(x), as 0 is the default value for the start argument. Omitting the start argument makes the code more concise and idiomatic. Example range(0, 3) Use instead: range(3)",
    "option": {}
  },
  "PIE810": {
    "description": "[Status: stable] [Name: multiple-starts-ends-with] [Message: Call <code>{attr}</code> once with a <code>tuple</code>] What it does Checks for startswith or endswith calls on the same value with different prefixes or suffixes. Why is this bad? The startswith and endswith methods accept tuples of prefixes or suffixes respectively. Passing a tuple of prefixes or suffixes is more efficient and readable than calling the method multiple times. Example msg = \"Hello, world!\" if msg.startswith(\"Hello\") or msg.startswith(\"Hi\"): print(\"Greetings!\") Use instead: msg = \"Hello, world!\" if msg.startswith((\"Hello\", \"Hi\")): print(\"Greetings!\") Fix safety This rule's fix is unsafe, as in some cases, it will be unable to determine whether the argument to an existing .startswith or .endswith call is a tuple. For example, given msg.startswith(x) or msg.startswith(y), if x or y is a tuple, and the semantic model is unable to detect it as such, the rule will suggest msg.startswith((x, y)), which will error at runtime.",
    "option": {}
  },
  "T201": {
    "description": "[Status: stable] [Name: print] [Message: <code>print</code> found] What it does Checks for print statements. Why is this bad? print statements used for debugging should be omitted from production code. They can lead the accidental inclusion of sensitive information in logs, and are not configurable by clients, unlike logging statements. print statements used to produce output as a part of a command-line interface program are not typically a problem. Example def sum_less_than_four(a, b): print(f\"Calling sum_less_than_four\") return a + b < 4 The automatic fix will remove the print statement entirely: def sum_less_than_four(a, b): return a + b < 4 To keep the line for logging purposes, instead use something like: import logging logger = logging.getLogger(__name__) def sum_less_than_four(a, b): logger.debug(\"Calling sum_less_than_four\") return a + b < 4 if __name__ == \"__main__\": logging.basicConfig(level=logging.INFO) Fix safety This rule's fix is marked as unsafe, as it will remove print statements that are used beyond debugging purposes.",
    "option": {}
  },
  "T203": {
    "description": "[Status: stable] [Name: p-print] [Message: <code>pprint</code> found] What it does Checks for pprint statements. Why is this bad? Like print statements, pprint statements used for debugging should be omitted from production code. They can lead the accidental inclusion of sensitive information in logs, and are not configurable by clients, unlike logging statements. pprint statements used to produce output as a part of a command-line interface program are not typically a problem. Example import pprint def merge_dicts(dict_a, dict_b): dict_c = {**dict_a, **dict_b} pprint.pprint(dict_c) return dict_c Use instead: def merge_dicts(dict_a, dict_b): dict_c = {**dict_a, **dict_b} return dict_c Fix safety This rule's fix is marked as unsafe, as it will remove pprint statements that are used beyond debugging purposes.",
    "option": {}
  },
  "PYI001": {
    "description": "[Status: stable] [Name: unprefixed-type-param] [Message: Name of private <code>{kind}</code> must start with <code>_</code>] What it does Checks that type TypeVars, ParamSpecs, and TypeVarTuples in stubs have names prefixed with _. Why is this bad? Prefixing type parameters with _ avoids accidentally exposing names internal to the stub. Example from typing import TypeVar T = TypeVar(\"T\") Use instead: from typing import TypeVar _T = TypeVar(\"_T\")",
    "option": {}
  },
  "PYI002": {
    "description": "[Status: stable] [Name: complex-if-statement-in-stub] [Message: <code>if</code> test must be a simple comparison against <code>sys.platform</code> or <code>sys.version_info</code>] What it does Checks for if statements with complex conditionals in stubs. Why is this bad? Type checkers understand simple conditionals to express variations between different Python versions and platforms. However, complex tests may not be understood by a type checker, leading to incorrect inferences when they analyze your code. Example import sys if (3, 10) <= sys.version_info < (3, 12): ... Use instead: import sys if sys.version_info >= (3, 10) and sys.version_info < (3, 12): ...",
    "option": {}
  },
  "PYI003": {
    "description": "[Status: stable] [Name: unrecognized-version-info-check] [Message: Unrecognized <code>sys.version_info</code> check] What it does Checks for problematic sys.version_info-related conditions in stubs. Why is this bad? Stub files support simple conditionals to test for differences in Python versions using sys.version_info. However, there are a number of common mistakes involving sys.version_info comparisons that should be avoided. For example, comparing against a string can lead to unexpected behavior. Example import sys if sys.version_info[0] == \"2\": ... Use instead: import sys if sys.version_info[0] == 2: ...",
    "option": {}
  },
  "PYI004": {
    "description": "[Status: stable] [Name: patch-version-comparison] [Message: Version comparison must use only major and minor version] What it does Checks for Python version comparisons in stubs that compare against patch versions (e.g., Python 3.8.3) instead of major and minor versions (e.g., Python 3.8). Why is this bad? Stub files support simple conditionals to test for differences in Python versions and platforms. However, type checkers only understand a limited subset of these conditionals. In particular, type checkers don't support patch versions (e.g., Python 3.8.3), only major and minor versions (e.g., Python 3.8). Therefore, version checks in stubs should only use the major and minor versions. Example import sys if sys.version_info >= (3, 4, 3): ... Use instead: import sys if sys.version_info >= (3, 4): ...",
    "option": {}
  },
  "PYI005": {
    "description": "[Status: stable] [Name: wrong-tuple-length-version-comparison] [Message: Version comparison must be against a length-{expected_length} tuple] What it does Checks for Python version comparisons that compare against a tuple of the wrong length. Why is this bad? Stub files support simple conditionals to test for differences in Python versions and platforms. When comparing against sys.version_info, avoid comparing against tuples of the wrong length, which can lead to unexpected behavior. Example import sys if sys.version_info[:2] == (3,): ... Use instead: import sys if sys.version_info[0] == 3: ...",
    "option": {}
  },
  "PYI006": {
    "description": "[Status: stable] [Name: bad-version-info-comparison] [Message: Use <code>&lt;</code> or <code>&gt;=</code> for <code>sys.version_info</code> comparisons] What it does Checks for uses of comparators other than < and >= for sys.version_info checks. All other comparators, such as >, <=, and ==, are banned. Why is this bad? Comparing sys.version_info with == or <= has unexpected behavior and can lead to bugs. For example, sys.version_info > (3, 8, 1) will resolve to True if your Python version is 3.8.1; meanwhile, sys.version_info <= (3, 8) will not resolve to True if your Python version is 3.8.10: >>> import sys >>> print(sys.version_info) sys.version_info(major=3, minor=8, micro=10, releaselevel='final', serial=0) >>> print(sys.version_info > (3, 8)) True >>> print(sys.version_info == (3, 8)) False >>> print(sys.version_info <= (3, 8)) False >>> print(sys.version_info in (3, 8)) False Example import sys if sys.version_info > (3, 8): ... Use instead: import sys if sys.version_info >= (3, 9): ...",
    "option": {}
  },
  "PYI007": {
    "description": "[Status: stable] [Name: unrecognized-platform-check] [Message: Unrecognized <code>sys.platform</code> check] What it does Check for unrecognized sys.platform checks. Platform checks should be simple string comparisons. Note: this rule is only enabled in .pyi stub files. Why is this bad? Some sys.platform checks are too complex for type checkers to understand, and thus result in incorrect inferences by these tools. sys.platform checks should be simple string comparisons, like if sys.platform == \"linux\". Example import sys if sys.platform == \"xunil\"[::-1]: # Linux specific definitions ... else: # Posix specific definitions ... Instead, use a simple string comparison, such as == or !=: import sys if sys.platform == \"linux\": # Linux specific definitions ... else: # Posix specific definitions ...",
    "option": {}
  },
  "PYI008": {
    "description": "[Status: stable] [Name: unrecognized-platform-name] [Message: Unrecognized platform <code>{platform}</code>] What it does Check for unrecognized platform names in sys.platform checks. Note: this rule is only enabled in .pyi stub files. Why is this bad? If a sys.platform check compares to a platform name outside of a small set of known platforms (e.g. \"linux\", \"win32\", etc.), it's likely a typo or a platform name that is not recognized by type checkers. The list of known platforms is: \"linux\", \"win32\", \"cygwin\", \"darwin\". Example import sys if sys.platform == \"linus\": ... Use instead: import sys if sys.platform == \"linux\": ...",
    "option": {}
  },
  "PYI009": {
    "description": "[Status: stable] [Name: pass-statement-stub-body] [Message: Empty body should contain <code>...</code>, not <code>pass</code>] What it does Checks for pass statements in empty stub bodies. Why is this bad? For stylistic consistency, ... should always be used rather than pass in stub files. Example def foo(bar: int) -> list[int]: pass Use instead: def foo(bar: int) -> list[int]: ...",
    "option": {}
  },
  "PYI010": {
    "description": "[Status: stable] [Name: non-empty-stub-body] [Message: Function body must contain only <code>...</code>] What it does Checks for non-empty function stub bodies. Why is this bad? Stub files are never executed at runtime; they should be thought of as \"data files\" for type checkers or IDEs. Function bodies are redundant for this purpose. Example def double(x: int) -> int: return x * 2 Use instead: def double(x: int) -> int: ...",
    "option": {}
  },
  "PYI011": {
    "description": "[Status: stable] [Name: typed-argument-default-in-stub] [Message: Only simple default values allowed for typed arguments] What it does Checks for typed function arguments in stubs with complex default values. Why is this bad? Stub (.pyi) files exist as \"data files\" for static analysis tools, and are not evaluated at runtime. While simple default values may be useful for some tools that consume stubs, such as IDEs, they are ignored by type checkers. Instead of including and reproducing a complex value, use ... to indicate that the assignment has a default value, but that the value is \"complex\" or varies according to the current platform or Python version. For the purposes of this rule, any default value counts as \"complex\" unless it is a literal int, float, complex, bytes, str, bool, None, ..., or a simple container literal. Example def foo(arg: list[int] = list(range(10_000))) -> None: ... Use instead: def foo(arg: list[int] = ...) -> None: ...",
    "option": {}
  },
  "PYI012": {
    "description": "[Status: stable] [Name: pass-in-class-body] [Message: Class body must not contain <code>pass</code>] What it does Checks for the presence of the pass statement in non-empty class bodies in .pyi files. Why is this bad? The pass statement is always unnecessary in non-empty class bodies in stubs. Example class MyClass: x: int pass Use instead: class MyClass: x: int",
    "option": {}
  },
  "PYI013": {
    "description": "[Status: stable] [Name: ellipsis-in-non-empty-class-body] [Message: Non-empty class body must not contain <code>...</code>] What it does Removes ellipses (...) in otherwise non-empty class bodies. Why is this bad? An ellipsis in a class body is only necessary if the class body is otherwise empty. If the class body is non-empty, then the ellipsis is redundant. Example class Foo: ... value: int Use instead: class Foo: value: int",
    "option": {}
  },
  "PYI014": {
    "description": "[Status: stable] [Name: argument-default-in-stub] [Message: Only simple default values allowed for arguments] What it does Checks for untyped function arguments in stubs with default values that are not \"simple\" /// (i.e., int, float, complex, bytes, str, bool, None, ..., or simple container literals). Why is this bad? Stub (.pyi) files exist to define type hints, and are not evaluated at runtime. As such, function arguments in stub files should not have default values, as they are ignored by type checkers. However, the use of default values may be useful for IDEs and other consumers of stub files, and so \"simple\" values may be worth including and are permitted by this rule. Instead of including and reproducing a complex value, use ... to indicate that the assignment has a default value, but that the value is non-simple or varies according to the current platform or Python version. Example def foo(arg=bar()) -> None: ... Use instead: def foo(arg=...) -> None: ...",
    "option": {}
  },
  "PYI015": {
    "description": "[Status: stable] [Name: assignment-default-in-stub] [Message: Only simple default values allowed for assignments] What it does Checks for assignments in stubs with default values that are not \"simple\" (i.e., int, float, complex, bytes, str, bool, None, ..., or simple container literals). Why is this bad? Stub (.pyi) files exist to define type hints, and are not evaluated at runtime. As such, assignments in stub files should not include values, as they are ignored by type checkers. However, the use of such values may be useful for IDEs and other consumers of stub files, and so \"simple\" values may be worth including and are permitted by this rule. Instead of including and reproducing a complex value, use ... to indicate that the assignment has a default value, but that the value is non-simple or varies according to the current platform or Python version. Example foo: str = bar() Use instead: foo: str = ...",
    "option": {}
  },
  "PYI016": {
    "description": "[Status: stable] [Name: duplicate-union-member] [Message: Duplicate union member <code>{}</code>] What it does Checks for duplicate union members. Why is this bad? Duplicate union members are redundant and should be removed. Example foo: str | str Use instead: foo: str Fix safety This rule's fix is marked as safe unless the union contains comments. For nested union, the fix will flatten type expressions into a single top-level union.",
    "option": {}
  },
  "PYI017": {
    "description": "[Status: stable] [Name: complex-assignment-in-stub] [Message: Stubs should not contain assignments to attributes or multiple targets] What it does Checks for assignments with multiple or non-name targets in stub files. Why is this bad? In general, stub files should be thought of as \"data files\" for a type checker, and are not intended to be executed. As such, it's useful to enforce that only a subset of Python syntax is allowed in a stub file, to ensure that everything in the stub is unambiguous for the type checker. The need to perform multi-assignment, or assignment to a non-name target, likely indicates a misunderstanding of how stub files are intended to be used. Example from typing import TypeAlias a = b = int class Klass: ... Klass.X: TypeAlias = int Use instead: from typing import TypeAlias a: TypeAlias = int b: TypeAlias = int class Klass: X: TypeAlias = int",
    "option": {}
  },
  "PYI018": {
    "description": "[Status: stable] [Name: unused-private-type-var] [Message: Private {type_var_like_kind} <code>{type_var_like_name}</code> is never used] What it does Checks for the presence of unused private TypeVar, ParamSpec or TypeVarTuple declarations. Why is this bad? A private TypeVar that is defined but not used is likely a mistake. It should either be used, made public, or removed to avoid confusion. A type variable is considered \"private\" if its name starts with an underscore. Example import typing import typing_extensions _T = typing.TypeVar(\"_T\") _Ts = typing_extensions.TypeVarTuple(\"_Ts\") Fix safety The fix is always marked as unsafe, as it would break your code if the type variable is imported by another module.",
    "option": {}
  },
  "PYI019": {
    "description": "[Status: stable] [Name: custom-type-var-for-self] [Message: Use <code>Self</code> instead of custom TypeVar <code>{}</code>] What it does Checks for methods that use custom TypeVars in their annotations when they could use Self instead. Why is this bad? While the semantics are often identical, using Self is more intuitive and succinct (per PEP 673) than a custom TypeVar. For example, the use of Self will typically allow for the omission of type parameters on the self and cls arguments. This check currently applies to instance methods that return self, class methods that return an instance of cls, class methods that return cls, and __new__ methods. Example from typing import TypeVar _S = TypeVar(\"_S\", bound=\"Foo\") class Foo: def __new__(cls: type[_S], *args: str, **kwargs: int) -> _S: ... def foo(self: _S, arg: bytes) -> _S: ... @classmethod def bar(cls: type[_S], arg: int) -> _S: ... Use instead: from typing import Self class Foo: def __new__(cls, *args: str, **kwargs: int) -> Self: ... def foo(self, arg: bytes) -> Self: ... @classmethod def bar(cls, arg: int) -> Self: ... Fix behaviour The fix replaces all references to the custom type variable in the method's header and body with references to Self. The fix also adds an import of Self if neither Self nor typing is already imported in the module. If your target-version setting is set to Python 3.11 or newer, the fix imports Self from the standard-library typing module; otherwise, the fix imports Self from the third-party typing_extensions backport package. If the custom type variable is a PEP-695-style TypeVar, the fix also removes the TypeVar declaration from the method's type parameter list. However, if the type variable is an old-style TypeVar, the declaration of the type variable will not be removed by this rule's fix, as the type variable could still be used by other functions, methods or classes. See unused-private-type-var for a rule that will clean up unused private type variables. Fix safety The fix is only marked as unsafe if there is the possibility that it might delete a comment from your code. Availability Because this rule relies on the third-party typing_extensions module for Python versions before 3.11, its diagnostic will not be emitted, and no fix will be offered, if typing_extensions imports have been disabled by the lint.typing-extensions linter option.",
    "option": {
      "lint.typing-extensions": {
        "option_description": "Whether to allow imports from the third-party typing_extensions module for Python versions\nbefore a symbol was added to the first-party typing module.\n\nMany rules try to import symbols from the typing module but fall back to\ntyping_extensions for earlier versions of Python. This option can be used to disable this\nfallback behavior in cases where typing_extensions is not installed.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\n# Disable `typing_extensions` imports\ntyping-extensions = false\n\n\n\n[lint]\n# Disable `typing_extensions` imports\ntyping-extensions = false",
        "option_data_type": "bool",
        "default_value": "true",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PYI020": {
    "description": "[Status: stable] [Name: quoted-annotation-in-stub] [Message: Quoted annotations should not be included in stubs] What it does Checks for quoted type annotations in stub (.pyi) files, which should be avoided. Why is this bad? Stub files natively support forward references in all contexts, as stubs are never executed at runtime. (They should be thought of as \"data files\" for type checkers and IDEs.) As such, quotes are never required for type annotations in stub files, and should be omitted. Example def function() -> \"int\": ... Use instead: def function() -> int: ...",
    "option": {}
  },
  "PYI021": {
    "description": "[Status: stable] [Name: docstring-in-stub] [Message: Docstrings should not be included in stubs] What it does Checks for the presence of docstrings in stub files. Why is this bad? Stub files should omit docstrings, as they're intended to provide type hints, rather than documentation. Example def func(param: int) -> str: \"\"\"This is a docstring.\"\"\" ... Use instead: def func(param: int) -> str: ...",
    "option": {}
  },
  "PYI024": {
    "description": "[Status: stable] [Name: collections-named-tuple] [Message: Use <code>typing.NamedTuple</code> instead of <code>collections.namedtuple</code>] What it does Checks for uses of collections.namedtuple in stub files. Why is this bad? typing.NamedTuple is the \"typed version\" of collections.namedtuple. Inheriting from typing.NamedTuple creates a custom tuple subclass in the same way as using the collections.namedtuple factory function. However, using typing.NamedTuple allows you to provide a type annotation for each field in the class. This means that type checkers will have more information to work with, and will be able to analyze your code more precisely. Example from collections import namedtuple Person = namedtuple(\"Person\", [\"name\", \"age\"]) Use instead: from typing import NamedTuple class Person(NamedTuple): name: str age: int",
    "option": {}
  },
  "PYI025": {
    "description": "[Status: stable] [Name: unaliased-collections-abc-set-import] [Message: Use <code>from collections.abc import Set as AbstractSet</code> to avoid confusion with the <code>set</code> builtin] What it does Checks for from collections.abc import Set imports that do not alias Set to AbstractSet. Why is this bad? The Set type in collections.abc is an abstract base class for set-like types. It is easily confused with, and not equivalent to, the set builtin. To avoid confusion, Set should be aliased to AbstractSet when imported. This makes it clear that the imported type is an abstract base class, and not the set builtin. Example from collections.abc import Set Use instead: from collections.abc import Set as AbstractSet Fix safety This rule's fix is marked as unsafe for Set imports defined at the top-level of a .py module. Top-level symbols are implicitly exported by the module, and so renaming a top-level symbol may break downstream modules that import it. The same is not true for .pyi files, where imported symbols are only re-exported if they are included in __all__, use a \"redundant\" import foo as foo alias, or are imported via a * import. As such, the fix is marked as safe in more cases for .pyi files.",
    "option": {}
  },
  "PYI026": {
    "description": "[Status: stable] [Name: type-alias-without-annotation] [Message: Use <code>{module}.TypeAlias</code> for type alias, e.g., <code>{name}: TypeAlias = {value}</code>] What it does Checks for type alias definitions that are not annotated with typing.TypeAlias. Why is this bad? In Python, a type alias is defined by assigning a type to a variable (e.g., Vector = list[float]). It's best to annotate type aliases with the typing.TypeAlias type to make it clear that the statement is a type alias declaration, as opposed to a normal variable assignment. Example Vector = list[float] Use instead: from typing import TypeAlias Vector: TypeAlias = list[float] Availability Because this rule relies on the third-party typing_extensions module for Python versions before 3.10, its diagnostic will not be emitted, and no fix will be offered, if typing_extensions imports have been disabled by the lint.typing-extensions linter option.",
    "option": {
      "lint.typing-extensions": {
        "option_description": "Whether to allow imports from the third-party typing_extensions module for Python versions\nbefore a symbol was added to the first-party typing module.\n\nMany rules try to import symbols from the typing module but fall back to\ntyping_extensions for earlier versions of Python. This option can be used to disable this\nfallback behavior in cases where typing_extensions is not installed.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\n# Disable `typing_extensions` imports\ntyping-extensions = false\n\n\n\n[lint]\n# Disable `typing_extensions` imports\ntyping-extensions = false",
        "option_data_type": "bool",
        "default_value": "true",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PYI029": {
    "description": "[Status: stable] [Name: str-or-repr-defined-in-stub] [Message: Defining <code>{name}</code> in a stub is almost always redundant] What it does Checks for redundant definitions of __str__ or __repr__ in stubs. Why is this bad? Defining __str__ or __repr__ in a stub is almost always redundant, as the signatures are almost always identical to those of the default equivalent, object.__str__ and object.__repr__, respectively. Example class Foo: def __repr__(self) -> str: ...",
    "option": {}
  },
  "PYI030": {
    "description": "[Status: stable] [Name: unnecessary-literal-union] [Message: Multiple literal members in a union. Use a single literal, e.g. <code>Literal[{}]</code>] What it does Checks for the presence of multiple literal types in a union. Why is this bad? Literal[\"foo\", 42] has identical semantics to Literal[\"foo\"] | Literal[42], but is clearer and more concise. Example from typing import Literal field: Literal[1] | Literal[2] | str Use instead: from typing import Literal field: Literal[1, 2] | str Fix safety This fix is marked unsafe if it would delete any comments within the replacement range. An example to illustrate where comments are preserved and where they are not: from typing import Literal field: ( # deleted comment Literal[\"a\", \"b\"] # deleted comment # deleted comment | Literal[\"c\", \"d\"] # preserved comment )",
    "option": {}
  },
  "PYI032": {
    "description": "[Status: stable] [Name: any-eq-ne-annotation] [Message: Prefer <code>object</code> to <code>Any</code> for the second parameter to <code>{method_name}</code>] What it does Checks for __eq__ and __ne__ implementations that use typing.Any as the type annotation for their second parameter. Why is this bad? The Python documentation recommends the use of object to \"indicate that a value could be any type in a typesafe manner\". Any, on the other hand, should be seen as an \"escape hatch when you need to mix dynamically and statically typed code\". Since using Any allows you to write highly unsafe code, you should generally only use Any when the semantics of your code would otherwise be inexpressible to the type checker. The expectation in Python is that a comparison of two arbitrary objects using == or != should never raise an exception. This contract can be fully expressed in the type system and does not involve requesting unsound behaviour from a type checker. As such, object is a more appropriate annotation than Any for the second parameter of the methods implementing these comparison operators -- __eq__ and __ne__. Example from typing import Any class Foo: def __eq__(self, obj: Any) -> bool: ... Use instead: class Foo: def __eq__(self, obj: object) -> bool: ...",
    "option": {}
  },
  "PYI033": {
    "description": "[Status: stable] [Name: type-comment-in-stub] [Message: Don't use type comments in stub file] What it does Checks for the use of type comments (e.g., x = 1 # type: int) in stub files. Why is this bad? Stub (.pyi) files should use type annotations directly, rather than type comments, even if they're intended to support Python 2, since stub files are not executed at runtime. The one exception is # type: ignore. Example x = 1 # type: int Use instead: x: int = 1",
    "option": {}
  },
  "PYI034": {
    "description": "[Status: stable] [Name: non-self-return-type] [Message: <code>__new__</code> methods usually return <code>self</code> at runtime] What it does Checks for methods that are annotated with a fixed return type which should instead be returning Self. Why is this bad? If methods that generally return self at runtime are annotated with a fixed return type, and the class is subclassed, type checkers will not be able to infer the correct return type. For example: class Shape: def set_scale(self, scale: float) -> Shape: self.scale = scale return self class Circle(Shape): def set_radius(self, radius: float) -> Circle: self.radius = radius return self # Type checker infers return type as `Shape`, not `Circle`. Circle().set_scale(0.5) # Thus, this expression is invalid, as `Shape` has no attribute `set_radius`. Circle().set_scale(0.5).set_radius(2.7) Specifically, this check enforces that the return type of the following methods is Self: In-place binary-operation dunder methods, like __iadd__, __imul__, etc. __new__, __enter__, and __aenter__, if those methods return the class name. __iter__ methods that return Iterator, despite the class inheriting directly from Iterator. __aiter__ methods that return AsyncIterator, despite the class inheriting directly from AsyncIterator. The rule attempts to avoid flagging methods on metaclasses, since PEP 673 specifies that Self is disallowed in metaclasses. Ruff can detect a class as being a metaclass if it inherits from a stdlib metaclass such as builtins.type or abc.ABCMeta, and additionally infers that a class may be a metaclass if it has a __new__ method with a similar signature to type.__new__. The heuristic used to identify a metaclass-like __new__ method signature is that it: Has exactly 5 parameters (including cls) Has a second parameter annotated with str Has a third parameter annotated with a tuple type Has a fourth parameter annotated with a dict type Has a fifth parameter is keyword-variadic (**kwargs) For example, the following class would be detected as a metaclass, disabling the rule: class MyMetaclass(django.db.models.base.ModelBase): def __new__(cls, name: str, bases: tuple[Any, ...], attrs: dict[str, Any], **kwargs: Any) -> MyMetaclass: ... Example class Foo: def __new__(cls, *args: Any, **kwargs: Any) -> Foo: ... def __enter__(self) -> Foo: ... async def __aenter__(self) -> Foo: ... def __iadd__(self, other: Foo) -> Foo: ... Use instead: from typing_extensions import Self class Foo: def __new__(cls, *args: Any, **kwargs: Any) -> Self: ... def __enter__(self) -> Self: ... async def __aenter__(self) -> Self: ... def __iadd__(self, other: Foo) -> Self: ... Fix safety This rule's fix is marked as unsafe as it changes the meaning of your type annotations. Availability Because this rule relies on the third-party typing_extensions module for Python versions before 3.11, its diagnostic will not be emitted, and no fix will be offered, if typing_extensions imports have been disabled by the lint.typing-extensions linter option.",
    "option": {
      "lint.typing-extensions": {
        "option_description": "Whether to allow imports from the third-party typing_extensions module for Python versions\nbefore a symbol was added to the first-party typing module.\n\nMany rules try to import symbols from the typing module but fall back to\ntyping_extensions for earlier versions of Python. This option can be used to disable this\nfallback behavior in cases where typing_extensions is not installed.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\n# Disable `typing_extensions` imports\ntyping-extensions = false\n\n\n\n[lint]\n# Disable `typing_extensions` imports\ntyping-extensions = false",
        "option_data_type": "bool",
        "default_value": "true",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PYI035": {
    "description": "[Status: stable] [Name: unassigned-special-variable-in-stub] [Message: <code>{name}</code> in a stub file must have a value, as it has the same semantics as <code>{name}</code> at runtime] What it does Checks that __all__, __match_args__, and __slots__ variables are assigned to values when defined in stub files. Why is this bad? Special variables like __all__ have the same semantics in stub files as they do in Python modules, and so should be consistent with their runtime counterparts. Example __all__: list[str] Use instead: __all__: list[str] = [\"foo\", \"bar\"]",
    "option": {}
  },
  "PYI036": {
    "description": "[Status: stable] [Name: bad-exit-annotation] [Message: Star-args in <code>{method_name}</code> should be annotated with <code>object</code>] What it does Checks for incorrect function signatures on __exit__ and __aexit__ methods. Why is this bad? Improperly annotated __exit__ and __aexit__ methods can cause unexpected behavior when interacting with type checkers. Example from types import TracebackType class Foo: def __exit__( self, typ: BaseException, exc: BaseException, tb: TracebackType ) -> None: ... Use instead: from types import TracebackType class Foo: def __exit__( self, typ: type[BaseException] | None, exc: BaseException | None, tb: TracebackType | None, ) -> None: ...",
    "option": {}
  },
  "PYI041": {
    "description": "[Status: stable] [Name: redundant-numeric-union] [Message: Use <code>{supertype}</code> instead of <code>{subtype} | {supertype}</code>] What it does Checks for parameter annotations that contain redundant unions between builtin numeric types (e.g., int | float). Why is this bad? The typing specification states: Pythons numeric types complex, float and int are not subtypes of each other, but to support common use cases, the type system contains a straightforward shortcut: when an argument is annotated as having type float, an argument of type int is acceptable; similar, for an argument annotated as having type complex, arguments of type float or int are acceptable. As such, a union that includes both int and float is redundant in the specific context of a parameter annotation, as it is equivalent to a union that only includes float. For readability and clarity, unions should omit redundant elements. Example def foo(x: float | int | str) -> None: ... Use instead: def foo(x: float | str) -> None: ... Fix safety This rule's fix is marked as safe, unless the type annotation contains comments. Note that while the fix may flatten nested unions into a single top-level union, the semantics of the annotation will remain unchanged.",
    "option": {}
  },
  "PYI042": {
    "description": "[Status: stable] [Name: snake-case-type-alias] [Message: Type alias <code>{name}</code> should be CamelCase] What it does Checks for type aliases that do not use the CamelCase naming convention. Why is this bad? It's conventional to use the CamelCase naming convention for type aliases, to distinguish them from other variables. Example from typing import TypeAlias type_alias_name: TypeAlias = int Use instead: from typing import TypeAlias TypeAliasName: TypeAlias = int",
    "option": {}
  },
  "PYI043": {
    "description": "[Status: stable] [Name: t-suffixed-type-alias] [Message: Private type alias <code>{name}</code> should not be suffixed with <code>T</code> (the <code>T</code> suffix implies that an object is a <code>TypeVar</code>)] What it does Checks for private type alias definitions suffixed with 'T'. Why is this bad? It's conventional to use the 'T' suffix for type variables; the use of such a suffix implies that the object is a TypeVar. Adding the 'T' suffix to a non-TypeVar, it can be misleading and should be avoided. Example from typing import TypeAlias _MyTypeT: TypeAlias = int Use instead: from typing import TypeAlias _MyType: TypeAlias = int",
    "option": {}
  },
  "PYI044": {
    "description": "[Status: stable] [Name: future-annotations-in-stub] [Message: <code>from __future__ import annotations</code> has no effect in stub files, since type checkers automatically treat stubs as having those semantics] What it does Checks for the presence of the from __future__ import annotations import statement in stub files. Why is this bad? Stub files natively support forward references in all contexts, as stubs are never executed at runtime. (They should be thought of as \"data files\" for type checkers.) As such, the from __future__ import annotations import statement has no effect and should be omitted.",
    "option": {}
  },
  "PYI045": {
    "description": "[Status: stable] [Name: iter-method-return-iterable] [Message: <code>__aiter__</code> methods should return an <code>AsyncIterator</code>, not an <code>AsyncIterable</code>] What it does Checks for __iter__ methods in stubs that return Iterable[T] instead of an Iterator[T]. Why is this bad? __iter__ methods should always should return an Iterator of some kind, not an Iterable. In Python, an Iterable is an object that has an __iter__ method; an Iterator is an object that has __iter__ and __next__ methods. All __iter__ methods are expected to return Iterators. Type checkers may not always recognize an object as being iterable if its __iter__ method does not return an Iterator. Every Iterator is an Iterable, but not every Iterable is an Iterator. For example, list is an Iterable, but not an Iterator; you can obtain an iterator over a list's elements by passing the list to iter(): >>> import collections.abc >>> x = [42] >>> isinstance(x, collections.abc.Iterable) True >>> isinstance(x, collections.abc.Iterator) False >>> next(x) Traceback (most recent call last): File \"<stdin>\", line 1, in <module> TypeError: 'list' object is not an iterator >>> y = iter(x) >>> isinstance(y, collections.abc.Iterable) True >>> isinstance(y, collections.abc.Iterator) True >>> next(y) 42 Using Iterable rather than Iterator as a return type for an __iter__ methods would imply that you would not necessarily be able to call next() on the returned object, violating the expectations of the interface. Example import collections.abc class Klass: def __iter__(self) -> collections.abc.Iterable[str]: ... Use instead: import collections.abc class Klass: def __iter__(self) -> collections.abc.Iterator[str]: ...",
    "option": {}
  },
  "PYI046": {
    "description": "[Status: stable] [Name: unused-private-protocol] [Message: Private protocol <code>{name}</code> is never used] What it does Checks for the presence of unused private typing.Protocol definitions. Why is this bad? A private typing.Protocol that is defined but not used is likely a mistake. It should either be used, made public, or removed to avoid confusion. Example import typing class _PrivateProtocol(typing.Protocol): foo: int Use instead: import typing class _PrivateProtocol(typing.Protocol): foo: int def func(arg: _PrivateProtocol) -> None: ...",
    "option": {}
  },
  "PYI047": {
    "description": "[Status: stable] [Name: unused-private-type-alias] [Message: Private TypeAlias <code>{name}</code> is never used] What it does Checks for the presence of unused private type aliases. Why is this bad? A private type alias that is defined but not used is likely a mistake. It should either be used, made public, or removed to avoid confusion. Example import typing _UnusedTypeAlias: typing.TypeAlias = int Use instead: import typing _UsedTypeAlias: typing.TypeAlias = int def func(arg: _UsedTypeAlias) -> _UsedTypeAlias: ...",
    "option": {}
  },
  "PYI048": {
    "description": "[Status: stable] [Name: stub-body-multiple-statements] [Message: Function body must contain exactly one statement] What it does Checks for functions in stub (.pyi) files that contain multiple statements. Why is this bad? Stub files are never executed, and are only intended to define type hints. As such, functions in stub files should not contain functional code, and should instead contain only a single statement (e.g., ...). Example def function(): x = 1 y = 2 return x + y Use instead: def function(): ...",
    "option": {}
  },
  "PYI049": {
    "description": "[Status: stable] [Name: unused-private-typed-dict] [Message: Private TypedDict <code>{name}</code> is never used] What it does Checks for the presence of unused private typing.TypedDict definitions. Why is this bad? A private typing.TypedDict that is defined but not used is likely a mistake. It should either be used, made public, or removed to avoid confusion. Example import typing class _UnusedPrivateTypedDict(typing.TypedDict): foo: list[int] Use instead: import typing class _UsedPrivateTypedDict(typing.TypedDict): foo: set[str] def func(arg: _UsedPrivateTypedDict) -> _UsedPrivateTypedDict: ...",
    "option": {}
  },
  "PYI050": {
    "description": "[Status: stable] [Name: no-return-argument-annotation-in-stub] [Message: Prefer <code>{module}.Never</code> over <code>NoReturn</code> for argument annotations] What it does Checks for uses of typing.NoReturn (and typing_extensions.NoReturn) for parameter annotations. Why is this bad? Prefer Never over NoReturn for parameter annotations. Never has a clearer name in these contexts, since it makes little sense to talk about a parameter annotation \"not returning\". This is a purely stylistic lint: the two types have identical semantics for type checkers. Both represent Python's \"bottom type\" (a type that has no members). Example from typing import NoReturn def foo(x: NoReturn): ... Use instead: from typing import Never def foo(x: Never): ...",
    "option": {}
  },
  "PYI051": {
    "description": "[Status: stable] [Name: redundant-literal-union] [Message: <code>Literal[{literal}]</code> is redundant in a union with <code>{builtin_type}</code>] What it does Checks for redundant unions between a Literal and a builtin supertype of that Literal. Why is this bad? Using a Literal type in a union with its builtin supertype is redundant, as the supertype will be strictly more general than the Literal type. For example, Literal[\"A\"] | str is equivalent to str, and Literal[1] | int is equivalent to int, as str and int are the supertypes of \"A\" and 1 respectively. Example from typing import Literal x: Literal[\"A\", b\"B\"] | str Use instead: from typing import Literal x: Literal[b\"B\"] | str",
    "option": {}
  },
  "PYI052": {
    "description": "[Status: stable] [Name: unannotated-assignment-in-stub] [Message: Need type annotation for <code>{name}</code>] What it does Checks for unannotated assignments in stub (.pyi) files. Why is this bad? Stub files exist to provide type hints, and are never executed. As such, all assignments in stub files should be annotated with a type.",
    "option": {}
  },
  "PYI053": {
    "description": "[Status: stable] [Name: string-or-bytes-too-long] [Message: String and bytes literals longer than 50 characters are not permitted] What it does Checks for the use of string and bytes literals longer than 50 characters in stub (.pyi) files. Why is this bad? If a function or variable has a default value where the string or bytes representation is greater than 50 characters long, it is likely to be an implementation detail or a constant that varies depending on the system you're running on. Although IDEs may find them useful, default values are ignored by type checkers, the primary consumers of stub files. Replace very long constants with ellipses (...) to simplify the stub. Example def foo(arg: str = \"51 character stringgggggggggggggggggggggggggggggggg\") -> None: ... Use instead: def foo(arg: str = ...) -> None: ...",
    "option": {}
  },
  "PYI054": {
    "description": "[Status: stable] [Name: numeric-literal-too-long] [Message: Numeric literals with a string representation longer than ten characters are not permitted] What it does Checks for numeric literals with a string representation longer than ten characters. Why is this bad? If a function has a default value where the literal representation is greater than 10 characters, the value is likely to be an implementation detail or a constant that varies depending on the system you're running on. Default values like these should generally be omitted from stubs. Use ellipses (...) instead. Example def foo(arg: int = 693568516352839939918568862861217771399698285293568) -> None: ... Use instead: def foo(arg: int = ...) -> None: ...",
    "option": {}
  },
  "PYI055": {
    "description": "[Status: stable] [Name: unnecessary-type-union] [Message: Multiple <code>type</code> members in a union. Combine them into one, e.g., <code>type[{union_str}]</code>.] What it does Checks for the presence of multiple types in a union. Why is this bad? type[T | S] has identical semantics to type[T] | type[S] in a type annotation, but is cleaner and more concise. Example field: type[int] | type[float] | str Use instead: field: type[int | float] | str Fix safety This rule's fix is marked as safe, unless the type annotation contains comments. Note that while the fix may flatten nested unions into a single top-level union, the semantics of the annotation will remain unchanged.",
    "option": {}
  },
  "PYI056": {
    "description": "[Status: stable] [Name: unsupported-method-call-on-all] [Message: Calling <code>.{name}()</code> on <code>__all__</code> may not be supported by all type checkers (use <code>+=</code> instead)] What it does Checks that append, extend and remove methods are not called on __all__. Why is this bad? Different type checkers have varying levels of support for calling these methods on __all__. Instead, use the += operator to add items to __all__, which is known to be supported by all major type checkers. Example import sys __all__ = [\"A\", \"B\"] if sys.version_info >= (3, 10): __all__.append(\"C\") if sys.version_info >= (3, 11): __all__.remove(\"B\") Use instead: import sys __all__ = [\"A\"] if sys.version_info < (3, 11): __all__ += [\"B\"] if sys.version_info >= (3, 10): __all__ += [\"C\"]",
    "option": {}
  },
  "PYI057": {
    "description": "[Status: stable] [Name: byte-string-usage] [Message: Do not use <code>{origin}.ByteString</code>, which has unclear semantics and is deprecated] What it does Checks for uses of typing.ByteString or collections.abc.ByteString. Why is this bad? ByteString has been deprecated since Python 3.9 and will be removed in Python 3.17. The Python documentation recommends using either collections.abc.Buffer (or the typing_extensions backport on Python \\<3.12) or a union like bytes | bytearray | memoryview instead. Example from typing import ByteString Use instead: from collections.abc import Buffer",
    "option": {}
  },
  "PYI058": {
    "description": "[Status: stable] [Name: generator-return-from-iter-method] [Message: Use <code>{return_type}</code> as the return value for simple <code>{method}</code> methods] What it does Checks for simple __iter__ methods that return Generator, and for simple __aiter__ methods that return AsyncGenerator. Why is this bad? Using (Async)Iterator for these methods is simpler and more elegant. More importantly, it also reflects the fact that the precise kind of iterator returned from an __iter__ method is usually an implementation detail that could change at any time. Type annotations help define a contract for a function; implementation details should not leak into that contract. For example: from collections.abc import AsyncGenerator, Generator from typing import Any class CustomIterator: def __iter__(self) -> Generator: yield from range(42) class CustomIterator2: def __iter__(self) -> Generator[str, Any, None]: yield from \"abcdefg\" Use instead: from collections.abc import Iterator class CustomIterator: def __iter__(self) -> Iterator: yield from range(42) class CustomIterator2: def __iter__(self) -> Iterator[str]: yield from \"abdefg\" Fix safety This rule tries hard to avoid false-positive errors, and the rule's fix should always be safe for .pyi stub files. However, there is a slightly higher chance that a false positive might be emitted by this rule when applied to runtime Python (.py files). As such, the fix is marked as unsafe for any __iter__ or __aiter__ method in a .py file that has more than two statements (including docstrings) in its body.",
    "option": {}
  },
  "PYI059": {
    "description": "[Status: stable] [Name: generic-not-last-base-class] [Message: <code>Generic[]</code> should always be the last base class] What it does Checks for classes inheriting from typing.Generic[] where Generic[] is not the last base class in the bases tuple. Why is this bad? If Generic[] is not the final class in the bases tuple, unexpected behaviour can occur at runtime (See this CPython issue for an example). The rule is also applied to stub files, where it won't cause issues at runtime. This is because type checkers may not be able to infer an accurate MRO for the class, which could lead to unexpected or inaccurate results when they analyze your code. For example: from collections.abc import Container, Iterable, Sized from typing import Generic, TypeVar T = TypeVar(\"T\") K = TypeVar(\"K\") V = TypeVar(\"V\") class LinkedList(Generic[T], Sized): def push(self, item: T) -> None: self._items.append(item) class MyMapping( Generic[K, V], Iterable[tuple[K, V]], Container[tuple[K, V]], ): ... Use instead: from collections.abc import Container, Iterable, Sized from typing import Generic, TypeVar T = TypeVar(\"T\") K = TypeVar(\"K\") V = TypeVar(\"V\") class LinkedList(Sized, Generic[T]): def push(self, item: T) -> None: self._items.append(item) class MyMapping( Iterable[tuple[K, V]], Container[tuple[K, V]], Generic[K, V], ): ... Fix safety This rule's fix is always unsafe because reordering base classes can change the behavior of the code by modifying the class's MRO. The fix will also delete trailing comments after the Generic base class in multi-line base class lists, if any are present. Fix availability This rule's fix is only available when there are no *args present in the base class list.",
    "option": {}
  },
  "PYI061": {
    "description": "[Status: stable] [Name: redundant-none-literal] [Message: Use <code>None</code> rather than <code>Literal[None]</code>] What it does Checks for redundant Literal[None] annotations. Why is this bad? While Literal[None] is a valid type annotation, it is semantically equivalent to None. Prefer None over Literal[None] for both consistency and readability. Example from typing import Literal Literal[None] Literal[1, 2, 3, \"foo\", 5, None] Use instead: from typing import Literal None Literal[1, 2, 3, \"foo\", 5] | None Fix safety and availability This rule's fix is marked as safe unless the literal contains comments. There is currently no fix available when applying the fix would lead to a TypeError from an expression of the form None | None or when we are unable to import the symbol typing.Optional and the Python version is 3.9 or below.",
    "option": {}
  },
  "PYI062": {
    "description": "[Status: stable] [Name: duplicate-literal-member] [Message: Duplicate literal member <code>{}</code>] What it does Checks for duplicate members in a typing.Literal[] slice. Why is this bad? Duplicate literal members are redundant and should be removed. Example from typing import Literal foo: Literal[\"a\", \"b\", \"a\"] Use instead: from typing import Literal foo: Literal[\"a\", \"b\"] Fix safety This rule's fix is marked as safe, unless the type annotation contains comments. Note that while the fix may flatten nested literals into a single top-level literal, the semantics of the annotation will remain unchanged.",
    "option": {}
  },
  "PYI063": {
    "description": "[Status: stable] [Name: pep484-style-positional-only-parameter] [Message: Use PEP 570 syntax for positional-only parameters] What it does Checks for the presence of PEP 484-style positional-only parameters. Why is this bad? Historically, PEP 484 recommended prefixing parameter names with double underscores (__) to indicate to a type checker that they were positional-only. However, PEP 570 (introduced in Python 3.8) introduced dedicated syntax for positional-only arguments. If a forward slash (/) is present in a function signature on Python 3.8+, all parameters prior to the slash are interpreted as positional-only. The new syntax should be preferred as it is more widely used, more concise and more readable. It is also respected by Python at runtime, whereas the old-style syntax was only understood by type checkers. Example def foo(__x: int) -> None: ... Use instead: def foo(x: int, /) -> None: ...",
    "option": {
      "target-version": {
        "option_description": "The minimum Python version to target, e.g., when considering automatic\ncode upgrades, like rewriting type annotations. Ruff will not propose\nchanges using features that are not available in the given version.\n\nFor example, to represent supporting Python >=3.11 or ==3.11\nspecify target-version = \"py311\".\n\nIf you're already using a pyproject.toml file, we recommend\nproject.requires-python instead, as it's based on Python packaging\nstandards, and will be respected by other tools. For example, Ruff\ntreats the following as identical to target-version = \"py38\":\n\n[project]\nrequires-python = \">=3.8\"\n\nIf both are specified, target-version takes precedence over\nrequires-python. See Inferring the Python version\nfor a complete description of how the target-version is determined\nwhen left unspecified.\n\nNote that a stub file can sometimes make use of a typing feature\nbefore it is available at runtime, as long as the stub does not make\nuse of new syntax. For example, a type checker will understand\nint | str in a stub as being a Union type annotation, even if the\ntype checker is run using Python 3.9, despite the fact that the |\noperator can only be used to create union types at runtime on Python\n3.10+. As such, Ruff will often recommend newer features in a stub\nfile than it would for an equivalent runtime file with the same target\nversion.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"\n\n\n\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"",
        "option_data_type": "\"py37\" | \"py38\" | \"py39\" | \"py310\" | \"py311\" | \"py312\" | \"py313\" | \"py314\"",
        "default_value": "\"py310\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PYI064": {
    "description": "[Status: stable] [Name: redundant-final-literal] [Message: <code>Final[Literal[{literal}]]</code> can be replaced with a bare <code>Final</code>] What it does Checks for redundant Final[Literal[...]] annotations. Why is this bad? All constant variables annotated as Final are understood as implicitly having Literal types by a type checker. As such, a Final[Literal[...]] annotation can often be replaced with a bare Final, annotation, which will have the same meaning to the type checker while being more concise and more readable. Example from typing import Final, Literal x: Final[Literal[42]] y: Final[Literal[42]] = 42 Use instead: from typing import Final, Literal x: Final = 42 y: Final = 42",
    "option": {}
  },
  "PYI066": {
    "description": "[Status: stable] [Name: bad-version-info-order] [Message: Put branches for newer Python versions first when branching on <code>sys.version_info</code> comparisons] What it does Checks for code that branches on sys.version_info comparisons where branches corresponding to older Python versions come before branches corresponding to newer Python versions. Why is this bad? As a convention, branches that correspond to newer Python versions should come first. This makes it easier to understand the desired behavior, which typically corresponds to the latest Python versions. This rule enforces the convention by checking for if tests that compare sys.version_info with < rather than >=. By default, this rule only applies to stub files. In preview, it will also flag this anti-pattern in non-stub files. Example import sys if sys.version_info < (3, 10): def read_data(x, *, preserve_order=True): ... else: def read_data(x): ... Use instead: if sys.version_info >= (3, 10): def read_data(x): ... else: def read_data(x, *, preserve_order=True): ...",
    "option": {}
  },
  "PT001": {
    "description": "[Status: stable] [Name: pytest-fixture-incorrect-parentheses-style] [Message: Use <code>@pytest.fixture{expected}</code> over <code>@pytest.fixture{actual}</code>] What it does Checks for argument-free @pytest.fixture() decorators with or without parentheses, depending on the lint.flake8-pytest-style.fixture-parentheses setting. Why is this bad? If a @pytest.fixture() doesn't take any arguments, the parentheses are optional. Either removing those unnecessary parentheses or requiring them for all fixtures is fine, but it's best to be consistent. The rule defaults to removing unnecessary parentheses, to match the documentation of the official pytest projects. Example import pytest @pytest.fixture() def my_fixture(): ... Use instead: import pytest @pytest.fixture def my_fixture(): ... Fix safety This rule's fix is marked as unsafe if there's comments in the pytest.fixture decorator. For example, the fix would be marked as unsafe in the following case: import pytest @pytest.fixture( # comment # scope = \"module\" ) def my_fixture(): ...",
    "option": {
      "lint.flake8-pytest-style.fixture-parentheses": {
        "option_description": "Boolean flag specifying whether @pytest.fixture() without parameters\nshould have parentheses. If the option is set to false (the default),\n@pytest.fixture is valid and @pytest.fixture() is invalid. If set\nto true, @pytest.fixture() is valid and @pytest.fixture is\ninvalid.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-pytest-style]\nfixture-parentheses = true\n\n\n\n[lint.flake8-pytest-style]\nfixture-parentheses = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PT002": {
    "description": "[Status: stable] [Name: pytest-fixture-positional-args] [Message: Configuration for fixture <code>{function}</code> specified via positional args, use kwargs] What it does Checks for pytest.fixture calls with positional arguments. Why is this bad? For clarity and consistency, prefer using keyword arguments to specify fixture configuration. Example import pytest @pytest.fixture(\"module\") def my_fixture(): ... Use instead: import pytest @pytest.fixture(scope=\"module\") def my_fixture(): ...",
    "option": {}
  },
  "PT003": {
    "description": "[Status: stable] [Name: pytest-extraneous-scope-function] [Message: <code>scope='function'</code> is implied in <code>@pytest.fixture()</code>] What it does Checks for pytest.fixture calls with scope=\"function\". Why is this bad? scope=\"function\" can be omitted, as it is the default. Example import pytest @pytest.fixture(scope=\"function\") def my_fixture(): ... Use instead: import pytest @pytest.fixture() def my_fixture(): ...",
    "option": {}
  },
  "PT006": {
    "description": "[Status: stable] [Name: pytest-parametrize-names-wrong-type] [Message: Wrong type passed to first argument of <code>pytest.mark.parametrize</code>; expected {expected_string}] What it does Checks for the type of parameter names passed to pytest.mark.parametrize. Why is this bad? The argnames argument of pytest.mark.parametrize takes a string or a sequence of strings. For a single parameter, it's preferable to use a string. For multiple parameters, it's preferable to use the style configured via the lint.flake8-pytest-style.parametrize-names-type setting. Example import pytest # single parameter, always expecting string @pytest.mark.parametrize((\"param\",), [1, 2, 3]) def test_foo(param): ... # multiple parameters, expecting tuple @pytest.mark.parametrize([\"param1\", \"param2\"], [(1, 2), (3, 4)]) def test_bar(param1, param2): ... # multiple parameters, expecting tuple @pytest.mark.parametrize(\"param1,param2\", [(1, 2), (3, 4)]) def test_baz(param1, param2): ... Use instead: import pytest @pytest.mark.parametrize(\"param\", [1, 2, 3]) def test_foo(param): ... @pytest.mark.parametrize((\"param1\", \"param2\"), [(1, 2), (3, 4)]) def test_bar(param1, param2): ...",
    "option": {
      "lint.flake8-pytest-style.parametrize-names-type": {
        "option_description": "Expected type for multiple argument names in @pytest.mark.parametrize.\nThe following values are supported:\n\ncsv  a comma-separated list, e.g.\n  @pytest.mark.parametrize(\"name1,name2\", ...)\ntuple (default)  e.g.\n  @pytest.mark.parametrize((\"name1\", \"name2\"), ...)\nlist  e.g. @pytest.mark.parametrize([\"name1\", \"name2\"], ...)\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-pytest-style]\nparametrize-names-type = \"list\"\n\n\n\n[lint.flake8-pytest-style]\nparametrize-names-type = \"list\"",
        "option_data_type": "\"csv\" | \"tuple\" | \"list\"",
        "default_value": "tuple",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PT007": {
    "description": "[Status: stable] [Name: pytest-parametrize-values-wrong-type] [Message: Wrong values type in <code>pytest.mark.parametrize</code> expected <code>{values}</code> of <code>{row}</code>] What it does Checks for the type of parameter values passed to pytest.mark.parametrize. Why is this bad? The argvalues argument of pytest.mark.parametrize takes an iterator of parameter values, which can be provided as lists or tuples. To aid in readability, it's recommended to use a consistent style for the list of values rows, and, in the case of multiple parameters, for each row of values. The style for the list of values rows can be configured via the lint.flake8-pytest-style.parametrize-values-type setting, while the style for each row of values can be configured via the lint.flake8-pytest-style.parametrize-values-row-type setting. For example, lint.flake8-pytest-style.parametrize-values-type will lead to the following expectations: tuple: @pytest.mark.parametrize(\"value\", (\"a\", \"b\", \"c\")) list: @pytest.mark.parametrize(\"value\", [\"a\", \"b\", \"c\"]) Similarly, lint.flake8-pytest-style.parametrize-values-row-type will lead to the following expectations: tuple: @pytest.mark.parametrize((\"key\", \"value\"), [(\"a\", \"b\"), (\"c\", \"d\")]) list: @pytest.mark.parametrize((\"key\", \"value\"), [[\"a\", \"b\"], [\"c\", \"d\"]]) Example import pytest # expected list, got tuple @pytest.mark.parametrize(\"param\", (1, 2)) def test_foo(param): ... # expected top-level list, got tuple @pytest.mark.parametrize( (\"param1\", \"param2\"), ( (1, 2), (3, 4), ), ) def test_bar(param1, param2): ... # expected individual rows to be tuples, got lists @pytest.mark.parametrize( (\"param1\", \"param2\"), [ [1, 2], [3, 4], ], ) def test_baz(param1, param2): ... Use instead: import pytest @pytest.mark.parametrize(\"param\", [1, 2, 3]) def test_foo(param): ... @pytest.mark.parametrize((\"param1\", \"param2\"), [(1, 2), (3, 4)]) def test_bar(param1, param2): ...",
    "option": {
      "lint.flake8-pytest-style.parametrize-values-type": {
        "option_description": "Expected type for the list of values rows in @pytest.mark.parametrize.\nThe following values are supported:\n\ntuple  e.g. @pytest.mark.parametrize(\"name\", (1, 2, 3))\nlist (default)  e.g. @pytest.mark.parametrize(\"name\", [1, 2, 3])\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-pytest-style]\nparametrize-values-type = \"tuple\"\n\n\n\n[lint.flake8-pytest-style]\nparametrize-values-type = \"tuple\"",
        "option_data_type": "\"tuple\" | \"list\"",
        "default_value": "list",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.flake8-pytest-style.parametrize-values-row-type": {
        "option_description": "Expected type for each row of values in @pytest.mark.parametrize in\ncase of multiple parameters. The following values are supported:\n\ntuple (default)  e.g.\n  @pytest.mark.parametrize((\"name1\", \"name2\"), [(1, 2), (3, 4)])\nlist  e.g.\n  @pytest.mark.parametrize((\"name1\", \"name2\"), [[1, 2], [3, 4]])\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-pytest-style]\nparametrize-values-row-type = \"list\"\n\n\n\n[lint.flake8-pytest-style]\nparametrize-values-row-type = \"list\"",
        "option_data_type": "\"tuple\" | \"list\"",
        "default_value": "tuple",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PT008": {
    "description": "[Status: stable] [Name: pytest-patch-with-lambda] [Message: Use <code>return_value=</code> instead of patching with <code>lambda</code>] What it does Checks for mocked calls that use a dummy lambda function instead of return_value. Why is this bad? When patching calls, an explicit return_value better conveys the intent than a lambda function, assuming the lambda does not use the arguments passed to it. return_value is also robust to changes in the patched function's signature, and enables additional assertions to verify behavior. For example, return_value allows for verification of the number of calls or the arguments passed to the patched function via assert_called_once_with and related methods. Example def test_foo(mocker): mocker.patch(\"module.target\", lambda x, y: 7) Use instead: def test_foo(mocker): mocker.patch(\"module.target\", return_value=7) # If the lambda makes use of the arguments, no diagnostic is emitted. mocker.patch(\"module.other_target\", lambda x, y: x)",
    "option": {}
  },
  "PT009": {
    "description": "[Status: stable] [Name: pytest-unittest-assertion] [Message: Use a regular <code>assert</code> instead of unittest-style <code>{assertion}</code>] What it does Checks for uses of assertion methods from the unittest module. Why is this bad? To make use of pytest's assertion rewriting, a regular assert statement is preferred over unittest's assertion methods. Example import unittest class TestFoo(unittest.TestCase): def test_foo(self): self.assertEqual(a, b) Use instead: import unittest class TestFoo(unittest.TestCase): def test_foo(self): assert a == b",
    "option": {}
  },
  "PT010": {
    "description": "[Status: stable] [Name: pytest-raises-without-exception] [Message: Set the expected exception in <code>pytest.raises()</code>] What it does Checks for pytest.raises calls without an expected exception. Why is this bad? pytest.raises expects to receive an expected exception as its first argument. If omitted, the pytest.raises call will fail at runtime. The rule will also accept calls without an expected exception but with match and/or check keyword arguments, which are also valid after pytest version 8.4.0. Example import pytest def test_foo(): with pytest.raises(): do_something() Use instead: import pytest def test_foo(): with pytest.raises(SomeException): do_something()",
    "option": {}
  },
  "PT011": {
    "description": "[Status: stable] [Name: pytest-raises-too-broad] [Message: <code>pytest.raises({exception})</code> is too broad, set the <code>match</code> parameter or use a more specific exception] What it does Checks for pytest.raises calls without a match or check parameter. Why is this bad? pytest.raises(Error) will catch any Error and may catch errors that are unrelated to the code under test. To avoid this, pytest.raises should be called with a match parameter (to constrain the exception message) or a check parameter (to constrain the exception itself). The exception names that require a match or check parameter can be configured via the lint.flake8-pytest-style.raises-require-match-for and lint.flake8-pytest-style.raises-extend-require-match-for settings. Example import pytest def test_foo(): with pytest.raises(ValueError): ... # empty string is also an error with pytest.raises(ValueError, match=\"\"): ... Use instead: import pytest def test_foo(): with pytest.raises(ValueError, match=\"expected message\"): ... Or, for pytest 8.4.0 and later: import pytest def test_foo(): with pytest.raises(OSError, check=lambda e: e.errno == 42): ...",
    "option": {
      "lint.flake8-pytest-style.raises-require-match-for": {
        "option_description": "List of exception names that require a match= parameter in a\npytest.raises() call.\n\nSupports glob patterns. For more information on the glob syntax, refer\nto the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-pytest-style]\nraises-require-match-for = [\"requests.RequestException\"]\n\n\n\n[lint.flake8-pytest-style]\nraises-require-match-for = [\"requests.RequestException\"]",
        "option_data_type": "list[str]",
        "default_value": "[\"BaseException\", \"Exception\", \"ValueError\", \"OSError\", \"IOError\", \"EnvironmentError\", \"socket.error\"]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.flake8-pytest-style.raises-extend-require-match-for": {
        "option_description": "List of additional exception names that require a match= parameter in a\npytest.raises() call. This extends the default list of exceptions\nthat require a match= parameter.\nThis option is useful if you want to extend the default list of\nexceptions that require a match= parameter without having to specify\nthe entire list.\nNote that this option does not remove any exceptions from the default\nlist.\n\nSupports glob patterns. For more information on the glob syntax, refer\nto the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-pytest-style]\nraises-extend-require-match-for = [\"requests.RequestException\"]\n\n\n\n[lint.flake8-pytest-style]\nraises-extend-require-match-for = [\"requests.RequestException\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PT012": {
    "description": "[Status: stable] [Name: pytest-raises-with-multiple-statements] [Message: <code>pytest.raises()</code> block should contain a single simple statement] What it does Checks for pytest.raises context managers with multiple statements. This rule allows pytest.raises bodies to contain for loops with empty bodies (e.g., pass or ... statements), to test iterator behavior. Why is this bad? When a pytest.raises is used as a context manager and contains multiple statements, it can lead to the test passing when it actually should fail. A pytest.raises context manager should only contain a single simple statement that raises the expected exception. Example import pytest def test_foo(): with pytest.raises(MyError): setup() func_to_test() # not executed if `setup()` raises `MyError` assert foo() # not executed Use instead: import pytest def test_foo(): setup() with pytest.raises(MyError): func_to_test() assert foo()",
    "option": {}
  },
  "PT013": {
    "description": "[Status: stable] [Name: pytest-incorrect-pytest-import] [Message: Incorrect import of <code>pytest</code>; use <code>import pytest</code> instead] What it does Checks for incorrect import of pytest. Why is this bad? For consistency, pytest should be imported as import pytest and its members should be accessed in the form of pytest.xxx.yyy for consistency Example import pytest as pt from pytest import fixture Use instead: import pytest",
    "option": {}
  },
  "PT014": {
    "description": "[Status: stable] [Name: pytest-duplicate-parametrize-test-cases] [Message: Duplicate of test case at index {index} in <code>pytest.mark.parametrize</code>] What it does Checks for duplicate test cases in pytest.mark.parametrize. Why is this bad? Duplicate test cases are redundant and should be removed. Example import pytest @pytest.mark.parametrize( (\"param1\", \"param2\"), [ (1, 2), (1, 2), ], ) def test_foo(param1, param2): ... Use instead: import pytest @pytest.mark.parametrize( (\"param1\", \"param2\"), [ (1, 2), ], ) def test_foo(param1, param2): ... Fix safety This rule's fix is marked as unsafe, as tests that rely on mutable global state may be affected by removing duplicate test cases.",
    "option": {}
  },
  "PT015": {
    "description": "[Status: stable] [Name: pytest-assert-always-false] [Message: Assertion always fails, replace with <code>pytest.fail()</code>] What it does Checks for assert statements whose test expression is a falsy value. Why is this bad? pytest.fail conveys the intent more clearly than assert falsy_value. Example def test_foo(): if some_condition: assert False, \"some_condition was True\" Use instead: import pytest def test_foo(): if some_condition: pytest.fail(\"some_condition was True\") ...",
    "option": {}
  },
  "PT016": {
    "description": "[Status: stable] [Name: pytest-fail-without-message] [Message: No message passed to <code>pytest.fail()</code>] What it does Checks for pytest.fail calls without a message. Why is this bad? pytest.fail calls without a message make it harder to understand and debug test failures. Example import pytest def test_foo(): pytest.fail() def test_bar(): pytest.fail(\"\") def test_baz(): pytest.fail(reason=\"\") Use instead: import pytest def test_foo(): pytest.fail(\"...\") def test_bar(): pytest.fail(reason=\"...\")",
    "option": {}
  },
  "PT017": {
    "description": "[Status: stable] [Name: pytest-assert-in-except] [Message: Found assertion on exception <code>{name}</code> in <code>except</code> block, use <code>pytest.raises()</code> instead] What it does Checks for assert statements in except clauses. Why is this bad? When testing for exceptions, pytest.raises() should be used instead of assert statements in except clauses, as it's more explicit and idiomatic. Further, pytest.raises() will fail if the exception is not raised, unlike the assert statement. Example def test_foo(): try: 1 / 0 except ZeroDivisionError as e: assert e.args Use instead: import pytest def test_foo(): with pytest.raises(ZeroDivisionError) as exc_info: 1 / 0 assert exc_info.value.args Or, for pytest 8.4.0 and later: import pytest def test_foo(): with pytest.raises(ZeroDivisionError, check=lambda e: e.args): 1 / 0",
    "option": {}
  },
  "PT018": {
    "description": "[Status: stable] [Name: pytest-composite-assertion] [Message: Assertion should be broken down into multiple parts] What it does Checks for assertions that combine multiple independent conditions. Why is this bad? Composite assertion statements are harder to debug upon failure, as the failure message will not indicate which condition failed. Example def test_foo(): assert something and something_else def test_bar(): assert not (something or something_else) Use instead: def test_foo(): assert something assert something_else def test_bar(): assert not something assert not something_else",
    "option": {}
  },
  "PT019": {
    "description": "[Status: stable] [Name: pytest-fixture-param-without-value] [Message: Fixture <code>{name}</code> without value is injected as parameter, use <code>@pytest.mark.usefixtures</code> instead] What it does Checks for pytest test functions that should be decorated with @pytest.mark.usefixtures. Why is this bad? In pytest, fixture injection is used to activate fixtures in a test function. Fixtures can be injected either by passing them as parameters to the test function, or by using the @pytest.mark.usefixtures decorator. If the test function depends on the fixture being activated, but does not use it in the test body or otherwise rely on its return value, prefer the @pytest.mark.usefixtures decorator, to make the dependency explicit and avoid the confusion caused by unused arguments. Example import pytest @pytest.fixture def _patch_something(): ... def test_foo(_patch_something): ... Use instead: import pytest @pytest.fixture def _patch_something(): ... @pytest.mark.usefixtures(\"_patch_something\") def test_foo(): ...",
    "option": {}
  },
  "PT020": {
    "description": "[Status: stable] [Name: pytest-deprecated-yield-fixture] [Message: <code>@pytest.yield_fixture</code> is deprecated, use <code>@pytest.fixture</code>] What it does Checks for pytest.yield_fixture usage. Why is this bad? pytest.yield_fixture is deprecated. pytest.fixture should be used instead. Example import pytest @pytest.yield_fixture() def my_fixture(): obj = SomeClass() yield obj obj.cleanup() Use instead: import pytest @pytest.fixture() def my_fixture(): obj = SomeClass() yield obj obj.cleanup()",
    "option": {}
  },
  "PT021": {
    "description": "[Status: stable] [Name: pytest-fixture-finalizer-callback] [Message: Use <code>yield</code> instead of <code>request.addfinalizer</code>] What it does Checks for unnecessary request.addfinalizer usages in pytest fixtures. Why is this bad? pytest offers two ways to perform cleanup in fixture code. The first is sequential (via the yield statement), the second callback-based (via request.addfinalizer). The sequential approach is more readable and should be preferred, unless the fixture uses the \"factory as fixture\" pattern. Example import pytest @pytest.fixture() def my_fixture(request): resource = acquire_resource() request.addfinalizer(resource.release) return resource Use instead: import pytest @pytest.fixture() def my_fixture(): resource = acquire_resource() yield resource resource.release() # \"factory-as-fixture\" pattern @pytest.fixture() def my_factory(request): def create_resource(arg): resource = acquire_resource(arg) request.addfinalizer(resource.release) return resource return create_resource",
    "option": {}
  },
  "PT022": {
    "description": "[Status: stable] [Name: pytest-useless-yield-fixture] [Message: No teardown in fixture <code>{name}</code>, use <code>return</code> instead of <code>yield</code>] What it does Checks for unnecessary yield expressions in pytest fixtures. Why is this bad? In pytest fixtures, the yield expression should only be used for fixtures that include teardown code, to clean up the fixture after the test function has finished executing. Example import pytest @pytest.fixture() def my_fixture(): resource = acquire_resource() yield resource Use instead: import pytest @pytest.fixture() def my_fixture_with_teardown(): resource = acquire_resource() yield resource resource.release() @pytest.fixture() def my_fixture_without_teardown(): resource = acquire_resource() return resource",
    "option": {}
  },
  "PT023": {
    "description": "[Status: stable] [Name: pytest-incorrect-mark-parentheses-style] [Message: Use <code>@pytest.mark.{mark_name}{expected_parens}</code> over <code>@pytest.mark.{mark_name}{actual_parens}</code>] What it does Checks for argument-free @pytest.mark.<marker>() decorators with or without parentheses, depending on the lint.flake8-pytest-style.mark-parentheses setting. The rule defaults to removing unnecessary parentheses, to match the documentation of the official pytest projects. Why is this bad? If a @pytest.mark.<marker>() doesn't take any arguments, the parentheses are optional. Either removing those unnecessary parentheses or requiring them for all fixtures is fine, but it's best to be consistent. Example import pytest @pytest.mark.foo() def test_something(): ... Use instead: import pytest @pytest.mark.foo def test_something(): ... Fix safety This rule's fix is marked as unsafe if there's comments in the pytest.mark.<marker> decorator. import pytest @pytest.mark.foo( # comment ) def test_something(): ...",
    "option": {
      "lint.flake8-pytest-style.mark-parentheses": {
        "option_description": "Boolean flag specifying whether @pytest.mark.foo() without parameters\nshould have parentheses. If the option is set to false (the\ndefault), @pytest.mark.foo is valid and @pytest.mark.foo() is\ninvalid. If set to true, @pytest.mark.foo() is valid and\n@pytest.mark.foo is invalid.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-pytest-style]\nmark-parentheses = true\n\n\n\n[lint.flake8-pytest-style]\nmark-parentheses = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PT024": {
    "description": "[Status: stable] [Name: pytest-unnecessary-asyncio-mark-on-fixture] [Message: <code>pytest.mark.asyncio</code> is unnecessary for fixtures] What it does Checks for unnecessary @pytest.mark.asyncio decorators applied to fixtures. Why is this bad? pytest.mark.asyncio is unnecessary for fixtures. Example import pytest @pytest.mark.asyncio() @pytest.fixture() async def my_fixture(): return 0 Use instead: import pytest @pytest.fixture() async def my_fixture(): return 0",
    "option": {}
  },
  "PT025": {
    "description": "[Status: stable] [Name: pytest-erroneous-use-fixtures-on-fixture] [Message: <code>pytest.mark.usefixtures</code> has no effect on fixtures] What it does Checks for pytest.mark.usefixtures decorators applied to pytest fixtures. Why is this bad? The pytest.mark.usefixtures decorator has no effect on pytest fixtures. Example import pytest @pytest.fixture() def a(): pass @pytest.mark.usefixtures(\"a\") @pytest.fixture() def b(a): pass Use instead: import pytest @pytest.fixture() def a(): pass @pytest.fixture() def b(a): pass",
    "option": {}
  },
  "PT026": {
    "description": "[Status: stable] [Name: pytest-use-fixtures-without-parameters] [Message: Useless <code>pytest.mark.usefixtures</code> without parameters] What it does Checks for @pytest.mark.usefixtures() decorators that aren't passed any arguments. Why is this bad? A @pytest.mark.usefixtures() decorator that isn't passed any arguments is useless and should be removed. Example import pytest @pytest.mark.usefixtures() def test_something(): ... Use instead: def test_something(): ...",
    "option": {}
  },
  "PT027": {
    "description": "[Status: stable] [Name: pytest-unittest-raises-assertion] [Message: Use <code>pytest.raises</code> instead of unittest-style <code>{assertion}</code>] What it does Checks for uses of exception-related assertion methods from the unittest module. Why is this bad? To enforce the assertion style recommended by pytest, pytest.raises is preferred over the exception-related assertion methods in unittest, like assertRaises. Example import unittest class TestFoo(unittest.TestCase): def test_foo(self): with self.assertRaises(ValueError): raise ValueError(\"foo\") Use instead: import unittest import pytest class TestFoo(unittest.TestCase): def test_foo(self): with pytest.raises(ValueError): raise ValueError(\"foo\")",
    "option": {}
  },
  "PT028": {
    "description": "[Status: stable] [Name: pytest-parameter-with-default-argument] [Message: Test function parameter <code>{}</code> has default argument] What it does Checks for parameters of test functions with default arguments. Why is this bad? Such a parameter will always have the default value during the test regardless of whether a fixture with the same name is defined. Example def test_foo(a=1): ... Use instead: def test_foo(a): ... Fix safety This rule's fix is marked as unsafe, as modifying a function signature can change the behavior of the code.",
    "option": {}
  },
  "PT029": {
    "description": "[Status: preview] [Name: pytest-warns-without-warning] [Message: Set the expected warning in <code>pytest.warns()</code>] What it does Checks for pytest.warns calls without an expected warning. Why is this bad? pytest.warns expects to receive an expected warning as its first argument. If omitted, the pytest.warns call will fail at runtime. Example import pytest def test_foo(): with pytest.warns(): do_something() Use instead: import pytest def test_foo(): with pytest.warns(SomeWarning): do_something()",
    "option": {}
  },
  "PT030": {
    "description": "[Status: stable] [Name: pytest-warns-too-broad] [Message: <code>pytest.warns({warning})</code> is too broad, set the <code>match</code> parameter or use a more specific warning] What it does Checks for pytest.warns calls without a match parameter. Why is this bad? pytest.warns(Warning) will catch any Warning and may catch warnings that are unrelated to the code under test. To avoid this, pytest.warns should be called with a match parameter. The warning names that require a match parameter can be configured via the lint.flake8-pytest-style.warns-require-match-for and lint.flake8-pytest-style.warns-extend-require-match-for settings. Example import pytest def test_foo(): with pytest.warns(Warning): ... # empty string is also an error with pytest.warns(Warning, match=\"\"): ... Use instead: import pytest def test_foo(): with pytest.warns(Warning, match=\"expected message\"): ...",
    "option": {
      "lint.flake8-pytest-style.warns-require-match-for": {
        "option_description": "List of warning names that require a match= parameter in a\npytest.warns() call.\n\nSupports glob patterns. For more information on the glob syntax, refer\nto the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-pytest-style]\nwarns-require-match-for = [\"requests.RequestsWarning\"]\n\n\n\n[lint.flake8-pytest-style]\nwarns-require-match-for = [\"requests.RequestsWarning\"]",
        "option_data_type": "list[str]",
        "default_value": "[\"Warning\", \"UserWarning\", \"DeprecationWarning\"]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.flake8-pytest-style.warns-extend-require-match-for": {
        "option_description": "List of additional warning names that require a match= parameter in a\npytest.warns() call. This extends the default list of warnings that\nrequire a match= parameter.\n\nThis option is useful if you want to extend the default list of warnings\nthat require a match= parameter without having to specify the entire\nlist.\n\nNote that this option does not remove any warnings from the default\nlist.\n\nSupports glob patterns. For more information on the glob syntax, refer\nto the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-pytest-style]\nwarns-extend-require-match-for = [\"requests.RequestsWarning\"]\n\n\n\n[lint.flake8-pytest-style]\nwarns-extend-require-match-for = [\"requests.RequestsWarning\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PT031": {
    "description": "[Status: stable] [Name: pytest-warns-with-multiple-statements] [Message: <code>pytest.warns()</code> block should contain a single simple statement] What it does Checks for pytest.warns context managers with multiple statements. This rule allows pytest.warns bodies to contain for loops with empty bodies (e.g., pass or ... statements), to test iterator behavior. Why is this bad? When pytest.warns is used as a context manager and contains multiple statements, it can lead to the test passing when it should instead fail. A pytest.warns context manager should only contain a single simple statement that triggers the expected warning. Example import pytest def test_foo_warns(): with pytest.warns(Warning): setup() # False negative if setup triggers a warning but foo does not. foo() Use instead: import pytest def test_foo_warns(): setup() with pytest.warns(Warning): foo()",
    "option": {}
  },
  "Q000": {
    "description": "[Status: stable] [Name: bad-quotes-inline-string] [Message: Single quotes found but double quotes preferred] What it does Checks for inline strings that use single quotes or double quotes, depending on the value of the lint.flake8-quotes.inline-quotes option. Why is this bad? Consistency is good. Use either single or double quotes for inline strings, but be consistent. Example foo = 'bar' Assuming inline-quotes is set to double, use instead: foo = \"bar\" Formatter compatibility We recommend against using this rule alongside the formatter. The formatter enforces consistent quotes for inline strings, making the rule redundant.",
    "option": {
      "lint.flake8-quotes.inline-quotes": {
        "option_description": "Quote style to prefer for inline strings (either \"single\" or\n\"double\").\n\nWhen using the formatter, ensure that format.quote-style is set to\nthe same preferred quote style.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-quotes]\ninline-quotes = \"single\"\n\n\n\n[lint.flake8-quotes]\ninline-quotes = \"single\"",
        "option_data_type": "\"single\" | \"double\"",
        "default_value": "\"double\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "Q001": {
    "description": "[Status: stable] [Name: bad-quotes-multiline-string] [Message: Single quote multiline found but double quotes preferred] What it does Checks for multiline strings that use single quotes or double quotes, depending on the value of the lint.flake8-quotes.multiline-quotes setting. Why is this bad? Consistency is good. Use either single or double quotes for multiline strings, but be consistent. Example foo = ''' bar ''' Assuming multiline-quotes is set to double, use instead: foo = \"\"\" bar \"\"\" Formatter compatibility We recommend against using this rule alongside the formatter. The formatter enforces double quotes for multiline strings, making the rule redundant.",
    "option": {
      "lint.flake8-quotes.multiline-quotes": {
        "option_description": "Quote style to prefer for multiline strings (either \"single\" or\n\"double\").\n\nWhen using the formatter, only \"double\" is compatible, as the formatter\nenforces double quotes for multiline strings.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-quotes]\nmultiline-quotes = \"single\"\n\n\n\n[lint.flake8-quotes]\nmultiline-quotes = \"single\"",
        "option_data_type": "\"single\" | \"double\"",
        "default_value": "\"double\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "Q002": {
    "description": "[Status: stable] [Name: bad-quotes-docstring] [Message: Single quote docstring found but double quotes preferred] What it does Checks for docstrings that use single quotes or double quotes, depending on the value of the lint.flake8-quotes.docstring-quotes setting. Why is this bad? Consistency is good. Use either single or double quotes for docstring strings, but be consistent. Example ''' bar ''' Assuming docstring-quotes is set to double, use instead: \"\"\" bar \"\"\" Formatter compatibility We recommend against using this rule alongside the formatter. The formatter enforces double quotes for docstrings, making the rule redundant.",
    "option": {
      "lint.flake8-quotes.docstring-quotes": {
        "option_description": "Quote style to prefer for docstrings (either \"single\" or \"double\").\n\nWhen using the formatter, only \"double\" is compatible, as the formatter\nenforces double quotes for docstrings strings.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-quotes]\ndocstring-quotes = \"single\"\n\n\n\n[lint.flake8-quotes]\ndocstring-quotes = \"single\"",
        "option_data_type": "\"single\" | \"double\"",
        "default_value": "\"double\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "Q003": {
    "description": "[Status: stable] [Name: avoidable-escaped-quote] [Message: Change outer quotes to avoid escaping inner quotes] What it does Checks for strings that include escaped quotes, and suggests changing the quote style to avoid the need to escape them. Why is this bad? It's preferable to avoid escaped quotes in strings. By changing the outer quote style, you can avoid escaping inner quotes. Example foo = \"bar\\\"s\" Use instead: foo = 'bar\"s' Formatter compatibility We recommend against using this rule alongside the formatter. The formatter automatically removes unnecessary escapes, making the rule redundant.",
    "option": {
      "lint.flake8-quotes.inline-quotes": {
        "option_description": "Quote style to prefer for inline strings (either \"single\" or\n\"double\").\n\nWhen using the formatter, ensure that format.quote-style is set to\nthe same preferred quote style.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-quotes]\ninline-quotes = \"single\"\n\n\n\n[lint.flake8-quotes]\ninline-quotes = \"single\"",
        "option_data_type": "\"single\" | \"double\"",
        "default_value": "\"double\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "Q004": {
    "description": "[Status: stable] [Name: unnecessary-escaped-quote] [Message: Unnecessary escape on inner quote character] What it does Checks for strings that include unnecessarily escaped quotes. Why is this bad? If a string contains an escaped quote that doesn't match the quote character used for the string, it's unnecessary and can be removed. Example foo = \"bar\\'s\" Use instead: foo = \"bar's\" Formatter compatibility We recommend against using this rule alongside the formatter. The formatter automatically removes unnecessary escapes, making the rule redundant.",
    "option": {}
  },
  "RSE102": {
    "description": "[Status: stable] [Name: unnecessary-paren-on-raise-exception] [Message: Unnecessary parentheses on raised exception] What it does Checks for unnecessary parentheses on raised exceptions. Why is this bad? If an exception is raised without any arguments, parentheses are not required, as the raise statement accepts either an exception instance or an exception class (which is then implicitly instantiated). Removing the parentheses makes the code more concise. Known problems Parentheses can only be omitted if the exception is a class, as opposed to a function call. This rule isn't always capable of distinguishing between the two. For example, if you import a function module.get_exception from another module, and module.get_exception returns an exception object, this rule will incorrectly mark the parentheses in raise module.get_exception() as unnecessary. Example raise TypeError() Use instead: raise TypeError Fix Safety This rule's fix is marked as unsafe if removing the parentheses would also remove comments or if its unclear whether the expression is a class or a function call.",
    "option": {}
  },
  "RET501": {
    "description": "[Status: stable] [Name: unnecessary-return-none] [Message: Do not explicitly <code>return None</code> in function if it is the only possible return value] What it does Checks for the presence of a return None statement when None is the only possible return value. Why is this bad? Python implicitly assumes return None if an explicit return value is omitted. Therefore, explicitly returning None is redundant and should be avoided when it is the only possible return value across all code paths in a given function. Example def foo(bar): if not bar: return return None Use instead: def foo(bar): if not bar: return return Fix safety This rule's fix is marked as unsafe for cases in which comments would be dropped from the return statement.",
    "option": {
      "lint.pydocstyle.property-decorators": {
        "option_description": "A list of decorators that, when applied to a method, indicate that the\nmethod should be treated as a property (in addition to the builtin\n@property and standard-library @functools.cached_property).\n\nFor example, Ruff will expect that any method decorated by a decorator\nin this list can use a non-imperative summary line.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\nproperty-decorators = [\"gi.repository.GObject.Property\"]\n\n\n\n[lint.pydocstyle]\nproperty-decorators = [\"gi.repository.GObject.Property\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "RET502": {
    "description": "[Status: stable] [Name: implicit-return-value] [Message: Do not implicitly <code>return None</code> in function able to return non-<code>None</code> value] What it does Checks for the presence of a return statement with no explicit value, for functions that return non-None values elsewhere. Why is this bad? Including a return statement with no explicit value can cause confusion when other return statements in the function return non-None values. Python implicitly assumes return None if no other return value is present. Adding an explicit return None can make the code more readable by clarifying intent. Example def foo(bar): if not bar: return return 1 Use instead: def foo(bar): if not bar: return None return 1",
    "option": {}
  },
  "RET503": {
    "description": "[Status: stable] [Name: implicit-return] [Message: Missing explicit <code>return</code> at the end of function able to return non-<code>None</code> value] What it does Checks for missing explicit return statements at the end of functions that can return non-None values. Why is this bad? The lack of an explicit return statement at the end of a function that can return non-None values can cause confusion. Python implicitly returns None if no other return value is present. Adding an explicit return None can make the code more readable by clarifying intent. Example def foo(bar): if not bar: return 1 Use instead: def foo(bar): if not bar: return 1 return None",
    "option": {}
  },
  "RET504": {
    "description": "[Status: stable] [Name: unnecessary-assign] [Message: Unnecessary assignment to <code>{name}</code> before <code>return</code> statement] What it does Checks for variable assignments that immediately precede a return of the assigned variable. Why is this bad? The variable assignment is not necessary, as the value can be returned directly. Example def foo(): bar = 1 return bar Use instead: def foo(): return 1",
    "option": {}
  },
  "RET505": {
    "description": "[Status: stable] [Name: superfluous-else-return] [Message: Unnecessary <code>{branch}</code> after <code>return</code> statement] What it does Checks for else statements with a return statement in the preceding if block. Why is this bad? The else statement is not needed as the return statement will always break out of the enclosing function. Removing the else will reduce nesting and make the code more readable. Example def foo(bar, baz): if bar: return 1 else: return baz Use instead: def foo(bar, baz): if bar: return 1 return baz",
    "option": {}
  },
  "RET506": {
    "description": "[Status: stable] [Name: superfluous-else-raise] [Message: Unnecessary <code>{branch}</code> after <code>raise</code> statement] What it does Checks for else statements with a raise statement in the preceding if block. Why is this bad? The else statement is not needed as the raise statement will always break out of the current scope. Removing the else will reduce nesting and make the code more readable. Example def foo(bar, baz): if bar == \"Specific Error\": raise Exception(bar) else: raise Exception(baz) Use instead: def foo(bar, baz): if bar == \"Specific Error\": raise Exception(bar) raise Exception(baz)",
    "option": {}
  },
  "RET507": {
    "description": "[Status: stable] [Name: superfluous-else-continue] [Message: Unnecessary <code>{branch}</code> after <code>continue</code> statement] What it does Checks for else statements with a continue statement in the preceding if block. Why is this bad? The else statement is not needed, as the continue statement will always continue onto the next iteration of a loop. Removing the else will reduce nesting and make the code more readable. Example def foo(bar, baz): for i in bar: if i < baz: continue else: x = 0 Use instead: def foo(bar, baz): for i in bar: if i < baz: continue x = 0",
    "option": {}
  },
  "RET508": {
    "description": "[Status: stable] [Name: superfluous-else-break] [Message: Unnecessary <code>{branch}</code> after <code>break</code> statement] What it does Checks for else statements with a break statement in the preceding if block. Why is this bad? The else statement is not needed, as the break statement will always break out of the loop. Removing the else will reduce nesting and make the code more readable. Example def foo(bar, baz): for i in bar: if i > baz: break else: x = 0 Use instead: def foo(bar, baz): for i in bar: if i > baz: break x = 0",
    "option": {}
  },
  "SLF001": {
    "description": "[Status: stable] [Name: private-member-access] [Message: Private member accessed: <code>{access}</code>] What it does Checks for accesses on \"private\" class members. Why is this bad? In Python, the convention is such that class members that are prefixed with a single underscore, or prefixed but not suffixed with a double underscore, are considered private and intended for internal use. Using such \"private\" members is considered a misuse of the class, as there are no guarantees that the member will be present in future versions, that it will have the same type, or that it will have the same behavior. Instead, use the class's public interface. This rule ignores accesses on dunder methods (e.g., __init__) and sunder methods (e.g., _missing_). Example class Class: def __init__(self): self._private_member = \"...\" var = Class() print(var._private_member) Use instead: class Class: def __init__(self): self.public_member = \"...\" var = Class() print(var.public_member)",
    "option": {
      "lint.flake8-self.ignore-names": {
        "option_description": "A list of names to ignore when considering flake8-self violations.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-self]\nignore-names = [\"_new\"]\n\n\n\n[lint.flake8-self]\nignore-names = [\"_new\"]",
        "option_data_type": "list[str]",
        "default_value": "[\"_make\", \"_asdict\", \"_replace\", \"_fields\", \"_field_defaults\", \"_name_\", \"_value_\"]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "SIM101": {
    "description": "[Status: stable] [Name: duplicate-isinstance-call] [Message: Multiple <code>isinstance</code> calls for <code>{name}</code>, merge into a single call] What it does Checks for multiple isinstance calls on the same target. Why is this bad? To check if an object is an instance of any one of multiple types or classes, it is unnecessary to use multiple isinstance calls, as the second argument of the isinstance built-in function accepts a tuple of types and classes. Using a single isinstance call implements the same behavior with more concise code and clearer intent. Example if isinstance(obj, int) or isinstance(obj, float): pass Use instead: if isinstance(obj, (int, float)): pass",
    "option": {}
  },
  "SIM102": {
    "description": "[Status: stable] [Name: collapsible-if] [Message: Use a single <code>if</code> statement instead of nested <code>if</code> statements] What it does Checks for nested if statements that can be collapsed into a single if statement. Why is this bad? Nesting if statements leads to deeper indentation and makes code harder to read. Instead, combine the conditions into a single if statement with an and operator. Example if foo: if bar: ... Use instead: if foo and bar: ...",
    "option": {
      "lint.pycodestyle.max-line-length": {
        "option_description": "The maximum line length to allow for line-too-long violations. By default,\nthis is set to the value of the line-length option.\n\nUse this option when you want to detect extra-long lines that the formatter can't automatically split by setting\npycodestyle.line-length to a value larger than line-length.\n\n# The formatter wraps lines at a length of 88.\nline-length = 88\n\n[pycodestyle]\n# E501 reports lines that exceed the length of 100.\nmax-line-length = 100\n\nThe length is determined by the number of characters per line, except for lines containing East Asian characters or emojis.\nFor these lines, the unicode width of each character is added up to determine the length.\n\nSee the line-too-long rule for more information.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pycodestyle]\nmax-line-length = 100\n\n\n\n[lint.pycodestyle]\nmax-line-length = 100",
        "option_data_type": "int",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      },
      "indent-width": {
        "option_description": "The number of spaces per indentation level (tab).\n\nUsed by the formatter and when enforcing long-line violations (like E501) to determine the visual\nwidth of a tab.\n\nThis option changes the number of spaces the formatter inserts when\nusing soft-tabs (indent-style = space).\n\nPEP 8 recommends using 4 spaces per indentation level.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\nindent-width = 2\n\n\n\nindent-width = 2",
        "option_data_type": "int",
        "default_value": "4",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "SIM103": {
    "description": "[Status: stable] [Name: needless-bool] [Message: Return the condition <code>{condition}</code> directly] What it does Checks for if statements that can be replaced with bool. Why is this bad? if statements that return True for a truthy condition and False for a falsy condition can be replaced with boolean casts. Example Given: def foo(x: int) -> bool: if x > 0: return True else: return False Use instead: def foo(x: int) -> bool: return x > 0 Or, given: def foo(x: int) -> bool: if x > 0: return True return False Use instead: def foo(x: int) -> bool: return x > 0 Fix safety This fix is marked as unsafe because it may change the programs behavior if the condition does not return a proper Boolean. While the fix will try to wrap non-boolean values in a call to bool, custom implementations of comparison functions like __eq__ can avoid the bool call and still lead to altered behavior.",
    "option": {}
  },
  "SIM105": {
    "description": "[Status: stable] [Name: suppressible-exception] [Message: Use <code>contextlib.suppress({exception})</code> instead of <code>try</code>-<code>except</code>-<code>pass</code>] What it does Checks for try-except-pass blocks that can be replaced with the contextlib.suppress context manager. Why is this bad? Using contextlib.suppress is more concise and directly communicates the intent of the code: to suppress a given exception. Note that contextlib.suppress is slower than using try-except-pass directly. For performance-critical code, consider retaining the try-except-pass pattern. Example try: 1 / 0 except ZeroDivisionError: pass Use instead: import contextlib with contextlib.suppress(ZeroDivisionError): 1 / 0",
    "option": {}
  },
  "SIM107": {
    "description": "[Status: stable] [Name: return-in-try-except-finally] [Message: Don't use <code>return</code> in <code>try</code>-<code>except</code> and <code>finally</code>] What it does Checks for return statements in try-except and finally blocks. Why is this bad? The return statement in a finally block will always be executed, even if an exception is raised in the try or except block. This can lead to unexpected behavior. Example def squared(n): try: sqr = n**2 return sqr except Exception: return \"An exception occurred\" finally: return -1 # Always returns -1. Use instead: def squared(n): try: return_value = n**2 except Exception: return_value = \"An exception occurred\" finally: return_value = -1 return return_value",
    "option": {}
  },
  "SIM108": {
    "description": "[Status: stable] [Name: if-else-block-instead-of-if-exp] [Message: Use ternary operator <code>{contents}</code> instead of <code>if</code>-<code>else</code>-block] What it does Check for if-else-blocks that can be replaced with a ternary or binary operator. The lint is suppressed if the suggested replacement would exceed the maximum line length configured in pycodestyle.max-line-length. Why is this bad? if-else-blocks that assign a value to a variable in both branches can be expressed more concisely by using a ternary or binary operator. Example if foo: bar = x else: bar = y Use instead: bar = x if foo else y Or: if cond: z = cond else: z = other_cond Use instead: z = cond or other_cond Known issues This is an opinionated style rule that may not always be to everyone's taste, especially for code that makes use of complex if conditions. Ternary operators can also make it harder to measure code coverage with tools that use line profiling.",
    "option": {
      "lint.pycodestyle.max-line-length": {
        "option_description": "The maximum line length to allow for line-too-long violations. By default,\nthis is set to the value of the line-length option.\n\nUse this option when you want to detect extra-long lines that the formatter can't automatically split by setting\npycodestyle.line-length to a value larger than line-length.\n\n# The formatter wraps lines at a length of 88.\nline-length = 88\n\n[pycodestyle]\n# E501 reports lines that exceed the length of 100.\nmax-line-length = 100\n\nThe length is determined by the number of characters per line, except for lines containing East Asian characters or emojis.\nFor these lines, the unicode width of each character is added up to determine the length.\n\nSee the line-too-long rule for more information.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pycodestyle]\nmax-line-length = 100\n\n\n\n[lint.pycodestyle]\nmax-line-length = 100",
        "option_data_type": "int",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      },
      "indent-width": {
        "option_description": "The number of spaces per indentation level (tab).\n\nUsed by the formatter and when enforcing long-line violations (like E501) to determine the visual\nwidth of a tab.\n\nThis option changes the number of spaces the formatter inserts when\nusing soft-tabs (indent-style = space).\n\nPEP 8 recommends using 4 spaces per indentation level.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\nindent-width = 2\n\n\n\nindent-width = 2",
        "option_data_type": "int",
        "default_value": "4",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "SIM109": {
    "description": "[Status: stable] [Name: compare-with-tuple] [Message: Use <code>{replacement}</code> instead of multiple equality comparisons] What it does Checks for boolean expressions that contain multiple equality comparisons to the same value. Why is this bad? To check if an object is equal to any one of multiple values, it's more concise to use the in operator with a tuple of values. Example if foo == x or foo == y: ... Use instead: if foo in (x, y): ...",
    "option": {}
  },
  "SIM110": {
    "description": "[Status: stable] [Name: reimplemented-builtin] [Message: Use <code>{replacement}</code> instead of <code>for</code> loop] What it does Checks for for loops that can be replaced with a builtin function, like any or all. Why is this bad? Using a builtin function is more concise and readable. Example def foo(): for item in iterable: if predicate(item): return True return False Use instead: def foo(): return any(predicate(item) for item in iterable) Fix safety This fix is always marked as unsafe because it might remove comments.",
    "option": {
      "lint.pycodestyle.max-line-length": {
        "option_description": "The maximum line length to allow for line-too-long violations. By default,\nthis is set to the value of the line-length option.\n\nUse this option when you want to detect extra-long lines that the formatter can't automatically split by setting\npycodestyle.line-length to a value larger than line-length.\n\n# The formatter wraps lines at a length of 88.\nline-length = 88\n\n[pycodestyle]\n# E501 reports lines that exceed the length of 100.\nmax-line-length = 100\n\nThe length is determined by the number of characters per line, except for lines containing East Asian characters or emojis.\nFor these lines, the unicode width of each character is added up to determine the length.\n\nSee the line-too-long rule for more information.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pycodestyle]\nmax-line-length = 100\n\n\n\n[lint.pycodestyle]\nmax-line-length = 100",
        "option_data_type": "int",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      },
      "indent-width": {
        "option_description": "The number of spaces per indentation level (tab).\n\nUsed by the formatter and when enforcing long-line violations (like E501) to determine the visual\nwidth of a tab.\n\nThis option changes the number of spaces the formatter inserts when\nusing soft-tabs (indent-style = space).\n\nPEP 8 recommends using 4 spaces per indentation level.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\nindent-width = 2\n\n\n\nindent-width = 2",
        "option_data_type": "int",
        "default_value": "4",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "SIM112": {
    "description": "[Status: stable] [Name: uncapitalized-environment-variables] [Message: Use capitalized environment variable <code>{expected}</code> instead of <code>{actual}</code>] What it does Check for environment variables that are not capitalized. Why is this bad? By convention, environment variables should be capitalized. On Windows, environment variables are case-insensitive and are converted to uppercase, so using lowercase environment variables can lead to subtle bugs. Example import os os.environ[\"foo\"] Use instead: import os os.environ[\"FOO\"] Fix safety This fix is always marked as unsafe because automatically capitalizing environment variable names can change program behavior in environments where the variable names are case-sensitive, such as most Unix-like systems.",
    "option": {}
  },
  "SIM113": {
    "description": "[Status: stable] [Name: enumerate-for-loop] [Message: Use <code>enumerate()</code> for index variable <code>{index}</code> in <code>for</code> loop] What it does Checks for for loops with explicit loop-index variables that can be replaced with enumerate(). In preview, this rule checks for index variables initialized with any integer rather than only a literal zero. Why is this bad? When iterating over a sequence, it's often desirable to keep track of the index of each element alongside the element itself. Prefer the enumerate builtin over manually incrementing a counter variable within the loop, as enumerate is more concise and idiomatic. Example fruits = [\"apple\", \"banana\", \"cherry\"] i = 0 for fruit in fruits: print(f\"{i + 1}. {fruit}\") i += 1 Use instead: fruits = [\"apple\", \"banana\", \"cherry\"] for i, fruit in enumerate(fruits): print(f\"{i + 1}. {fruit}\")",
    "option": {}
  },
  "SIM114": {
    "description": "[Status: stable] [Name: if-with-same-arms] [Message: Combine <code>if</code> branches using logical <code>or</code> operator] What it does Checks for if branches with identical arm bodies. Why is this bad? If multiple arms of an if statement have the same body, using or better signals the intent of the statement. Example if x == 1: print(\"Hello\") elif x == 2: print(\"Hello\") Use instead: if x == 1 or x == 2: print(\"Hello\")",
    "option": {}
  },
  "SIM115": {
    "description": "[Status: stable] [Name: open-file-with-context-handler] [Message: Use a context manager for opening files] What it does Checks for cases where files are opened (e.g., using the builtin open() function) without using a context manager. Why is this bad? If a file is opened without a context manager, it is not guaranteed that the file will be closed (e.g., if an exception is raised), which can cause resource leaks. The rule detects a wide array of IO calls where context managers could be used, such as open, pathlib.Path(...).open(), tempfile.TemporaryFile() ortarfile.TarFile(...).gzopen(). Example file = open(\"foo.txt\") ... file.close() Use instead: with open(\"foo.txt\") as file: ...",
    "option": {}
  },
  "SIM116": {
    "description": "[Status: stable] [Name: if-else-block-instead-of-dict-lookup] [Message: Use a dictionary instead of consecutive <code>if</code> statements] What it does Checks for three or more consecutive if-statements with direct returns Why is this bad? These can be simplified by using a dictionary Example def find_phrase(x): if x == 1: return \"Hello\" elif x == 2: return \"Goodbye\" elif x == 3: return \"Good morning\" else: return \"Goodnight\" Use instead: def find_phrase(x): phrases = {1: \"Hello\", 2: \"Goodye\", 3: \"Good morning\"} return phrases.get(x, \"Goodnight\")",
    "option": {}
  },
  "SIM117": {
    "description": "[Status: stable] [Name: multiple-with-statements] [Message: Use a single <code>with</code> statement with multiple contexts instead of nested <code>with</code> statements] What it does Checks for the unnecessary nesting of multiple consecutive context managers. Why is this bad? In Python 3, a single with block can include multiple context managers. Combining multiple context managers into a single with statement will minimize the indentation depth of the code, making it more readable. The following context managers are exempt when used as standalone statements: anyio.{CancelScope, fail_after, move_on_after} asyncio.{timeout, timeout_at} trio.{fail_after, fail_at, move_on_after, move_on_at} Example with A() as a: with B() as b: pass Use instead: with A() as a, B() as b: pass",
    "option": {
      "lint.pycodestyle.max-line-length": {
        "option_description": "The maximum line length to allow for line-too-long violations. By default,\nthis is set to the value of the line-length option.\n\nUse this option when you want to detect extra-long lines that the formatter can't automatically split by setting\npycodestyle.line-length to a value larger than line-length.\n\n# The formatter wraps lines at a length of 88.\nline-length = 88\n\n[pycodestyle]\n# E501 reports lines that exceed the length of 100.\nmax-line-length = 100\n\nThe length is determined by the number of characters per line, except for lines containing East Asian characters or emojis.\nFor these lines, the unicode width of each character is added up to determine the length.\n\nSee the line-too-long rule for more information.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pycodestyle]\nmax-line-length = 100\n\n\n\n[lint.pycodestyle]\nmax-line-length = 100",
        "option_data_type": "int",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      },
      "indent-width": {
        "option_description": "The number of spaces per indentation level (tab).\n\nUsed by the formatter and when enforcing long-line violations (like E501) to determine the visual\nwidth of a tab.\n\nThis option changes the number of spaces the formatter inserts when\nusing soft-tabs (indent-style = space).\n\nPEP 8 recommends using 4 spaces per indentation level.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\nindent-width = 2\n\n\n\nindent-width = 2",
        "option_data_type": "int",
        "default_value": "4",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "SIM118": {
    "description": "[Status: stable] [Name: in-dict-keys] [Message: Use <code>key {operator} dict</code> instead of <code>key {operator} dict.keys()</code>] What it does Checks for key-existence checks against dict.keys() calls. Why is this bad? When checking for the existence of a key in a given dictionary, using key in dict is more readable and efficient than key in dict.keys(), while having the same semantics. Example key in foo.keys() Use instead: key in foo Fix safety Given key in obj.keys(), obj could be a dictionary, or it could be another type that defines a .keys() method. In the latter case, removing the .keys() attribute could lead to a runtime error. The fix is marked as safe when the type of obj is known to be a dictionary; otherwise, it is marked as unsafe.",
    "option": {}
  },
  "SIM201": {
    "description": "[Status: stable] [Name: negate-equal-op] [Message: Use <code>{left} != {right}</code> instead of <code>not {left} == {right}</code>] What it does Checks for negated == operators. Why is this bad? Negated == operators are less readable than != operators. When testing for non-equality, it is more common to use != than ==. Example not a == b Use instead: a != b Fix safety The fix is marked as unsafe, as it might change the behaviour if a and/or b overrides __eq__/__ne__ in such a manner that they don't return booleans.",
    "option": {}
  },
  "SIM202": {
    "description": "[Status: stable] [Name: negate-not-equal-op] [Message: Use <code>{left} == {right}</code> instead of <code>not {left} != {right}</code>] What it does Checks for negated != operators. Why is this bad? Negated != operators are less readable than == operators, as they avoid a double negation. Example not a != b Use instead: a == b Fix safety The fix is marked as unsafe, as it might change the behaviour if a and/or b overrides __ne__/__eq__ in such a manner that they don't return booleans.",
    "option": {}
  },
  "SIM208": {
    "description": "[Status: stable] [Name: double-negation] [Message: Use <code>{expr}</code> instead of <code>not (not {expr})</code>] What it does Checks for double negations (i.e., multiple not operators). Why is this bad? A double negation is redundant and less readable than omitting the not operators entirely. Example not (not a) Use instead: a",
    "option": {}
  },
  "SIM210": {
    "description": "[Status: stable] [Name: if-expr-with-true-false] [Message: Remove unnecessary <code>True if ... else False</code>] What it does Checks for if expressions that can be replaced with bool() calls. Why is this bad? if expressions that evaluate to True for a truthy condition an False for a falsey condition can be replaced with bool() calls, which are more concise and readable. Example True if a else False Use instead: bool(a) Fix safety This fix is marked as unsafe because it may change the programs behavior if the condition does not return a proper Boolean. While the fix will try to wrap non-boolean values in a call to bool, custom implementations of comparison functions like __eq__ can avoid the bool call and still lead to altered behavior. Moreover, the fix may remove comments.",
    "option": {}
  },
  "SIM211": {
    "description": "[Status: stable] [Name: if-expr-with-false-true] [Message: Use <code>not ...</code> instead of <code>False if ... else True</code>] What it does Checks for if expressions that can be replaced by negating a given condition. Why is this bad? if expressions that evaluate to False for a truthy condition and True for a falsey condition can be replaced with not operators, which are more concise and readable. Example False if a else True Use instead: not a",
    "option": {}
  },
  "SIM212": {
    "description": "[Status: stable] [Name: if-expr-with-twisted-arms] [Message: Use <code>{expr_else} if {expr_else} else {expr_body}</code> instead of <code>{expr_body} if not {expr_else} else {expr_else}</code>] What it does Checks for if expressions that check against a negated condition. Why is this bad? if expressions that check against a negated condition are more difficult to read than if expressions that check against the condition directly. Example b if not a else a Use instead: a if a else b",
    "option": {}
  },
  "SIM220": {
    "description": "[Status: stable] [Name: expr-and-not-expr] [Message: Use <code>False</code> instead of <code>{name} and not {name}</code>] What it does Checks for and expressions that include both an expression and its negation. Why is this bad? An and expression that includes both an expression and its negation will always evaluate to False. Example x and not x",
    "option": {}
  },
  "SIM221": {
    "description": "[Status: stable] [Name: expr-or-not-expr] [Message: Use <code>True</code> instead of <code>{name} or not {name}</code>] What it does Checks for or expressions that include both an expression and its negation. Why is this bad? An or expression that includes both an expression and its negation will always evaluate to True. Example x or not x",
    "option": {}
  },
  "SIM222": {
    "description": "[Status: stable] [Name: expr-or-true] [Message: Use <code>{expr}</code> instead of <code>{replaced}</code>] What it does Checks for or expressions that contain truthy values. Why is this bad? If the expression is used as a condition, it can be replaced in-full with True. In other cases, the expression can be short-circuited to the first truthy value. By using True (or the first truthy value), the code is more concise and easier to understand, since it no longer contains redundant conditions. Example if x or [1] or y: pass a = x or [1] or y Use instead: if True: pass a = x or [1]",
    "option": {}
  },
  "SIM223": {
    "description": "[Status: stable] [Name: expr-and-false] [Message: Use <code>{expr}</code> instead of <code>{replaced}</code>] What it does Checks for and expressions that contain falsey values. Why is this bad? If the expression is used as a condition, it can be replaced in-full with False. In other cases, the expression can be short-circuited to the first falsey value. By using False (or the first falsey value), the code is more concise and easier to understand, since it no longer contains redundant conditions. Example if x and [] and y: pass a = x and [] and y Use instead: if False: pass a = x and []",
    "option": {}
  },
  "SIM300": {
    "description": "[Status: stable] [Name: yoda-conditions] [Message: Yoda condition detected] What it does Checks for conditions that position a constant on the left-hand side of the comparison operator, rather than the right-hand side. Why is this bad? These conditions (sometimes referred to as \"Yoda conditions\") are less readable than conditions that place the variable on the left-hand side of the comparison operator. In some languages, Yoda conditions are used to prevent accidental assignment in conditions (i.e., accidental uses of the = operator, instead of the == operator). However, Python does not allow assignments in conditions unless using the := operator, so Yoda conditions provide no benefit in this regard. Example if \"Foo\" == foo: ... Use instead: if foo == \"Foo\": ...",
    "option": {}
  },
  "SIM401": {
    "description": "[Status: stable] [Name: if-else-block-instead-of-dict-get] [Message: Use <code>{contents}</code> instead of an <code>if</code> block] What it does Checks for if statements that can be replaced with dict.get calls. Why is this bad? dict.get() calls can be used to replace if statements that assign a value to a variable in both branches, falling back to a default value if the key is not found. When possible, using dict.get is more concise and more idiomatic. Under preview mode, this rule will also suggest replacing if-else expressions with dict.get calls. Example foo = {} if \"bar\" in foo: value = foo[\"bar\"] else: value = 0 Use instead: foo = {} value = foo.get(\"bar\", 0) If preview mode is enabled: value = foo[\"bar\"] if \"bar\" in foo else 0 Use instead: value = foo.get(\"bar\", 0)",
    "option": {
      "lint.pycodestyle.max-line-length": {
        "option_description": "The maximum line length to allow for line-too-long violations. By default,\nthis is set to the value of the line-length option.\n\nUse this option when you want to detect extra-long lines that the formatter can't automatically split by setting\npycodestyle.line-length to a value larger than line-length.\n\n# The formatter wraps lines at a length of 88.\nline-length = 88\n\n[pycodestyle]\n# E501 reports lines that exceed the length of 100.\nmax-line-length = 100\n\nThe length is determined by the number of characters per line, except for lines containing East Asian characters or emojis.\nFor these lines, the unicode width of each character is added up to determine the length.\n\nSee the line-too-long rule for more information.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pycodestyle]\nmax-line-length = 100\n\n\n\n[lint.pycodestyle]\nmax-line-length = 100",
        "option_data_type": "int",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      },
      "indent-width": {
        "option_description": "The number of spaces per indentation level (tab).\n\nUsed by the formatter and when enforcing long-line violations (like E501) to determine the visual\nwidth of a tab.\n\nThis option changes the number of spaces the formatter inserts when\nusing soft-tabs (indent-style = space).\n\nPEP 8 recommends using 4 spaces per indentation level.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\nindent-width = 2\n\n\n\nindent-width = 2",
        "option_data_type": "int",
        "default_value": "4",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "SIM905": {
    "description": "[Status: stable] [Name: split-static-string] [Message: Consider using a list literal instead of <code>str.{}</code>] What it does Checks for static str.split calls that can be replaced with list literals. Why is this bad? List literals are more readable and do not require the overhead of calling str.split. Example \"a,b,c,d\".split(\",\") Use instead: [\"a\", \"b\", \"c\", \"d\"] Fix safety This rule's fix is marked as unsafe for implicit string concatenations with comments interleaved between segments, as comments may be removed. For example, the fix would be marked as unsafe in the following case: ( \"a\" # comment \",\" # comment \"b\" # comment ).split(\",\") as this is converted to [\"a\", \"b\"] without any of the comments.",
    "option": {}
  },
  "SIM910": {
    "description": "[Status: stable] [Name: dict-get-with-none-default] [Message: Use <code>{expected}</code> instead of <code>{actual}</code>] What it does Checks for dict.get() calls that pass None as the default value. Why is this bad? None is the default value for dict.get(), so it is redundant to pass it explicitly. Example ages = {\"Tom\": 23, \"Maria\": 23, \"Dog\": 11} age = ages.get(\"Cat\", None) Use instead: ages = {\"Tom\": 23, \"Maria\": 23, \"Dog\": 11} age = ages.get(\"Cat\") Fix safety This rule's fix is marked as safe, unless the expression contains comments.",
    "option": {}
  },
  "SIM911": {
    "description": "[Status: stable] [Name: zip-dict-keys-and-values] [Message: Use <code>{expected}</code> instead of <code>{actual}</code>] What it does Checks for use of zip() to iterate over keys and values of a dictionary at once. Why is this bad? The dict type provides an .items() method which is faster and more readable. Example flag_stars = {\"USA\": 50, \"Slovenia\": 3, \"Panama\": 2, \"Australia\": 6} for country, stars in zip(flag_stars.keys(), flag_stars.values()): print(f\"{country}'s flag has {stars} stars.\") Use instead: flag_stars = {\"USA\": 50, \"Slovenia\": 3, \"Panama\": 2, \"Australia\": 6} for country, stars in flag_stars.items(): print(f\"{country}'s flag has {stars} stars.\") Fix safety This rule's fix is marked as safe, unless the expression contains comments.",
    "option": {}
  },
  "SLOT000": {
    "description": "[Status: stable] [Name: no-slots-in-str-subclass] [Message: Subclasses of <code>str</code> should define <code>__slots__</code>] What it does Checks for subclasses of str that lack a __slots__ definition. Why is this bad? In Python, the __slots__ attribute allows you to explicitly define the attributes (instance variables) that a class can have. By default, Python uses a dictionary to store an object's attributes, which incurs some memory overhead. However, when __slots__ is defined, Python uses a more compact internal structure to store the object's attributes, resulting in memory savings. Subclasses of str inherit all the attributes and methods of the built-in str class. Since strings are typically immutable, they don't require additional attributes beyond what the str class provides. Defining __slots__ for subclasses of str prevents the creation of a dictionary for each instance, reducing memory consumption. Example class Foo(str): pass Use instead: class Foo(str): __slots__ = ()",
    "option": {}
  },
  "SLOT001": {
    "description": "[Status: stable] [Name: no-slots-in-tuple-subclass] [Message: Subclasses of <code>tuple</code> should define <code>__slots__</code>] What it does Checks for subclasses of tuple that lack a __slots__ definition. Why is this bad? In Python, the __slots__ attribute allows you to explicitly define the attributes (instance variables) that a class can have. By default, Python uses a dictionary to store an object's attributes, which incurs some memory overhead. However, when __slots__ is defined, Python uses a more compact internal structure to store the object's attributes, resulting in memory savings. Subclasses of tuple inherit all the attributes and methods of the built-in tuple class. Since tuples are typically immutable, they don't require additional attributes beyond what the tuple class provides. Defining __slots__ for subclasses of tuple prevents the creation of a dictionary for each instance, reducing memory consumption. Example class Foo(tuple): pass Use instead: class Foo(tuple): __slots__ = ()",
    "option": {}
  },
  "SLOT002": {
    "description": "[Status: stable] [Name: no-slots-in-namedtuple-subclass] [Message: Subclasses of {namedtuple_kind} should define <code>__slots__</code>] What it does Checks for subclasses of collections.namedtuple or typing.NamedTuple that lack a __slots__ definition. Why is this bad? In Python, the __slots__ attribute allows you to explicitly define the attributes (instance variables) that a class can have. By default, Python uses a dictionary to store an object's attributes, which incurs some memory overhead. However, when __slots__ is defined, Python uses a more compact internal structure to store the object's attributes, resulting in memory savings. Subclasses of namedtuple inherit all the attributes and methods of the built-in namedtuple class. Since tuples are typically immutable, they don't require additional attributes beyond what the namedtuple class provides. Defining __slots__ for subclasses of namedtuple prevents the creation of a dictionary for each instance, reducing memory consumption. Example from collections import namedtuple class Foo(namedtuple(\"foo\", [\"str\", \"int\"])): pass Use instead: from collections import namedtuple class Foo(namedtuple(\"foo\", [\"str\", \"int\"])): __slots__ = ()",
    "option": {}
  },
  "TID251": {
    "description": "[Status: stable] [Name: banned-api] [Message: <code>{name}</code> is banned: {message}] What it does Checks for banned imports. Why is this bad? Projects may want to ensure that specific modules or module members are not imported or accessed. Security or other company policies may be a reason to impose restrictions on importing external Python libraries. In some cases, projects may adopt conventions around the use of certain modules or module members that are not enforceable by the language itself. This rule enforces certain import conventions project-wide automatically.",
    "option": {
      "lint.flake8-tidy-imports.banned-api": {
        "option_description": "Specific modules or module members that may not be imported or accessed.\nNote that this rule is only meant to flag accidental uses,\nand can be circumvented via eval or importlib.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-tidy-imports.banned-api]\n\"cgi\".msg = \"The cgi module is deprecated, see https://peps.python.org/pep-0594/#cgi.\"\n\"typing.TypedDict\".msg = \"Use typing_extensions.TypedDict instead.\"\n\n\n\n[lint.flake8-tidy-imports.banned-api]\n\"cgi\".msg = \"The cgi module is deprecated, see https://peps.python.org/pep-0594/#cgi.\"\n\"typing.TypedDict\".msg = \"Use typing_extensions.TypedDict instead.\"",
        "option_data_type": "dict[str, { \"msg\": str }]",
        "default_value": "{}",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "TID252": {
    "description": "[Status: stable] [Name: relative-imports] [Message: Prefer absolute imports over relative imports from parent modules] What it does Checks for relative imports. Why is this bad? Absolute imports, or relative imports from siblings, are recommended by PEP 8: Absolute imports are recommended, as they are usually more readable and tend to be better behaved... import mypkg.sibling from mypkg import sibling from mypkg.sibling import example However, explicit relative imports are an acceptable alternative to absolute imports, especially when dealing with complex package layouts where using absolute imports would be unnecessarily verbose: from . import sibling from .sibling import example Example from .. import foo Use instead: from mypkg import foo",
    "option": {
      "lint.flake8-tidy-imports.ban-relative-imports": {
        "option_description": "Whether to ban all relative imports (\"all\"), or only those imports\nthat extend into the parent module or beyond (\"parents\").\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-tidy-imports]\n# Disallow all relative imports.\nban-relative-imports = \"all\"\n\n\n\n[lint.flake8-tidy-imports]\n# Disallow all relative imports.\nban-relative-imports = \"all\"",
        "option_data_type": "\"parents\" | \"all\"",
        "default_value": "\"parents\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "TID253": {
    "description": "[Status: stable] [Name: banned-module-level-imports] [Message: <code>{name}</code> is banned at the module level] What it does Checks for module-level imports that should instead be imported lazily (e.g., within a function definition, or an if TYPE_CHECKING: block, or some other nested context). Why is this bad? Some modules are expensive to import. For example, importing torch or tensorflow can introduce a noticeable delay in the startup time of a Python program. In such cases, you may want to enforce that the module is imported lazily as needed, rather than at the top of the file. This could involve inlining the import into the function that uses it, rather than importing it unconditionally, to ensure that the module is only imported when necessary. Example import tensorflow as tf def show_version(): print(tf.__version__) Use instead: def show_version(): import tensorflow as tf print(tf.__version__)",
    "option": {
      "lint.flake8-tidy-imports.banned-module-level-imports": {
        "option_description": "List of specific modules that may not be imported at module level, and should instead be\nimported lazily (e.g., within a function definition, or an if TYPE_CHECKING:\nblock, or some other nested context). This also affects the rule import-outside-top-level\nif banned-module-level-imports is enabled.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-tidy-imports]\n# Ban certain modules from being imported at module level, instead requiring\n# that they're imported lazily (e.g., within a function definition).\nbanned-module-level-imports = [\"torch\", \"tensorflow\"]\n\n\n\n[lint.flake8-tidy-imports]\n# Ban certain modules from being imported at module level, instead requiring\n# that they're imported lazily (e.g., within a function definition).\nbanned-module-level-imports = [\"torch\", \"tensorflow\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "TD001": {
    "description": "[Status: stable] [Name: invalid-todo-tag] [Message: Invalid TODO tag: <code>{tag}</code>] What it does Checks that a TODO comment is labelled with \"TODO\". Why is this bad? Ambiguous tags reduce code visibility and can lead to dangling TODOs. For example, if a comment is tagged with \"FIXME\" rather than \"TODO\", it may be overlooked by future readers. Note that this rule will only flag \"FIXME\" and \"XXX\" tags as incorrect. Example # FIXME(ruff): this should get fixed! Use instead: # TODO(ruff): this is now fixed!",
    "option": {}
  },
  "TD002": {
    "description": "[Status: stable] [Name: missing-todo-author] [Message: Missing author in TODO; try: <code># TODO(&lt;author_name&gt;): ...</code> or <code># TODO @&lt;author_name&gt;: ...</code>] What it does Checks that a TODO comment includes an author. Why is this bad? Including an author on a TODO provides future readers with context around the issue. While the TODO author is not always considered responsible for fixing the issue, they are typically the individual with the most context. Example # TODO: should assign an author here Use instead # TODO(charlie): now an author is assigned",
    "option": {}
  },
  "TD003": {
    "description": "[Status: stable] [Name: missing-todo-link] [Message: Missing issue link for this TODO] What it does Checks that a TODO comment is associated with a link to a relevant issue or ticket. Why is this bad? Including an issue link near a TODO makes it easier for resolvers to get context around the issue. Example # TODO: this link has no issue Use one of these instead: # TODO(charlie): this comment has an issue link # https://github.com/astral-sh/ruff/issues/3870 # TODO(charlie): this comment has a 3-digit issue code # 003 # TODO(charlie): https://github.com/astral-sh/ruff/issues/3870 # this comment has an issue link # TODO(charlie): #003 this comment has a 3-digit issue code # with leading character `#` # TODO(charlie): this comment has an issue code (matches the regex `[A-Z]+\\-?\\d+`) # SIXCHR-003",
    "option": {}
  },
  "TD004": {
    "description": "[Status: stable] [Name: missing-todo-colon] [Message: Missing colon in TODO] What it does Checks that a \"TODO\" tag is followed by a colon. Why is this bad? \"TODO\" tags are typically followed by a parenthesized author name, a colon, a space, and a description of the issue, in that order. Deviating from this pattern can lead to inconsistent and non-idiomatic comments. Example # TODO(charlie) fix this colon Used instead: # TODO(charlie): colon fixed",
    "option": {}
  },
  "TD005": {
    "description": "[Status: stable] [Name: missing-todo-description] [Message: Missing issue description after <code>TODO</code>] What it does Checks that a \"TODO\" tag contains a description of the issue following the tag itself. Why is this bad? TODO comments should include a description of the issue to provide context for future readers. Example # TODO(charlie) Use instead: # TODO(charlie): fix some issue",
    "option": {}
  },
  "TD006": {
    "description": "[Status: stable] [Name: invalid-todo-capitalization] [Message: Invalid TODO capitalization: <code>{tag}</code> should be <code>TODO</code>] What it does Checks that a \"TODO\" tag is properly capitalized (i.e., that the tag is uppercase). Why is this bad? Capitalizing the \"TODO\" in a TODO comment is a convention that makes it easier for future readers to identify TODOs. Example # todo(charlie): capitalize this Use instead: # TODO(charlie): this is capitalized",
    "option": {}
  },
  "TD007": {
    "description": "[Status: stable] [Name: missing-space-after-todo-colon] [Message: Missing space after colon in TODO] What it does Checks that the colon after a \"TODO\" tag is followed by a space. Why is this bad? \"TODO\" tags are typically followed by a parenthesized author name, a colon, a space, and a description of the issue, in that order. Deviating from this pattern can lead to inconsistent and non-idiomatic comments. Example # TODO(charlie):fix this Use instead: # TODO(charlie): fix this",
    "option": {}
  },
  "TC001": {
    "description": "[Status: stable] [Name: typing-only-first-party-import] [Message: Move application import <code>{}</code> into a type-checking block] What it does Checks for first-party imports that are only used for type annotations, but aren't defined in a type-checking block. Why is this bad? Unused imports add a performance overhead at runtime, and risk creating import cycles. If an import is only used in typing-only contexts, it can instead be imported conditionally under an if TYPE_CHECKING: block to minimize runtime overhead. If lint.flake8-type-checking.quote-annotations is set to true, annotations will be wrapped in quotes if doing so would enable the corresponding import to be moved into an if TYPE_CHECKING: block. If a class requires that type annotations be available at runtime (as is the case for Pydantic, SQLAlchemy, and other libraries), consider using the lint.flake8-type-checking.runtime-evaluated-base-classes and lint.flake8-type-checking.runtime-evaluated-decorators settings to mark them as such. If lint.future-annotations is set to true, from __future__ import annotations will be added if doing so would enable an import to be moved into an if TYPE_CHECKING: block. This takes precedence over the lint.flake8-type-checking.quote-annotations setting described above if both settings are enabled. Example from __future__ import annotations from . import local_module def func(sized: local_module.Container) -> int: return len(sized) Use instead: from __future__ import annotations from typing import TYPE_CHECKING if TYPE_CHECKING: from . import local_module def func(sized: local_module.Container) -> int: return len(sized)",
    "option": {
      "lint.flake8-type-checking.quote-annotations": {
        "option_description": "Whether to add quotes around type annotations, if doing so would allow\nthe corresponding import to be moved into a type-checking block.\n\nFor example, in the following, Python requires that Sequence be\navailable at runtime, despite the fact that it's only used in a type\nannotation:\n\nfrom collections.abc import Sequence\n\n\ndef func(value: Sequence[int]) -> None:\n    ...\n\nIn other words, moving from collections.abc import Sequence into an\nif TYPE_CHECKING: block above would cause a runtime error, as the\ntype would no longer be available at runtime.\n\nBy default, Ruff will respect such runtime semantics and avoid moving\nthe import to prevent such runtime errors.\n\nSetting quote-annotations to true will instruct Ruff to add quotes\naround the annotation (e.g., \"Sequence[int]\"), which in turn enables\nRuff to move the import into an if TYPE_CHECKING: block, like so:\n\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from collections.abc import Sequence\n\n\ndef func(value: \"Sequence[int]\") -> None:\n    ...\n\nNote that this setting has no effect when from __future__ import annotations\nis present, as __future__ annotations are always treated equivalently\nto quoted annotations. Similarly, this setting has no effect on Python\nversions after 3.14 because these annotations are also deferred.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-type-checking]\n# Add quotes around type annotations, if doing so would allow\n# an import to be moved into a type-checking block.\nquote-annotations = true\n\n\n\n[lint.flake8-type-checking]\n# Add quotes around type annotations, if doing so would allow\n# an import to be moved into a type-checking block.\nquote-annotations = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.flake8-type-checking.runtime-evaluated-base-classes": {
        "option_description": "Exempt classes that list any of the enumerated classes as a base class\nfrom needing to be moved into type-checking blocks.\n\nCommon examples include Pydantic's pydantic.BaseModel and SQLAlchemy's\nsqlalchemy.orm.DeclarativeBase, but can also support user-defined\nclasses that inherit from those base classes. For example, if you define\na common DeclarativeBase subclass that's used throughout your project\n(e.g., class Base(DeclarativeBase) ... in base.py), you can add it to\nthis list (runtime-evaluated-base-classes = [\"base.Base\"]) to exempt\nmodels from being moved into type-checking blocks.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-type-checking]\nruntime-evaluated-base-classes = [\"pydantic.BaseModel\", \"sqlalchemy.orm.DeclarativeBase\"]\n\n\n\n[lint.flake8-type-checking]\nruntime-evaluated-base-classes = [\"pydantic.BaseModel\", \"sqlalchemy.orm.DeclarativeBase\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.flake8-type-checking.runtime-evaluated-decorators": {
        "option_description": "Exempt classes and functions decorated with any of the enumerated\ndecorators from being moved into type-checking blocks.\n\nCommon examples include Pydantic's @pydantic.validate_call decorator\n(for functions) and attrs' @attrs.define decorator (for classes).\n\nThis also supports framework decorators like FastAPI's fastapi.FastAPI.get\nwhich will work across assignments in the same module.\n\nFor example:\nimport fastapi\n\napp = FastAPI(\"app\")\n\n@app.get(\"/home\")\ndef home() -> str: ...\n\nHere app.get will correctly be identified as fastapi.FastAPI.get.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-type-checking]\nruntime-evaluated-decorators = [\"pydantic.validate_call\", \"attrs.define\"]\n\n\n\n[lint.flake8-type-checking]\nruntime-evaluated-decorators = [\"pydantic.validate_call\", \"attrs.define\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.flake8-type-checking.strict": {
        "option_description": "Enforce TC001, TC002, and TC003 rules even when valid runtime imports\nare present for the same module.\n\nSee flake8-type-checking's strict option.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-type-checking]\nstrict = true\n\n\n\n[lint.flake8-type-checking]\nstrict = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.typing-modules": {
        "option_description": "A list of modules whose exports should be treated equivalently to\nmembers of the typing module.\n\nThis is useful for ensuring proper type annotation inference for\nprojects that re-export typing and typing_extensions members\nfrom a compatibility module. If omitted, any members imported from\nmodules apart from typing and typing_extensions will be treated\nas ordinary Python objects.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\ntyping-modules = [\"airflow.typing_compat\"]\n\n\n\n[lint]\ntyping-modules = [\"airflow.typing_compat\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.future-annotations": {
        "option_description": "Whether to allow rules to add from __future__ import annotations in cases where this would\nsimplify a fix or enable a new diagnostic.\n\nFor example, TC001, TC002, and TC003 can move more imports into TYPE_CHECKING blocks\nif __future__ annotations are enabled.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\n# Enable `from __future__ import annotations` imports\nfuture-annotations = true\n\n\n\n[lint]\n# Enable `from __future__ import annotations` imports\nfuture-annotations = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "TC002": {
    "description": "[Status: stable] [Name: typing-only-third-party-import] [Message: Move third-party import <code>{}</code> into a type-checking block] What it does Checks for third-party imports that are only used for type annotations, but aren't defined in a type-checking block. Why is this bad? Unused imports add a performance overhead at runtime, and risk creating import cycles. If an import is only used in typing-only contexts, it can instead be imported conditionally under an if TYPE_CHECKING: block to minimize runtime overhead. If lint.flake8-type-checking.quote-annotations is set to true, annotations will be wrapped in quotes if doing so would enable the corresponding import to be moved into an if TYPE_CHECKING: block. If a class requires that type annotations be available at runtime (as is the case for Pydantic, SQLAlchemy, and other libraries), consider using the lint.flake8-type-checking.runtime-evaluated-base-classes and lint.flake8-type-checking.runtime-evaluated-decorators settings to mark them as such. If lint.future-annotations is set to true, from __future__ import annotations will be added if doing so would enable an import to be moved into an if TYPE_CHECKING: block. This takes precedence over the lint.flake8-type-checking.quote-annotations setting described above if both settings are enabled. Example from __future__ import annotations import pandas as pd def func(df: pd.DataFrame) -> int: return len(df) Use instead: from __future__ import annotations from typing import TYPE_CHECKING if TYPE_CHECKING: import pandas as pd def func(df: pd.DataFrame) -> int: return len(df)",
    "option": {
      "lint.flake8-type-checking.quote-annotations": {
        "option_description": "Whether to add quotes around type annotations, if doing so would allow\nthe corresponding import to be moved into a type-checking block.\n\nFor example, in the following, Python requires that Sequence be\navailable at runtime, despite the fact that it's only used in a type\nannotation:\n\nfrom collections.abc import Sequence\n\n\ndef func(value: Sequence[int]) -> None:\n    ...\n\nIn other words, moving from collections.abc import Sequence into an\nif TYPE_CHECKING: block above would cause a runtime error, as the\ntype would no longer be available at runtime.\n\nBy default, Ruff will respect such runtime semantics and avoid moving\nthe import to prevent such runtime errors.\n\nSetting quote-annotations to true will instruct Ruff to add quotes\naround the annotation (e.g., \"Sequence[int]\"), which in turn enables\nRuff to move the import into an if TYPE_CHECKING: block, like so:\n\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from collections.abc import Sequence\n\n\ndef func(value: \"Sequence[int]\") -> None:\n    ...\n\nNote that this setting has no effect when from __future__ import annotations\nis present, as __future__ annotations are always treated equivalently\nto quoted annotations. Similarly, this setting has no effect on Python\nversions after 3.14 because these annotations are also deferred.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-type-checking]\n# Add quotes around type annotations, if doing so would allow\n# an import to be moved into a type-checking block.\nquote-annotations = true\n\n\n\n[lint.flake8-type-checking]\n# Add quotes around type annotations, if doing so would allow\n# an import to be moved into a type-checking block.\nquote-annotations = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.flake8-type-checking.runtime-evaluated-base-classes": {
        "option_description": "Exempt classes that list any of the enumerated classes as a base class\nfrom needing to be moved into type-checking blocks.\n\nCommon examples include Pydantic's pydantic.BaseModel and SQLAlchemy's\nsqlalchemy.orm.DeclarativeBase, but can also support user-defined\nclasses that inherit from those base classes. For example, if you define\na common DeclarativeBase subclass that's used throughout your project\n(e.g., class Base(DeclarativeBase) ... in base.py), you can add it to\nthis list (runtime-evaluated-base-classes = [\"base.Base\"]) to exempt\nmodels from being moved into type-checking blocks.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-type-checking]\nruntime-evaluated-base-classes = [\"pydantic.BaseModel\", \"sqlalchemy.orm.DeclarativeBase\"]\n\n\n\n[lint.flake8-type-checking]\nruntime-evaluated-base-classes = [\"pydantic.BaseModel\", \"sqlalchemy.orm.DeclarativeBase\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.flake8-type-checking.runtime-evaluated-decorators": {
        "option_description": "Exempt classes and functions decorated with any of the enumerated\ndecorators from being moved into type-checking blocks.\n\nCommon examples include Pydantic's @pydantic.validate_call decorator\n(for functions) and attrs' @attrs.define decorator (for classes).\n\nThis also supports framework decorators like FastAPI's fastapi.FastAPI.get\nwhich will work across assignments in the same module.\n\nFor example:\nimport fastapi\n\napp = FastAPI(\"app\")\n\n@app.get(\"/home\")\ndef home() -> str: ...\n\nHere app.get will correctly be identified as fastapi.FastAPI.get.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-type-checking]\nruntime-evaluated-decorators = [\"pydantic.validate_call\", \"attrs.define\"]\n\n\n\n[lint.flake8-type-checking]\nruntime-evaluated-decorators = [\"pydantic.validate_call\", \"attrs.define\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.flake8-type-checking.strict": {
        "option_description": "Enforce TC001, TC002, and TC003 rules even when valid runtime imports\nare present for the same module.\n\nSee flake8-type-checking's strict option.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-type-checking]\nstrict = true\n\n\n\n[lint.flake8-type-checking]\nstrict = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.typing-modules": {
        "option_description": "A list of modules whose exports should be treated equivalently to\nmembers of the typing module.\n\nThis is useful for ensuring proper type annotation inference for\nprojects that re-export typing and typing_extensions members\nfrom a compatibility module. If omitted, any members imported from\nmodules apart from typing and typing_extensions will be treated\nas ordinary Python objects.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\ntyping-modules = [\"airflow.typing_compat\"]\n\n\n\n[lint]\ntyping-modules = [\"airflow.typing_compat\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.future-annotations": {
        "option_description": "Whether to allow rules to add from __future__ import annotations in cases where this would\nsimplify a fix or enable a new diagnostic.\n\nFor example, TC001, TC002, and TC003 can move more imports into TYPE_CHECKING blocks\nif __future__ annotations are enabled.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\n# Enable `from __future__ import annotations` imports\nfuture-annotations = true\n\n\n\n[lint]\n# Enable `from __future__ import annotations` imports\nfuture-annotations = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "TC003": {
    "description": "[Status: stable] [Name: typing-only-standard-library-import] [Message: Move standard library import <code>{}</code> into a type-checking block] What it does Checks for standard library imports that are only used for type annotations, but aren't defined in a type-checking block. Why is this bad? Unused imports add a performance overhead at runtime, and risk creating import cycles. If an import is only used in typing-only contexts, it can instead be imported conditionally under an if TYPE_CHECKING: block to minimize runtime overhead. If lint.flake8-type-checking.quote-annotations is set to true, annotations will be wrapped in quotes if doing so would enable the corresponding import to be moved into an if TYPE_CHECKING: block. If a class requires that type annotations be available at runtime (as is the case for Pydantic, SQLAlchemy, and other libraries), consider using the lint.flake8-type-checking.runtime-evaluated-base-classes and lint.flake8-type-checking.runtime-evaluated-decorators settings to mark them as such. If lint.future-annotations is set to true, from __future__ import annotations will be added if doing so would enable an import to be moved into an if TYPE_CHECKING: block. This takes precedence over the lint.flake8-type-checking.quote-annotations setting described above if both settings are enabled. Example from __future__ import annotations from pathlib import Path def func(path: Path) -> str: return str(path) Use instead: from __future__ import annotations from typing import TYPE_CHECKING if TYPE_CHECKING: from pathlib import Path def func(path: Path) -> str: return str(path)",
    "option": {
      "lint.flake8-type-checking.quote-annotations": {
        "option_description": "Whether to add quotes around type annotations, if doing so would allow\nthe corresponding import to be moved into a type-checking block.\n\nFor example, in the following, Python requires that Sequence be\navailable at runtime, despite the fact that it's only used in a type\nannotation:\n\nfrom collections.abc import Sequence\n\n\ndef func(value: Sequence[int]) -> None:\n    ...\n\nIn other words, moving from collections.abc import Sequence into an\nif TYPE_CHECKING: block above would cause a runtime error, as the\ntype would no longer be available at runtime.\n\nBy default, Ruff will respect such runtime semantics and avoid moving\nthe import to prevent such runtime errors.\n\nSetting quote-annotations to true will instruct Ruff to add quotes\naround the annotation (e.g., \"Sequence[int]\"), which in turn enables\nRuff to move the import into an if TYPE_CHECKING: block, like so:\n\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from collections.abc import Sequence\n\n\ndef func(value: \"Sequence[int]\") -> None:\n    ...\n\nNote that this setting has no effect when from __future__ import annotations\nis present, as __future__ annotations are always treated equivalently\nto quoted annotations. Similarly, this setting has no effect on Python\nversions after 3.14 because these annotations are also deferred.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-type-checking]\n# Add quotes around type annotations, if doing so would allow\n# an import to be moved into a type-checking block.\nquote-annotations = true\n\n\n\n[lint.flake8-type-checking]\n# Add quotes around type annotations, if doing so would allow\n# an import to be moved into a type-checking block.\nquote-annotations = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.flake8-type-checking.runtime-evaluated-base-classes": {
        "option_description": "Exempt classes that list any of the enumerated classes as a base class\nfrom needing to be moved into type-checking blocks.\n\nCommon examples include Pydantic's pydantic.BaseModel and SQLAlchemy's\nsqlalchemy.orm.DeclarativeBase, but can also support user-defined\nclasses that inherit from those base classes. For example, if you define\na common DeclarativeBase subclass that's used throughout your project\n(e.g., class Base(DeclarativeBase) ... in base.py), you can add it to\nthis list (runtime-evaluated-base-classes = [\"base.Base\"]) to exempt\nmodels from being moved into type-checking blocks.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-type-checking]\nruntime-evaluated-base-classes = [\"pydantic.BaseModel\", \"sqlalchemy.orm.DeclarativeBase\"]\n\n\n\n[lint.flake8-type-checking]\nruntime-evaluated-base-classes = [\"pydantic.BaseModel\", \"sqlalchemy.orm.DeclarativeBase\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.flake8-type-checking.runtime-evaluated-decorators": {
        "option_description": "Exempt classes and functions decorated with any of the enumerated\ndecorators from being moved into type-checking blocks.\n\nCommon examples include Pydantic's @pydantic.validate_call decorator\n(for functions) and attrs' @attrs.define decorator (for classes).\n\nThis also supports framework decorators like FastAPI's fastapi.FastAPI.get\nwhich will work across assignments in the same module.\n\nFor example:\nimport fastapi\n\napp = FastAPI(\"app\")\n\n@app.get(\"/home\")\ndef home() -> str: ...\n\nHere app.get will correctly be identified as fastapi.FastAPI.get.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-type-checking]\nruntime-evaluated-decorators = [\"pydantic.validate_call\", \"attrs.define\"]\n\n\n\n[lint.flake8-type-checking]\nruntime-evaluated-decorators = [\"pydantic.validate_call\", \"attrs.define\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.flake8-type-checking.strict": {
        "option_description": "Enforce TC001, TC002, and TC003 rules even when valid runtime imports\nare present for the same module.\n\nSee flake8-type-checking's strict option.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-type-checking]\nstrict = true\n\n\n\n[lint.flake8-type-checking]\nstrict = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.typing-modules": {
        "option_description": "A list of modules whose exports should be treated equivalently to\nmembers of the typing module.\n\nThis is useful for ensuring proper type annotation inference for\nprojects that re-export typing and typing_extensions members\nfrom a compatibility module. If omitted, any members imported from\nmodules apart from typing and typing_extensions will be treated\nas ordinary Python objects.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\ntyping-modules = [\"airflow.typing_compat\"]\n\n\n\n[lint]\ntyping-modules = [\"airflow.typing_compat\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.future-annotations": {
        "option_description": "Whether to allow rules to add from __future__ import annotations in cases where this would\nsimplify a fix or enable a new diagnostic.\n\nFor example, TC001, TC002, and TC003 can move more imports into TYPE_CHECKING blocks\nif __future__ annotations are enabled.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\n# Enable `from __future__ import annotations` imports\nfuture-annotations = true\n\n\n\n[lint]\n# Enable `from __future__ import annotations` imports\nfuture-annotations = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "TC004": {
    "description": "[Status: stable] [Name: runtime-import-in-type-checking-block] [Message: Move import <code>{qualified_name}</code> out of type-checking block. Import is used for more than type hinting.] What it does Checks for imports that are required at runtime but are only defined in type-checking blocks. Why is this bad? The type-checking block is not executed at runtime, so if the only definition of a symbol is in a type-checking block, it will not be available at runtime. If lint.flake8-type-checking.quote-annotations is set to true, annotations will be wrapped in quotes if doing so would enable the corresponding import to remain in the type-checking block. Example from typing import TYPE_CHECKING if TYPE_CHECKING: import foo def bar() -> None: foo.bar() # raises NameError: name 'foo' is not defined Use instead: import foo def bar() -> None: foo.bar()",
    "option": {
      "lint.flake8-type-checking.quote-annotations": {
        "option_description": "Whether to add quotes around type annotations, if doing so would allow\nthe corresponding import to be moved into a type-checking block.\n\nFor example, in the following, Python requires that Sequence be\navailable at runtime, despite the fact that it's only used in a type\nannotation:\n\nfrom collections.abc import Sequence\n\n\ndef func(value: Sequence[int]) -> None:\n    ...\n\nIn other words, moving from collections.abc import Sequence into an\nif TYPE_CHECKING: block above would cause a runtime error, as the\ntype would no longer be available at runtime.\n\nBy default, Ruff will respect such runtime semantics and avoid moving\nthe import to prevent such runtime errors.\n\nSetting quote-annotations to true will instruct Ruff to add quotes\naround the annotation (e.g., \"Sequence[int]\"), which in turn enables\nRuff to move the import into an if TYPE_CHECKING: block, like so:\n\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from collections.abc import Sequence\n\n\ndef func(value: \"Sequence[int]\") -> None:\n    ...\n\nNote that this setting has no effect when from __future__ import annotations\nis present, as __future__ annotations are always treated equivalently\nto quoted annotations. Similarly, this setting has no effect on Python\nversions after 3.14 because these annotations are also deferred.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-type-checking]\n# Add quotes around type annotations, if doing so would allow\n# an import to be moved into a type-checking block.\nquote-annotations = true\n\n\n\n[lint.flake8-type-checking]\n# Add quotes around type annotations, if doing so would allow\n# an import to be moved into a type-checking block.\nquote-annotations = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "TC005": {
    "description": "[Status: stable] [Name: empty-type-checking-block] [Message: Found empty type-checking block] What it does Checks for an empty type-checking block. Why is this bad? The type-checking block does not do anything and should be removed to avoid confusion. Example from typing import TYPE_CHECKING if TYPE_CHECKING: pass print(\"Hello, world!\") Use instead: print(\"Hello, world!\")",
    "option": {}
  },
  "TC006": {
    "description": "[Status: stable] [Name: runtime-cast-value] [Message: Add quotes to type expression in <code>typing.cast()</code>] What it does Checks for unquoted type expressions in typing.cast() calls. Why is this bad? This rule helps enforce a consistent style across your codebase. It's often necessary to quote the first argument passed to cast(), as type expressions can involve forward references, or references to symbols which are only imported in typing.TYPE_CHECKING blocks. This can lead to a visual inconsistency across different cast() calls, where some type expressions are quoted but others are not. By enabling this rule, you ensure that all type expressions passed to cast() are quoted, enforcing stylistic consistency across all of your cast() calls. In some cases where cast() is used in a hot loop, this rule may also help avoid overhead from repeatedly evaluating complex type expressions at runtime. Example from typing import cast x = cast(dict[str, int], foo) Use instead: from typing import cast x = cast(\"dict[str, int]\", foo) Fix safety This fix is safe as long as the type expression doesn't span multiple lines and includes comments on any of the lines apart from the last one.",
    "option": {}
  },
  "TC007": {
    "description": "[Status: stable] [Name: unquoted-type-alias] [Message: Add quotes to type alias] What it does Checks if PEP 613 explicit type aliases contain references to symbols that are not available at runtime. Why is this bad? Referencing type-checking only symbols results in a NameError at runtime. Example from typing import TYPE_CHECKING, TypeAlias if TYPE_CHECKING: from foo import Foo OptFoo: TypeAlias = Foo | None Use instead: from typing import TYPE_CHECKING, TypeAlias if TYPE_CHECKING: from foo import Foo OptFoo: TypeAlias = \"Foo | None\" Fix safety This rule's fix is currently always marked as unsafe, since runtime typing libraries may try to access/resolve the type alias in a way that we can't statically determine during analysis and relies on the type alias not containing any forward references.",
    "option": {}
  },
  "TC008": {
    "description": "[Status: preview] [Name: quoted-type-alias] [Message: Remove quotes from type alias] What it does Checks for unnecessary quotes in PEP 613 explicit type aliases and PEP 695 type statements. Why is this bad? Unnecessary string forward references can lead to additional overhead in runtime libraries making use of type hints. They can also have bad interactions with other runtime uses like PEP 604 type unions. PEP-613 type aliases are only flagged by the rule if Ruff can have high confidence that the quotes are unnecessary. Specifically, any PEP-613 type alias where the type expression on the right-hand side contains subscripts or attribute accesses will not be flagged. This is because type aliases can reference types that are, for example, generic in stub files but not at runtime. That can mean that a type checker expects the referenced type to be subscripted with type arguments despite the fact that doing so would fail at runtime if the type alias value was not quoted. Similarly, a type alias might need to reference a module-level attribute that exists in a stub file but not at runtime, meaning that the type alias value would need to be quoted to avoid a runtime error. Example Given: from typing import TypeAlias OptInt: TypeAlias = \"int | None\" Use instead: from typing import TypeAlias OptInt: TypeAlias = int | None Given: type OptInt = \"int | None\" Use instead: type OptInt = int | None Fix safety This rule's fix is marked as safe, unless the type annotation contains comments. See also This rule only applies to type aliases in non-stub files. For removing quotes in other contexts or in stub files, see: quoted-annotation-in-stub: A rule that removes all quoted annotations from stub files quoted-annotation: A rule that removes unnecessary quotes from annotations in runtime files.",
    "option": {}
  },
  "TC010": {
    "description": "[Status: stable] [Name: runtime-string-union] [Message: Invalid string member in <code>X | Y</code>-style union type] What it does Checks for the presence of string literals in X | Y-style union types. Why is this bad? PEP 604 introduced a new syntax for union type annotations based on the | operator. While Python's type annotations can typically be wrapped in strings to avoid runtime evaluation, the use of a string member within an X | Y-style union type will cause a runtime error. Instead, remove the quotes, wrap the entire union in quotes, or use from __future__ import annotations to disable runtime evaluation of annotations entirely. Example var: \"Foo\" | None class Foo: ... Use instead: from __future__ import annotations var: Foo | None class Foo: ... Or, extend the quotes to include the entire union: var: \"Foo | None\" class Foo: ...",
    "option": {}
  },
  "ARG001": {
    "description": "[Status: stable] [Name: unused-function-argument] [Message: Unused function argument: <code>{name}</code>] What it does Checks for the presence of unused arguments in function definitions. Why is this bad? An argument that is defined but not used is likely a mistake, and should be removed to avoid confusion. If a variable is intentionally defined-but-not-used, it should be prefixed with an underscore, or some other value that adheres to the lint.dummy-variable-rgx pattern. Example def foo(bar, baz): return bar * 2 Use instead: def foo(bar): return bar * 2",
    "option": {
      "lint.dummy-variable-rgx": {
        "option_description": "A regular expression used to identify \"dummy\" variables, or those which\nshould be ignored when enforcing (e.g.) unused-variable rules. The\ndefault expression matches _, __, and _var, but not _var_.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\n# Only ignore variables named \"_\".\ndummy-variable-rgx = \"^_$\"\n\n\n\n[lint]\n# Only ignore variables named \"_\".\ndummy-variable-rgx = \"^_$\"",
        "option_data_type": "str",
        "default_value": "\"^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "ARG002": {
    "description": "[Status: stable] [Name: unused-method-argument] [Message: Unused method argument: <code>{name}</code>] What it does Checks for the presence of unused arguments in instance method definitions. Why is this bad? An argument that is defined but not used is likely a mistake, and should be removed to avoid confusion. If a variable is intentionally defined-but-not-used, it should be prefixed with an underscore, or some other value that adheres to the lint.dummy-variable-rgx pattern. This rule exempts methods decorated with @typing.override. Removing a parameter from a subclass method (or changing a parameter's name) may cause type checkers to complain about a violation of the Liskov Substitution Principle if it means that the method now incompatibly overrides a method defined on a superclass. Explicitly decorating an overriding method with @override signals to Ruff that the method is intended to override a superclass method and that a type checker will enforce that it does so; Ruff therefore knows that it should not enforce rules about unused arguments on such methods. Example class Class: def foo(self, arg1, arg2): print(arg1) Use instead: class Class: def foo(self, arg1): print(arg1)",
    "option": {
      "lint.dummy-variable-rgx": {
        "option_description": "A regular expression used to identify \"dummy\" variables, or those which\nshould be ignored when enforcing (e.g.) unused-variable rules. The\ndefault expression matches _, __, and _var, but not _var_.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\n# Only ignore variables named \"_\".\ndummy-variable-rgx = \"^_$\"\n\n\n\n[lint]\n# Only ignore variables named \"_\".\ndummy-variable-rgx = \"^_$\"",
        "option_data_type": "str",
        "default_value": "\"^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "ARG003": {
    "description": "[Status: stable] [Name: unused-class-method-argument] [Message: Unused class method argument: <code>{name}</code>] What it does Checks for the presence of unused arguments in class method definitions. Why is this bad? An argument that is defined but not used is likely a mistake, and should be removed to avoid confusion. If a variable is intentionally defined-but-not-used, it should be prefixed with an underscore, or some other value that adheres to the lint.dummy-variable-rgx pattern. This rule exempts methods decorated with @typing.override. Removing a parameter from a subclass method (or changing a parameter's name) may cause type checkers to complain about a violation of the Liskov Substitution Principle if it means that the method now incompatibly overrides a method defined on a superclass. Explicitly decorating an overriding method with @override signals to Ruff that the method is intended to override a superclass method and that a type checker will enforce that it does so; Ruff therefore knows that it should not enforce rules about unused arguments on such methods. Example class Class: @classmethod def foo(cls, arg1, arg2): print(arg1) Use instead: class Class: @classmethod def foo(cls, arg1): print(arg1)",
    "option": {
      "lint.dummy-variable-rgx": {
        "option_description": "A regular expression used to identify \"dummy\" variables, or those which\nshould be ignored when enforcing (e.g.) unused-variable rules. The\ndefault expression matches _, __, and _var, but not _var_.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\n# Only ignore variables named \"_\".\ndummy-variable-rgx = \"^_$\"\n\n\n\n[lint]\n# Only ignore variables named \"_\".\ndummy-variable-rgx = \"^_$\"",
        "option_data_type": "str",
        "default_value": "\"^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "ARG004": {
    "description": "[Status: stable] [Name: unused-static-method-argument] [Message: Unused static method argument: <code>{name}</code>] What it does Checks for the presence of unused arguments in static method definitions. Why is this bad? An argument that is defined but not used is likely a mistake, and should be removed to avoid confusion. If a variable is intentionally defined-but-not-used, it should be prefixed with an underscore, or some other value that adheres to the lint.dummy-variable-rgx pattern. This rule exempts methods decorated with @typing.override. Removing a parameter from a subclass method (or changing a parameter's name) may cause type checkers to complain about a violation of the Liskov Substitution Principle if it means that the method now incompatibly overrides a method defined on a superclass. Explicitly decorating an overriding method with @override signals to Ruff that the method is intended to override a superclass method, and that a type checker will enforce that it does so; Ruff therefore knows that it should not enforce rules about unused arguments on such methods. Example class Class: @staticmethod def foo(arg1, arg2): print(arg1) Use instead: class Class: @staticmethod def foo(arg1): print(arg1)",
    "option": {
      "lint.dummy-variable-rgx": {
        "option_description": "A regular expression used to identify \"dummy\" variables, or those which\nshould be ignored when enforcing (e.g.) unused-variable rules. The\ndefault expression matches _, __, and _var, but not _var_.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\n# Only ignore variables named \"_\".\ndummy-variable-rgx = \"^_$\"\n\n\n\n[lint]\n# Only ignore variables named \"_\".\ndummy-variable-rgx = \"^_$\"",
        "option_data_type": "str",
        "default_value": "\"^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "ARG005": {
    "description": "[Status: stable] [Name: unused-lambda-argument] [Message: Unused lambda argument: <code>{name}</code>] What it does Checks for the presence of unused arguments in lambda expression definitions. Why is this bad? An argument that is defined but not used is likely a mistake, and should be removed to avoid confusion. If a variable is intentionally defined-but-not-used, it should be prefixed with an underscore, or some other value that adheres to the lint.dummy-variable-rgx pattern. Example my_list = [1, 2, 3, 4, 5] squares = map(lambda x, y: x**2, my_list) Use instead: my_list = [1, 2, 3, 4, 5] squares = map(lambda x: x**2, my_list)",
    "option": {
      "lint.dummy-variable-rgx": {
        "option_description": "A regular expression used to identify \"dummy\" variables, or those which\nshould be ignored when enforcing (e.g.) unused-variable rules. The\ndefault expression matches _, __, and _var, but not _var_.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\n# Only ignore variables named \"_\".\ndummy-variable-rgx = \"^_$\"\n\n\n\n[lint]\n# Only ignore variables named \"_\".\ndummy-variable-rgx = \"^_$\"",
        "option_data_type": "str",
        "default_value": "\"^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PTH100": {
    "description": "[Status: stable] [Name: os-path-abspath] [Message: <code>os.path.abspath()</code> should be replaced by <code>Path.resolve()</code>] What it does Checks for uses of os.path.abspath. Why is this bad? pathlib offers a high-level API for path manipulation, as compared to the lower-level API offered by os.path. When possible, using Path object methods such as Path.resolve() can improve readability over the os.path module's counterparts (e.g., os.path.abspath()). Examples import os file_path = os.path.abspath(\"../path/to/file\") Use instead: from pathlib import Path file_path = Path(\"../path/to/file\").resolve() Known issues While using pathlib can improve the readability and type safety of your code, it can be less performant than the lower-level alternatives that work directly with strings, especially on older versions of Python. Fix Safety This rule's fix is always marked as unsafe because Path.resolve() resolves symlinks, while os.path.abspath() does not. If resolving symlinks is important, you may need to use Path.absolute(). However, Path.absolute() also does not remove any .. components in a path, unlike os.path.abspath() and Path.resolve(), so if that specific combination of behaviors is required, there's no existing pathlib alternative. See CPython issue #69200. Additionally, the fix is marked as unsafe because os.path.abspath() returns str or bytes (AnyStr), while Path.resolve() returns a Path object. This change in return type can break code that uses the return value.",
    "option": {}
  },
  "PTH101": {
    "description": "[Status: stable] [Name: os-chmod] [Message: <code>os.chmod()</code> should be replaced by <code>Path.chmod()</code>] What it does Checks for uses of os.chmod. Why is this bad? pathlib offers a high-level API for path manipulation, as compared to the lower-level API offered by os. When possible, using Path object methods such as Path.chmod() can improve readability over the os module's counterparts (e.g., os.chmod()). Examples import os os.chmod(\"file.py\", 0o444) Use instead: from pathlib import Path Path(\"file.py\").chmod(0o444) Known issues While using pathlib can improve the readability and type safety of your code, it can be less performant than the lower-level alternatives that work directly with strings, especially on older versions of Python. Fix Safety This rule's fix is marked as unsafe if the replacement would remove comments attached to the original expression.",
    "option": {}
  },
  "PTH102": {
    "description": "[Status: stable] [Name: os-mkdir] [Message: <code>os.mkdir()</code> should be replaced by <code>Path.mkdir()</code>] What it does Checks for uses of os.mkdir. Why is this bad? pathlib offers a high-level API for path manipulation, as compared to the lower-level API offered by os. When possible, using Path object methods such as Path.mkdir() can improve readability over the os module's counterparts (e.g., os.mkdir()). Examples import os os.mkdir(\"./directory/\") Use instead: from pathlib import Path Path(\"./directory/\").mkdir() Known issues While using pathlib can improve the readability and type safety of your code, it can be less performant than the lower-level alternatives that work directly with strings, especially on older versions of Python. Fix Safety This rule's fix is marked as unsafe if the replacement would remove comments attached to the original expression.",
    "option": {}
  },
  "PTH103": {
    "description": "[Status: stable] [Name: os-makedirs] [Message: <code>os.makedirs()</code> should be replaced by <code>Path.mkdir(parents=True)</code>] What it does Checks for uses of os.makedirs. Why is this bad? pathlib offers a high-level API for path manipulation, as compared to the lower-level API offered by os. When possible, using Path object methods such as Path.mkdir(parents=True) can improve readability over the os module's counterparts (e.g., os.makedirs(). Examples import os os.makedirs(\"./nested/directory/\") Use instead: from pathlib import Path Path(\"./nested/directory/\").mkdir(parents=True) Known issues While using pathlib can improve the readability and type safety of your code, it can be less performant than the lower-level alternatives that work directly with strings, especially on older versions of Python. Fix Safety This rule's fix is marked as unsafe if the replacement would remove comments attached to the original expression.",
    "option": {}
  },
  "PTH104": {
    "description": "[Status: stable] [Name: os-rename] [Message: <code>os.rename()</code> should be replaced by <code>Path.rename()</code>] What it does Checks for uses of os.rename. Why is this bad? pathlib offers a high-level API for path manipulation, as compared to the lower-level API offered by os. When possible, using Path object methods such as Path.rename() can improve readability over the os module's counterparts (e.g., os.rename()). Examples import os os.rename(\"old.py\", \"new.py\") Use instead: from pathlib import Path Path(\"old.py\").rename(\"new.py\") Known issues While using pathlib can improve the readability and type safety of your code, it can be less performant than the lower-level alternatives that work directly with strings, especially on older versions of Python. Fix Safety This rule's fix is marked as unsafe if the replacement would remove comments attached to the original expression. Additionally, the fix is marked as unsafe when the return value is used because the type changes from None to a Path object.",
    "option": {}
  },
  "PTH105": {
    "description": "[Status: stable] [Name: os-replace] [Message: <code>os.replace()</code> should be replaced by <code>Path.replace()</code>] What it does Checks for uses of os.replace. Why is this bad? pathlib offers a high-level API for path manipulation, as compared to the lower-level API offered by os. When possible, using Path object methods such as Path.replace() can improve readability over the os module's counterparts (e.g., os.replace()). Note that os functions may be preferable if performance is a concern, e.g., in hot loops. Examples import os os.replace(\"old.py\", \"new.py\") Use instead: from pathlib import Path Path(\"old.py\").replace(\"new.py\") Known issues While using pathlib can improve the readability and type safety of your code, it can be less performant than the lower-level alternatives that work directly with strings, especially on older versions of Python. Fix Safety This rule's fix is marked as unsafe if the replacement would remove comments attached to the original expression. Additionally, the fix is marked as unsafe when the return value is used because the type changes from None to a Path object.",
    "option": {}
  },
  "PTH106": {
    "description": "[Status: stable] [Name: os-rmdir] [Message: <code>os.rmdir()</code> should be replaced by <code>Path.rmdir()</code>] What it does Checks for uses of os.rmdir. Why is this bad? pathlib offers a high-level API for path manipulation, as compared to the lower-level API offered by os. When possible, using Path object methods such as Path.rmdir() can improve readability over the os module's counterparts (e.g., os.rmdir()). Examples import os os.rmdir(\"folder/\") Use instead: from pathlib import Path Path(\"folder/\").rmdir() Known issues While using pathlib can improve the readability and type safety of your code, it can be less performant than the lower-level alternatives that work directly with strings, especially on older versions of Python. Fix Safety This rule's fix is marked as unsafe if the replacement would remove comments attached to the original expression.",
    "option": {}
  },
  "PTH107": {
    "description": "[Status: stable] [Name: os-remove] [Message: <code>os.remove()</code> should be replaced by <code>Path.unlink()</code>] What it does Checks for uses of os.remove. Why is this bad? pathlib offers a high-level API for path manipulation, as compared to the lower-level API offered by os. When possible, using Path object methods such as Path.unlink() can improve readability over the os module's counterparts (e.g., os.remove()). Examples import os os.remove(\"file.py\") Use instead: from pathlib import Path Path(\"file.py\").unlink() Known issues While using pathlib can improve the readability and type safety of your code, it can be less performant than the lower-level alternatives that work directly with strings, especially on older versions of Python. Fix Safety This rule's fix is marked as unsafe if the replacement would remove comments attached to the original expression.",
    "option": {}
  },
  "PTH108": {
    "description": "[Status: stable] [Name: os-unlink] [Message: <code>os.unlink()</code> should be replaced by <code>Path.unlink()</code>] What it does Checks for uses of os.unlink. Why is this bad? pathlib offers a high-level API for path manipulation, as compared to the lower-level API offered by os. When possible, using Path object methods such as Path.unlink() can improve readability over the os module's counterparts (e.g., os.unlink()). Examples import os os.unlink(\"file.py\") Use instead: from pathlib import Path Path(\"file.py\").unlink() Known issues While using pathlib can improve the readability and type safety of your code, it can be less performant than the lower-level alternatives that work directly with strings, especially on older versions of Python. Fix Safety This rule's fix is marked as unsafe if the replacement would remove comments attached to the original expression.",
    "option": {}
  },
  "PTH109": {
    "description": "[Status: stable] [Name: os-getcwd] [Message: <code>os.getcwd()</code> should be replaced by <code>Path.cwd()</code>] What it does Checks for uses of os.getcwd and os.getcwdb. Why is this bad? pathlib offers a high-level API for path manipulation, as compared to the lower-level API offered by os. When possible, using Path object methods such as Path.cwd() can improve readability over the os module's counterparts (e.g., os.getcwd()). Examples import os cwd = os.getcwd() Use instead: from pathlib import Path cwd = Path.cwd() Known issues While using pathlib can improve the readability and type safety of your code, it can be less performant than the lower-level alternatives that work directly with strings, especially on older versions of Python. Fix Safety This rule's fix is marked as unsafe if the replacement would remove comments attached to the original expression. Additionally, the fix is marked as unsafe when the return value is used because the type changes from str or bytes to a Path object.",
    "option": {}
  },
  "PTH110": {
    "description": "[Status: stable] [Name: os-path-exists] [Message: <code>os.path.exists()</code> should be replaced by <code>Path.exists()</code>] What it does Checks for uses of os.path.exists. Why is this bad? pathlib offers a high-level API for path manipulation, as compared to the lower-level API offered by os.path. When possible, using Path object methods such as Path.exists() can improve readability over the os.path module's counterparts (e.g., os.path.exists()). Examples import os os.path.exists(\"file.py\") Use instead: from pathlib import Path Path(\"file.py\").exists() Known issues While using pathlib can improve the readability and type safety of your code, it can be less performant than the lower-level alternatives that work directly with strings, especially on older versions of Python. Fix Safety This rule's fix is marked as unsafe if the replacement would remove comments attached to the original expression.",
    "option": {}
  },
  "PTH111": {
    "description": "[Status: stable] [Name: os-path-expanduser] [Message: <code>os.path.expanduser()</code> should be replaced by <code>Path.expanduser()</code>] What it does Checks for uses of os.path.expanduser. Why is this bad? pathlib offers a high-level API for path manipulation, as compared to the lower-level API offered by os.path. When possible, using Path object methods such as Path.expanduser() can improve readability over the os.path module's counterparts (e.g., as os.path.expanduser()). Examples import os os.path.expanduser(\"~/films/Monty Python\") Use instead: from pathlib import Path Path(\"~/films/Monty Python\").expanduser() Known issues While using pathlib can improve the readability and type safety of your code, it can be less performant than the lower-level alternatives that work directly with strings, especially on older versions of Python. Fix Safety This rule's fix is always marked as unsafe because the behaviors of os.path.expanduser and Path.expanduser differ when a user's home directory can't be resolved: os.path.expanduser returns the input unchanged, while Path.expanduser raises RuntimeError. Additionally, the fix is marked as unsafe because os.path.expanduser() returns str or bytes (AnyStr), while Path.expanduser() returns a Path object. This change in return type can break code that uses the return value.",
    "option": {}
  },
  "PTH112": {
    "description": "[Status: stable] [Name: os-path-isdir] [Message: <code>os.path.isdir()</code> should be replaced by <code>Path.is_dir()</code>] What it does Checks for uses of os.path.isdir. Why is this bad? pathlib offers a high-level API for path manipulation, as compared to the lower-level API offered by os.path. When possible, using Path object methods such as Path.is_dir() can improve readability over the os.path module's counterparts (e.g., os.path.isdir()). Examples import os os.path.isdir(\"docs\") Use instead: from pathlib import Path Path(\"docs\").is_dir() Known issues While using pathlib can improve the readability and type safety of your code, it can be less performant than the lower-level alternatives that work directly with strings, especially on older versions of Python. Fix Safety This rule's fix is marked as unsafe if the replacement would remove comments attached to the original expression.",
    "option": {}
  },
  "PTH113": {
    "description": "[Status: stable] [Name: os-path-isfile] [Message: <code>os.path.isfile()</code> should be replaced by <code>Path.is_file()</code>] What it does Checks for uses of os.path.isfile. Why is this bad? pathlib offers a high-level API for path manipulation, as compared to the lower-level API offered by os.path. When possible, using Path object methods such as Path.is_file() can improve readability over the os.path module's counterparts (e.g., os.path.isfile()). Examples import os os.path.isfile(\"docs\") Use instead: from pathlib import Path Path(\"docs\").is_file() Known issues While using pathlib can improve the readability and type safety of your code, it can be less performant than the lower-level alternatives that work directly with strings, especially on older versions of Python. Fix Safety This rule's fix is marked as unsafe if the replacement would remove comments attached to the original expression.",
    "option": {}
  },
  "PTH114": {
    "description": "[Status: stable] [Name: os-path-islink] [Message: <code>os.path.islink()</code> should be replaced by <code>Path.is_symlink()</code>] What it does Checks for uses of os.path.islink. Why is this bad? pathlib offers a high-level API for path manipulation, as compared to the lower-level API offered by os.path. When possible, using Path object methods such as Path.is_symlink() can improve readability over the os.path module's counterparts (e.g., os.path.islink()). Examples import os os.path.islink(\"docs\") Use instead: from pathlib import Path Path(\"docs\").is_symlink() Known issues While using pathlib can improve the readability and type safety of your code, it can be less performant than the lower-level alternatives that work directly with strings, especially on older versions of Python. Fix Safety This rule's fix is marked as unsafe if the replacement would remove comments attached to the original expression.",
    "option": {}
  },
  "PTH115": {
    "description": "[Status: stable] [Name: os-readlink] [Message: <code>os.readlink()</code> should be replaced by <code>Path.readlink()</code>] What it does Checks for uses of os.readlink. Why is this bad? pathlib offers a high-level API for path manipulation, as compared to the lower-level API offered by os. When possible, using Path object methods such as Path.readlink() can improve readability over the os module's counterparts (e.g., os.readlink()). Examples import os os.readlink(file_name) Use instead: from pathlib import Path Path(file_name).readlink() Known issues While using pathlib can improve the readability and type safety of your code, it can be less performant than the lower-level alternatives that work directly with strings, especially on older versions of Python. Fix Safety This rule's fix is marked as unsafe if the replacement would remove comments attached to the original expression. Additionally, the fix is marked as unsafe when the return value is used because the type changes from str or bytes (AnyStr) to a Path object.",
    "option": {}
  },
  "PTH116": {
    "description": "[Status: stable] [Name: os-stat] [Message: <code>os.stat()</code> should be replaced by <code>Path.stat()</code>, <code>Path.owner()</code>, or <code>Path.group()</code>] What it does Checks for uses of os.stat. Why is this bad? pathlib offers a high-level API for path manipulation, as compared to the lower-level API offered by os. When possible, using Path object methods such as Path.stat() can improve readability over the os module's counterparts (e.g., os.path.stat()). Examples import os from pwd import getpwuid from grp import getgrgid stat = os.stat(file_name) owner_name = getpwuid(stat.st_uid).pw_name group_name = getgrgid(stat.st_gid).gr_name Use instead: from pathlib import Path file_path = Path(file_name) stat = file_path.stat() owner_name = file_path.owner() group_name = file_path.group() Known issues While using pathlib can improve the readability and type safety of your code, it can be less performant than the lower-level alternatives that work directly with strings, especially on older versions of Python.",
    "option": {}
  },
  "PTH117": {
    "description": "[Status: stable] [Name: os-path-isabs] [Message: <code>os.path.isabs()</code> should be replaced by <code>Path.is_absolute()</code>] What it does Checks for uses of os.path.isabs. Why is this bad? pathlib offers a high-level API for path manipulation, as compared to the lower-level API offered by os.path. When possible, using Path object methods such as Path.is_absolute() can improve readability over the os.path module's counterparts (e.g., as os.path.isabs()). Examples import os if os.path.isabs(file_name): print(\"Absolute path!\") Use instead: from pathlib import Path if Path(file_name).is_absolute(): print(\"Absolute path!\") Known issues While using pathlib can improve the readability and type safety of your code, it can be less performant than the lower-level alternatives that work directly with strings, especially on older versions of Python.",
    "option": {}
  },
  "PTH118": {
    "description": "[Status: stable] [Name: os-path-join] [Message: <code>os.{module}.join()</code> should be replaced by <code>Path</code> with <code>/</code> operator] What it does Checks for uses of os.path.join. Why is this bad? pathlib offers a high-level API for path manipulation, as compared to the lower-level API offered by os.path. When possible, using Path object methods such as Path.joinpath() or the / operator can improve readability over the os.path module's counterparts (e.g., os.path.join()). Examples import os os.path.join(os.path.join(ROOT_PATH, \"folder\"), \"file.py\") Use instead: from pathlib import Path Path(ROOT_PATH) / \"folder\" / \"file.py\" Known issues While using pathlib can improve the readability and type safety of your code, it can be less performant than the lower-level alternatives that work directly with strings, especially on older versions of Python.",
    "option": {}
  },
  "PTH119": {
    "description": "[Status: stable] [Name: os-path-basename] [Message: <code>os.path.basename()</code> should be replaced by <code>Path.name</code>] What it does Checks for uses of os.path.basename. Why is this bad? pathlib offers a high-level API for path manipulation, as compared to the lower-level API offered by os.path. When possible, using Path object methods such as Path.name can improve readability over the os.path module's counterparts (e.g., os.path.basename()). Examples import os os.path.basename(__file__) Use instead: from pathlib import Path Path(__file__).name Known issues While using pathlib can improve the readability and type safety of your code, it can be less performant than the lower-level alternatives that work directly with strings, especially on older versions of Python. Fix Safety This rule's fix is always marked as unsafe because the replacement is not always semantically equivalent to the original code. In particular, pathlib performs path normalization, which can alter the result compared to os.path.basename. For example, this normalization: Collapses consecutive slashes (e.g., \"a//b\"  \"a/b\"). Removes trailing slashes (e.g., \"a/b/\"  \"a/b\"). Eliminates \".\" (e.g., \"a/./b\"  \"a/b\"). As a result, code relying on the exact string returned by os.path.basename may behave differently after the fix.",
    "option": {}
  },
  "PTH120": {
    "description": "[Status: stable] [Name: os-path-dirname] [Message: <code>os.path.dirname()</code> should be replaced by <code>Path.parent</code>] What it does Checks for uses of os.path.dirname. Why is this bad? pathlib offers a high-level API for path manipulation, as compared to the lower-level API offered by os.path. When possible, using Path object methods such as Path.parent can improve readability over the os.path module's counterparts (e.g., os.path.dirname()). Examples import os os.path.dirname(__file__) Use instead: from pathlib import Path Path(__file__).parent Fix Safety This rule's fix is always marked as unsafe because the replacement is not always semantically equivalent to the original code. In particular, pathlib performs path normalization, which can alter the result compared to os.path.dirname. For example, this normalization: Collapses consecutive slashes (e.g., \"a//b\"  \"a/b\"). Removes trailing slashes (e.g., \"a/b/\"  \"a/b\"). Eliminates \".\" (e.g., \"a/./b\"  \"a/b\"). As a result, code relying on the exact string returned by os.path.dirname may behave differently after the fix. Additionally, the fix is marked as unsafe because os.path.dirname() returns str or bytes (AnyStr), while Path.parent returns a Path object. This change in return type can break code that uses the return value. Known issues While using pathlib can improve the readability and type safety of your code, it can be less performant than the lower-level alternatives that work directly with strings, especially on older versions of Python.",
    "option": {}
  },
  "PTH121": {
    "description": "[Status: stable] [Name: os-path-samefile] [Message: <code>os.path.samefile()</code> should be replaced by <code>Path.samefile()</code>] What it does Checks for uses of os.path.samefile. Why is this bad? pathlib offers a high-level API for path manipulation, as compared to the lower-level API offered by os.path. When possible, using Path object methods such as Path.samefile() can improve readability over the os.path module's counterparts (e.g., os.path.samefile()). Examples import os os.path.samefile(\"f1.py\", \"f2.py\") Use instead: from pathlib import Path Path(\"f1.py\").samefile(\"f2.py\") Known issues While using pathlib can improve the readability and type safety of your code, it can be less performant than the lower-level alternatives that work directly with strings, especially on older versions of Python. Fix Safety This rule's fix is marked as unsafe if the replacement would remove comments attached to the original expression.",
    "option": {}
  },
  "PTH122": {
    "description": "[Status: stable] [Name: os-path-splitext] [Message: <code>os.path.splitext()</code> should be replaced by <code>Path.suffix</code>, <code>Path.stem</code>, and <code>Path.parent</code>] What it does Checks for uses of os.path.splitext. Why is this bad? pathlib offers a high-level API for path manipulation, as compared to the lower-level API offered by os.path. When possible, using Path object methods such as Path.suffix and Path.stem can improve readability over the os.path module's counterparts (e.g., os.path.splitext()). os.path.splitext() specifically returns a tuple of the file root and extension (e.g., given splitext('/foo/bar.py'), os.path.splitext() returns (\"foo/bar\", \".py\"). These outputs can be reconstructed through a combination of Path.suffix (\".py\"), Path.stem (\"bar\"), and Path.parent (\"foo\"). Examples import os (root, ext) = os.path.splitext(\"foo/bar.py\") Use instead: from pathlib import Path path = Path(\"foo/bar.py\") root = path.parent / path.stem ext = path.suffix Known issues While using pathlib can improve the readability and type safety of your code, it can be less performant than the lower-level alternatives that work directly with strings, especially on older versions of Python.",
    "option": {}
  },
  "PTH123": {
    "description": "[Status: stable] [Name: builtin-open] [Message: <code>open()</code> should be replaced by <code>Path.open()</code>] What it does Checks for uses of the open() builtin. Why is this bad? pathlib offers a high-level API for path manipulation. When possible, using Path object methods such as Path.open() can improve readability over the open builtin. Examples with open(\"f1.py\", \"wb\") as fp: ... Use instead: from pathlib import Path with Path(\"f1.py\").open(\"wb\") as fp: ... Known issues While using pathlib can improve the readability and type safety of your code, it can be less performant than working directly with strings, especially on older versions of Python. Fix Safety This rule's fix is marked as unsafe if the replacement would remove comments attached to the original expression.",
    "option": {}
  },
  "PTH124": {
    "description": "[Status: stable] [Name: py-path] [Message: <code>py.path</code> is in maintenance mode, use <code>pathlib</code> instead] What it does Checks for uses of the py.path library. Why is this bad? The py.path library is in maintenance mode. Instead, prefer the standard library's pathlib module, or third-party modules like path (formerly py.path). Examples import py.path p = py.path.local(\"/foo/bar\").join(\"baz/qux\") Use instead: from pathlib import Path p = Path(\"/foo/bar\") / \"bar\" / \"qux\"",
    "option": {}
  },
  "PTH201": {
    "description": "[Status: stable] [Name: path-constructor-current-directory] [Message: Do not pass the current directory explicitly to <code>Path</code>] What it does Checks for pathlib.Path objects that are initialized with the current directory. Why is this bad? The Path() constructor defaults to the current directory, so passing it in explicitly (as \".\") is unnecessary. Example from pathlib import Path _ = Path(\".\") Use instead: from pathlib import Path _ = Path() Fix safety This fix is marked unsafe if there are comments inside the parentheses, as applying the fix will delete them.",
    "option": {}
  },
  "PTH202": {
    "description": "[Status: stable] [Name: os-path-getsize] [Message: <code>os.path.getsize</code> should be replaced by <code>Path.stat().st_size</code>] What it does Checks for uses of os.path.getsize. Why is this bad? pathlib offers a high-level API for path manipulation, as compared to the lower-level API offered by os.path. When possible, using Path object methods such as Path.stat() can improve readability over the os.path module's counterparts (e.g., os.path.getsize()). Example import os os.path.getsize(__file__) Use instead: from pathlib import Path Path(__file__).stat().st_size Known issues While using pathlib can improve the readability and type safety of your code, it can be less performant than the lower-level alternatives that work directly with strings, especially on older versions of Python. Fix Safety This rule's fix is marked as unsafe if the replacement would remove comments attached to the original expression.",
    "option": {}
  },
  "PTH203": {
    "description": "[Status: stable] [Name: os-path-getatime] [Message: <code>os.path.getatime</code> should be replaced by <code>Path.stat().st_atime</code>] What it does Checks for uses of os.path.getatime. Why is this bad? pathlib offers a high-level API for path manipulation, as compared to the lower-level API offered by os.path. When possible, using Path object methods such as Path.stat() can improve readability over the os.path module's counterparts (e.g., os.path.getatime()). Example import os os.path.getatime(__file__) Use instead: from pathlib import Path Path(__file__).stat().st_atime Known issues While using pathlib can improve the readability and type safety of your code, it can be less performant than the lower-level alternatives that work directly with strings, especially on older versions of Python. Fix Safety This rule's fix is marked as unsafe if the replacement would remove comments attached to the original expression.",
    "option": {}
  },
  "PTH204": {
    "description": "[Status: stable] [Name: os-path-getmtime] [Message: <code>os.path.getmtime</code> should be replaced by <code>Path.stat().st_mtime</code>] What it does Checks for uses of os.path.getmtime. Why is this bad? pathlib offers a high-level API for path manipulation, as compared to the lower-level API offered by os.path. When possible, using Path object methods such as Path.stat() can improve readability over the os.path module's counterparts (e.g., os.path.getmtime()). Example import os os.path.getmtime(__file__) Use instead: from pathlib import Path Path(__file__).stat().st_mtime Known issues While using pathlib can improve the readability and type safety of your code, it can be less performant than the lower-level alternatives that work directly with strings, especially on older versions of Python. Fix Safety This rule's fix is marked as unsafe if the replacement would remove comments attached to the original expression.",
    "option": {}
  },
  "PTH205": {
    "description": "[Status: stable] [Name: os-path-getctime] [Message: <code>os.path.getctime</code> should be replaced by <code>Path.stat().st_ctime</code>] What it does Checks for uses of os.path.getctime. Why is this bad? pathlib offers a high-level API for path manipulation, as compared to the lower-level API offered by os.path. When possible, using Path object methods such as Path.stat() can improve readability over the os.path module's counterparts (e.g., os.path.getctime()). Example import os os.path.getctime(__file__) Use instead: from pathlib import Path Path(__file__).stat().st_ctime Known issues While using pathlib can improve the readability and type safety of your code, it can be less performant than the lower-level alternatives that work directly with strings, especially on older versions of Python. Fix Safety This rule's fix is marked as unsafe if the replacement would remove comments attached to the original expression.",
    "option": {}
  },
  "PTH206": {
    "description": "[Status: stable] [Name: os-sep-split] [Message: Replace <code>.split(os.sep)</code> with <code>Path.parts</code>] What it does Checks for uses of .split(os.sep) Why is this bad? The pathlib module in the standard library should be used for path manipulation. It provides a high-level API with the functionality needed for common operations on Path objects. Example If not all parts of the path are needed, then the name and parent attributes of the Path object should be used. Otherwise, the parts attribute can be used as shown in the last example. import os \"path/to/file_name.txt\".split(os.sep)[-1] \"path/to/file_name.txt\".split(os.sep)[-2] # Iterating over the path parts if any(part in blocklist for part in \"my/file/path\".split(os.sep)): ... Use instead: from pathlib import Path Path(\"path/to/file_name.txt\").name Path(\"path/to/file_name.txt\").parent.name # Iterating over the path parts if any(part in blocklist for part in Path(\"my/file/path\").parts): ... Known issues While using pathlib can improve the readability and type safety of your code, it can be less performant than working directly with strings, especially on older versions of Python.",
    "option": {}
  },
  "PTH207": {
    "description": "[Status: stable] [Name: glob] [Message: Replace <code>{function}</code> with <code>Path.glob</code> or <code>Path.rglob</code>] What it does Checks for the use of glob.glob() and glob.iglob(). Why is this bad? pathlib offers a high-level API for path manipulation, as compared to the lower-level API offered by os and glob. When possible, using Path object methods such as Path.glob() can improve readability over their low-level counterparts (e.g., glob.glob()). Note that glob.glob() and Path.glob() are not exact equivalents: glob-module functions Path.glob() Hidden files Hidden files are excluded by default. On Python 3.11+, the include_hidden keyword can be used to include hidden directories. Includes hidden files by default. Eagerness glob.iglob() returns a lazy iterator. Under the hood, glob.glob() simply converts the iterator to a list. Path.glob() returns a lazy iterator. Working directory glob.glob() and glob.iglob() take a root_dir keyword to set the current working directory. Path.rglob() can be used to return the relative path. Globstar (**) The recursive flag must be set to True for the ** pattern to match any files and zero or more directories, subdirectories, and symbolic links. The ** pattern in Path.glob() means \"this directory and all subdirectories, recursively\". In other words, it enables recursive globbing. Example import glob import os glob.glob(os.path.join(\"my_path\", \"requirements*.txt\")) Use instead: from pathlib import Path Path(\"my_path\").glob(\"requirements*.txt\") Known issues While using pathlib can improve the readability and type safety of your code, it can be less performant than the lower-level alternatives that work directly with strings, especially on older versions of Python.",
    "option": {}
  },
  "PTH208": {
    "description": "[Status: stable] [Name: os-listdir] [Message: Use <code>pathlib.Path.iterdir()</code> instead.] What it does Checks for uses of os.listdir. Why is this bad? pathlib offers a high-level API for path manipulation, as compared to the lower-level API offered by os. When possible, using pathlib's Path.iterdir() can improve readability over os.listdir(). Example p = \".\" for d in os.listdir(p): ... if os.listdir(p): ... if \"file\" in os.listdir(p): ... Use instead: p = Path(\".\") for d in p.iterdir(): ... if any(p.iterdir()): ... if (p / \"file\").exists(): ... Known issues While using pathlib can improve the readability and type safety of your code, it can be less performant than the lower-level alternatives that work directly with strings, especially on older versions of Python.",
    "option": {}
  },
  "PTH210": {
    "description": "[Status: stable] [Name: invalid-pathlib-with-suffix] [Message: Invalid suffix passed to <code>.with_suffix()</code>] What it does Checks for pathlib.Path.with_suffix() calls where the given suffix does not have a leading dot or the given suffix is a single dot \".\" and the Python version is less than 3.14. Why is this bad? Path.with_suffix() will raise an error at runtime if the given suffix is not prefixed with a dot or, in versions prior to Python 3.14, if it is a single dot \".\". Example from pathlib import Path path = Path() path.with_suffix(\"py\") Use instead: from pathlib import Path path = Path() path.with_suffix(\".py\") Known problems This rule is likely to have false negatives, as Ruff can only emit the lint if it can say for sure that a binding refers to a Path object at runtime. Due to type inference limitations, Ruff is currently only confident about this if it can see that the binding originates from a function parameter annotated with Path or from a direct assignment to a Path() constructor call. Fix safety The fix for this rule adds a leading period to the string passed to the with_suffix() call. This fix is marked as unsafe, as it changes runtime behaviour: the call would previously always have raised an exception, but no longer will. Moreover, it's impossible to determine if this is the correct fix for a given situation (it's possible that the string was correct but was being passed to the wrong method entirely, for example). No fix is offered if the suffix \".\" is given, since the intent is unclear.",
    "option": {}
  },
  "PTH211": {
    "description": "[Status: stable] [Name: os-symlink] [Message: <code>os.symlink</code> should be replaced by <code>Path.symlink_to</code>] What it does Checks for uses of os.symlink. Why is this bad? pathlib offers a high-level API for path manipulation, as compared to the lower-level API offered by os.symlink. Example import os os.symlink(\"usr/bin/python\", \"tmp/python\", target_is_directory=False) Use instead: from pathlib import Path Path(\"tmp/python\").symlink_to(\"usr/bin/python\") Known issues While using pathlib can improve the readability and type safety of your code, it can be less performant than the lower-level alternatives that work directly with strings, especially on older versions of Python. Fix Safety This rule's fix is marked as unsafe if the replacement would remove comments attached to the original expression.",
    "option": {}
  },
  "FLY002": {
    "description": "[Status: stable] [Name: static-join-to-f-string] [Message: Consider <code>{expression}</code> instead of string join] What it does Checks for str.join calls that can be replaced with f-strings. Why is this bad? f-strings are more readable and generally preferred over str.join calls. Example \" \".join((foo, bar)) Use instead: f\"{foo} {bar}\" Fix safety The fix is always marked unsafe because the evaluation of the f-string expressions will default to calling the __format__ method of each object, whereas str.join expects each object to be an instance of str and uses the corresponding string. Therefore it is possible for the values of the resulting strings to differ, or for one expression to raise an exception while the other does not.",
    "option": {}
  },
  "I001": {
    "description": "[Status: stable] [Name: unsorted-imports] [Message: Import block is un-sorted or un-formatted] What it does De-duplicates, groups, and sorts imports based on the provided isort settings. Why is this bad? Consistency is good. Use a common convention for imports to make your code more readable and idiomatic. Example import pandas import numpy as np Use instead: import numpy as np import pandas",
    "option": {}
  },
  "I002": {
    "description": "[Status: stable] [Name: missing-required-import] [Message: Missing required import: <code>{name}</code>] What it does Adds any required imports, as specified by the user, to the top of the file. Why is this bad? In some projects, certain imports are required to be present in all files. For example, some projects assume that from __future__ import annotations is enabled, and thus require that import to be present in all files. Omitting a \"required\" import (as specified by the user) can cause errors or unexpected behavior. Example import typing Use instead: from __future__ import annotations import typing",
    "option": {
      "lint.isort.required-imports": {
        "option_description": "Add the specified import line to all files.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.isort]\nrequired-imports = [\"from __future__ import annotations\"]\n\n\n\n[lint.isort]\nrequired-imports = [\"from __future__ import annotations\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "C901": {
    "description": "[Status: stable] [Name: complex-structure] [Message: <code>{name}</code> is too complex ({complexity} > {max_complexity})] What it does Checks for functions with a high McCabe complexity. Why is this bad? The McCabe complexity of a function is a measure of the complexity of the control flow graph of the function. It is calculated by adding one to the number of decision points in the function. A decision point is a place in the code where the program has a choice of two or more paths to follow. Functions with a high complexity are hard to understand and maintain. Example def foo(a, b, c): if a: if b: if c: return 1 else: return 2 else: return 3 else: return 4 Use instead: def foo(a, b, c): if not a: return 4 if not b: return 3 if not c: return 2 return 1",
    "option": {
      "lint.mccabe.max-complexity": {
        "option_description": "The maximum McCabe complexity to allow before triggering C901 errors.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.mccabe]\n# Flag errors (`C901`) whenever the complexity level exceeds 5.\nmax-complexity = 5\n\n\n\n[lint.mccabe]\n# Flag errors (`C901`) whenever the complexity level exceeds 5.\nmax-complexity = 5",
        "option_data_type": "int",
        "default_value": "10",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "NPY001": {
    "description": "[Status: stable] [Name: numpy-deprecated-type-alias] [Message: Type alias <code>np.{type_name}</code> is deprecated, replace with builtin type] What it does Checks for deprecated NumPy type aliases. Why is this bad? NumPy's np.int has long been an alias of the builtin int; the same is true of np.float and others. These aliases exist primarily for historic reasons, and have been a cause of frequent confusion for newcomers. These aliases were deprecated in 1.20, and removed in 1.24. Note, however, that np.bool and np.long were reintroduced in 2.0 with different semantics, and are thus omitted from this rule. Example import numpy as np np.int Use instead: int",
    "option": {}
  },
  "NPY002": {
    "description": "[Status: stable] [Name: numpy-legacy-random] [Message: Replace legacy <code>np.random.{method_name}</code> call with <code>np.random.Generator</code>] What it does Checks for the use of legacy np.random function calls. Why is this bad? According to the NumPy documentation's Legacy Random Generation: The RandomState provides access to legacy generators... This class should only be used if it is essential to have randoms that are identical to what would have been produced by previous versions of NumPy. The members exposed directly on the random module are convenience functions that alias to methods on a global singleton RandomState instance. NumPy recommends using a dedicated Generator instance rather than the random variate generation methods exposed directly on the random module, as the new Generator is both faster and has better statistical properties. See the documentation on Random Sampling and NEP 19 for further details. Example import numpy as np np.random.seed(1337) np.random.normal() Use instead: rng = np.random.default_rng(1337) rng.normal()",
    "option": {}
  },
  "NPY003": {
    "description": "[Status: stable] [Name: numpy-deprecated-function] [Message: <code>np.{existing}</code> is deprecated; use <code>np.{replacement}</code> instead] What it does Checks for uses of deprecated NumPy functions. Why is this bad? When NumPy functions are deprecated, they are usually replaced with newer, more efficient versions, or with functions that are more consistent with the rest of the NumPy API. Prefer newer APIs over deprecated ones. Example import numpy as np np.alltrue([True, False]) Use instead: import numpy as np np.all([True, False])",
    "option": {}
  },
  "NPY201": {
    "description": "[Status: stable] [Name: numpy2-deprecation] [Message: <code>np.{existing}</code> will be removed in NumPy 2.0. {migration_guide}] What it does Checks for uses of NumPy functions and constants that were removed from the main namespace in NumPy 2.0. Why is this bad? NumPy 2.0 includes an overhaul of NumPy's Python API, intended to remove redundant aliases and routines, and establish unambiguous mechanisms for accessing constants, dtypes, and functions. As part of this overhaul, a variety of deprecated NumPy functions and constants were removed from the main namespace. The majority of these functions and constants can be automatically replaced by other members of the NumPy API or by equivalents from the Python standard library. With the exception of renaming numpy.byte_bounds to numpy.lib.array_utils.byte_bounds, all such replacements are backwards compatible with earlier versions of NumPy. This rule flags all uses of removed members, along with automatic fixes for any backwards-compatible replacements. Example import numpy as np arr1 = [np.Infinity, np.NaN, np.nan, np.PINF, np.inf] arr2 = [np.float_(1.5), np.float64(5.1)] np.round_(arr2) Use instead: import numpy as np arr1 = [np.inf, np.nan, np.nan, np.inf, np.inf] arr2 = [np.float64(1.5), np.float64(5.1)] np.round(arr2)",
    "option": {}
  },
  "PD002": {
    "description": "[Status: stable] [Name: pandas-use-of-inplace-argument] [Message: <code>inplace=True</code> should be avoided; it has inconsistent behavior] What it does Checks for inplace=True usages in pandas function and method calls. Why is this bad? Using inplace=True encourages mutation rather than immutable data, which is harder to reason about and may cause bugs. It also removes the ability to use the method chaining style for pandas operations. Further, in many cases, inplace=True does not provide a performance benefit, as pandas will often copy DataFrames in the background. Example import pandas as pd students = pd.read_csv(\"students.csv\") students.sort_values(\"name\", inplace=True) Use instead: import pandas as pd students = pd.read_csv(\"students.csv\").sort_values(\"name\")",
    "option": {}
  },
  "PD003": {
    "description": "[Status: stable] [Name: pandas-use-of-dot-is-null] [Message: <code>.isna</code> is preferred to <code>.isnull</code>; functionality is equivalent] What it does Checks for uses of .isnull on Pandas objects. Why is this bad? In the Pandas API, .isna and .isnull are equivalent. For consistency, prefer .isna over .isnull. As a name, .isna more accurately reflects the behavior of the method, since these methods check for NaN and NaT values in addition to None values. Example import pandas as pd animals_df = pd.read_csv(\"animals.csv\") pd.isnull(animals_df) Use instead: import pandas as pd animals_df = pd.read_csv(\"animals.csv\") pd.isna(animals_df)",
    "option": {}
  },
  "PD004": {
    "description": "[Status: stable] [Name: pandas-use-of-dot-not-null] [Message: <code>.notna</code> is preferred to <code>.notnull</code>; functionality is equivalent] What it does Checks for uses of .notnull on Pandas objects. Why is this bad? In the Pandas API, .notna and .notnull are equivalent. For consistency, prefer .notna over .notnull. As a name, .notna more accurately reflects the behavior of the method, since these methods check for NaN and NaT values in addition to None values. Example import pandas as pd animals_df = pd.read_csv(\"animals.csv\") pd.notnull(animals_df) Use instead: import pandas as pd animals_df = pd.read_csv(\"animals.csv\") pd.notna(animals_df)",
    "option": {}
  },
  "PD007": {
    "description": "[Status: stable] [Name: pandas-use-of-dot-ix] [Message: <code>.ix</code> is deprecated; use more explicit <code>.loc</code> or <code>.iloc</code>] What it does Checks for uses of .ix on Pandas objects. Why is this bad? The .ix method is deprecated as its behavior is ambiguous. Specifically, it's often unclear whether .ix is indexing by label or by ordinal position. Instead, prefer the .loc method for label-based indexing, and .iloc for ordinal indexing. Example import pandas as pd students_df = pd.read_csv(\"students.csv\") students_df.ix[0] # 0th row or row with label 0? Use instead: import pandas as pd students_df = pd.read_csv(\"students.csv\") students_df.iloc[0] # 0th row.",
    "option": {}
  },
  "PD008": {
    "description": "[Status: stable] [Name: pandas-use-of-dot-at] [Message: Use <code>.loc</code> instead of <code>.at</code>. If speed is important, use NumPy.] What it does Checks for uses of .at on Pandas objects. Why is this bad? The .at method selects a single value from a DataFrame or Series based on a label index, and is slightly faster than using .loc. However, .loc is more idiomatic and versatile, as it can be used to select multiple values at once. If performance is an important consideration, convert the object to a NumPy array, which will provide a much greater performance boost than using .at over .loc. Example import pandas as pd students_df = pd.read_csv(\"students.csv\") students_df.at[\"Maria\"] Use instead: import pandas as pd students_df = pd.read_csv(\"students.csv\") students_df.loc[\"Maria\"]",
    "option": {}
  },
  "PD009": {
    "description": "[Status: stable] [Name: pandas-use-of-dot-iat] [Message: Use <code>.iloc</code> instead of <code>.iat</code>. If speed is important, use NumPy.] What it does Checks for uses of .iat on Pandas objects. Why is this bad? The .iat method selects a single value from a DataFrame or Series based on an ordinal index, and is slightly faster than using .iloc. However, .iloc is more idiomatic and versatile, as it can be used to select multiple values at once. If performance is an important consideration, convert the object to a NumPy array, which will provide a much greater performance boost than using .iat over .iloc. Example import pandas as pd students_df = pd.read_csv(\"students.csv\") students_df.iat[0] Use instead: import pandas as pd students_df = pd.read_csv(\"students.csv\") students_df.iloc[0] Or, using NumPy: import numpy as np import pandas as pd students_df = pd.read_csv(\"students.csv\") students_df.to_numpy()[0]",
    "option": {}
  },
  "PD010": {
    "description": "[Status: stable] [Name: pandas-use-of-dot-pivot-or-unstack] [Message: <code>.pivot_table</code> is preferred to <code>.pivot</code> or <code>.unstack</code>; provides same functionality] What it does Checks for uses of .pivot or .unstack on Pandas objects. Why is this bad? Prefer .pivot_table to .pivot or .unstack. .pivot_table is more general and can be used to implement the same behavior as .pivot and .unstack. Example import pandas as pd df = pd.read_csv(\"cities.csv\") df.pivot(index=\"city\", columns=\"year\", values=\"population\") Use instead: import pandas as pd df = pd.read_csv(\"cities.csv\") df.pivot_table(index=\"city\", columns=\"year\", values=\"population\")",
    "option": {}
  },
  "PD011": {
    "description": "[Status: stable] [Name: pandas-use-of-dot-values] [Message: Use <code>.to_numpy()</code> instead of <code>.values</code>] What it does Checks for uses of .values on Pandas Series and Index objects. Why is this bad? The .values attribute is ambiguous as its return type is unclear. As such, it is no longer recommended by the Pandas documentation. Instead, use .to_numpy() to return a NumPy array, or .array to return a Pandas ExtensionArray. Example import pandas as pd animals = pd.read_csv(\"animals.csv\").values # Ambiguous. Use instead: import pandas as pd animals = pd.read_csv(\"animals.csv\").to_numpy() # Explicit.",
    "option": {}
  },
  "PD012": {
    "description": "[Status: stable] [Name: pandas-use-of-dot-read-table] [Message: Use <code>.read_csv</code> instead of <code>.read_table</code> to read CSV files] What it does Checks for uses of pd.read_table to read CSV files. Why is this bad? In the Pandas API, pd.read_csv and pd.read_table are equivalent apart from their default separator: pd.read_csv defaults to a comma (,), while pd.read_table defaults to a tab (\\t) as the default separator. Prefer pd.read_csv over pd.read_table when reading comma-separated data (like CSV files), as it is more idiomatic. Example import pandas as pd cities_df = pd.read_table(\"cities.csv\", sep=\",\") Use instead: import pandas as pd cities_df = pd.read_csv(\"cities.csv\")",
    "option": {}
  },
  "PD013": {
    "description": "[Status: stable] [Name: pandas-use-of-dot-stack] [Message: <code>.melt</code> is preferred to <code>.stack</code>; provides same functionality] What it does Checks for uses of .stack on Pandas objects. Why is this bad? Prefer .melt to .stack, which has the same functionality but with support for direct column renaming and no dependence on MultiIndex. Example import pandas as pd cities_df = pd.read_csv(\"cities.csv\") cities_df.set_index(\"city\").stack() Use instead: import pandas as pd cities_df = pd.read_csv(\"cities.csv\") cities_df.melt(id_vars=\"city\")",
    "option": {}
  },
  "PD015": {
    "description": "[Status: stable] [Name: pandas-use-of-pd-merge] [Message: Use <code>.merge</code> method instead of <code>pd.merge</code> function. They have equivalent functionality.] What it does Checks for uses of pd.merge on Pandas objects. Why is this bad? In Pandas, the .merge method (exposed on, e.g., DataFrame objects) and the pd.merge function (exposed on the Pandas module) are equivalent. For consistency, prefer calling .merge on an object over calling pd.merge on the Pandas module, as the former is more idiomatic. Further, pd.merge is not a method, but a function, which prohibits it from being used in method chains, a common pattern in Pandas code. Example import pandas as pd cats_df = pd.read_csv(\"cats.csv\") dogs_df = pd.read_csv(\"dogs.csv\") rabbits_df = pd.read_csv(\"rabbits.csv\") pets_df = pd.merge(pd.merge(cats_df, dogs_df), rabbits_df) # Hard to read. Use instead: import pandas as pd cats_df = pd.read_csv(\"cats.csv\") dogs_df = pd.read_csv(\"dogs.csv\") rabbits_df = pd.read_csv(\"rabbits.csv\") pets_df = cats_df.merge(dogs_df).merge(rabbits_df)",
    "option": {}
  },
  "PD101": {
    "description": "[Status: stable] [Name: pandas-nunique-constant-series-check] [Message: Using <code>series.nunique()</code> for checking that a series is constant is inefficient] What it does Check for uses of .nunique() to check if a Pandas Series is constant (i.e., contains only one unique value). Why is this bad? .nunique() is computationally inefficient for checking if a Series is constant. Consider, for example, a Series of length n that consists of increasing integer values (e.g., 1, 2, 3, 4). The .nunique() method will iterate over the entire Series to count the number of unique values. But in this case, we can detect that the Series is non-constant after visiting the first two values, which are non-equal. In general, .nunique() requires iterating over the entire Series, while a more efficient approach allows short-circuiting the operation as soon as a non-equal value is found. Instead of calling .nunique(), convert the Series to a NumPy array, and check if all values in the array are equal to the first observed value. Example import pandas as pd data = pd.Series(range(1000)) if data.nunique() <= 1: print(\"Series is constant\") Use instead: import pandas as pd data = pd.Series(range(1000)) array = data.to_numpy() if array.shape[0] == 0 or (array[0] == array).all(): print(\"Series is constant\")",
    "option": {}
  },
  "N801": {
    "description": "[Status: stable] [Name: invalid-class-name] [Message: Class name <code>{name}</code> should use CapWords convention] What it does Checks for class names that do not follow the CamelCase convention. Why is this bad? PEP 8 recommends the use of the CapWords (or CamelCase) convention for class names: Class names should normally use the CapWords convention. The naming convention for functions may be used instead in cases where the interface is documented and used primarily as a callable. Note that there is a separate convention for builtin names: most builtin names are single words (or two words run together), with the CapWords convention used only for exception names and builtin constants. Example class my_class: pass Use instead: class MyClass: pass",
    "option": {
      "lint.pep8-naming.ignore-names": {
        "option_description": "A list of names (or patterns) to ignore when considering pep8-naming violations.\n\nSupports glob patterns. For example, to ignore all names starting with test_\nor ending with _test, you could use ignore-names = [\"test_*\", \"*_test\"].\nFor more information on the glob syntax, refer to the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nignore-names = [\"callMethod\"]\n\n\n\n[lint.pep8-naming]\nignore-names = [\"callMethod\"]",
        "option_data_type": "list[str]",
        "default_value": "[\"setUp\", \"tearDown\", \"setUpClass\", \"tearDownClass\", \"setUpModule\", \"tearDownModule\", \"asyncSetUp\", \"asyncTearDown\", \"setUpTestData\", \"failureException\", \"longMessage\", \"maxDiff\"]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pep8-naming.extend-ignore-names": {
        "option_description": "Additional names (or patterns) to ignore when considering pep8-naming violations,\nin addition to those included in ignore-names.\n\nSupports glob patterns. For example, to ignore all names starting with test_\nor ending with _test, you could use ignore-names = [\"test_*\", \"*_test\"].\nFor more information on the glob syntax, refer to the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nextend-ignore-names = [\"callMethod\"]\n\n\n\n[lint.pep8-naming]\nextend-ignore-names = [\"callMethod\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "N802": {
    "description": "[Status: stable] [Name: invalid-function-name] [Message: Function name <code>{name}</code> should be lowercase] What it does Checks for functions names that do not follow the snake_case naming convention. Why is this bad? PEP 8 recommends that function names follow snake_case: Function names should be lowercase, with words separated by underscores as necessary to improve readability. mixedCase is allowed only in contexts where thats already the prevailing style (e.g. threading.py), to retain backwards compatibility. Names can be excluded from this rule using the lint.pep8-naming.ignore-names or lint.pep8-naming.extend-ignore-names configuration options. For example, to ignore all functions starting with test_ from this rule, set the lint.pep8-naming.extend-ignore-names option to [\"test_*\"]. This rule exempts methods decorated with @typing.override. Explicitly decorating a method with @override signals to Ruff that the method is intended to override a superclass method, and that a type checker will enforce that it does so. Ruff therefore knows that it should not enforce naming conventions on such methods. Example def myFunction(): pass Use instead: def my_function(): pass",
    "option": {
      "lint.pep8-naming.ignore-names": {
        "option_description": "A list of names (or patterns) to ignore when considering pep8-naming violations.\n\nSupports glob patterns. For example, to ignore all names starting with test_\nor ending with _test, you could use ignore-names = [\"test_*\", \"*_test\"].\nFor more information on the glob syntax, refer to the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nignore-names = [\"callMethod\"]\n\n\n\n[lint.pep8-naming]\nignore-names = [\"callMethod\"]",
        "option_data_type": "list[str]",
        "default_value": "[\"setUp\", \"tearDown\", \"setUpClass\", \"tearDownClass\", \"setUpModule\", \"tearDownModule\", \"asyncSetUp\", \"asyncTearDown\", \"setUpTestData\", \"failureException\", \"longMessage\", \"maxDiff\"]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pep8-naming.extend-ignore-names": {
        "option_description": "Additional names (or patterns) to ignore when considering pep8-naming violations,\nin addition to those included in ignore-names.\n\nSupports glob patterns. For example, to ignore all names starting with test_\nor ending with _test, you could use ignore-names = [\"test_*\", \"*_test\"].\nFor more information on the glob syntax, refer to the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nextend-ignore-names = [\"callMethod\"]\n\n\n\n[lint.pep8-naming]\nextend-ignore-names = [\"callMethod\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "N803": {
    "description": "[Status: stable] [Name: invalid-argument-name] [Message: Argument name <code>{name}</code> should be lowercase] What it does Checks for argument names that do not follow the snake_case convention. Why is this bad? PEP 8 recommends that function names should be lower case and separated by underscores (also known as snake_case). Function names should be lowercase, with words separated by underscores as necessary to improve readability. Variable names follow the same convention as function names. mixedCase is allowed only in contexts where thats already the prevailing style (e.g. threading.py), to retain backwards compatibility. Methods decorated with @typing.override are ignored. Example def my_function(A, myArg): pass Use instead: def my_function(a, my_arg): pass",
    "option": {
      "lint.pep8-naming.ignore-names": {
        "option_description": "A list of names (or patterns) to ignore when considering pep8-naming violations.\n\nSupports glob patterns. For example, to ignore all names starting with test_\nor ending with _test, you could use ignore-names = [\"test_*\", \"*_test\"].\nFor more information on the glob syntax, refer to the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nignore-names = [\"callMethod\"]\n\n\n\n[lint.pep8-naming]\nignore-names = [\"callMethod\"]",
        "option_data_type": "list[str]",
        "default_value": "[\"setUp\", \"tearDown\", \"setUpClass\", \"tearDownClass\", \"setUpModule\", \"tearDownModule\", \"asyncSetUp\", \"asyncTearDown\", \"setUpTestData\", \"failureException\", \"longMessage\", \"maxDiff\"]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pep8-naming.extend-ignore-names": {
        "option_description": "Additional names (or patterns) to ignore when considering pep8-naming violations,\nin addition to those included in ignore-names.\n\nSupports glob patterns. For example, to ignore all names starting with test_\nor ending with _test, you could use ignore-names = [\"test_*\", \"*_test\"].\nFor more information on the glob syntax, refer to the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nextend-ignore-names = [\"callMethod\"]\n\n\n\n[lint.pep8-naming]\nextend-ignore-names = [\"callMethod\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "N804": {
    "description": "[Status: stable] [Name: invalid-first-argument-name-for-class-method] [Message: First argument of a class method should be named <code>cls</code>] What it does Checks for class methods that use a name other than cls for their first argument. The method __new__ is exempted from this check and the corresponding violation is caught by bad-staticmethod-argument. Why is this bad? PEP 8 recommends the use of cls as the first argument for all class methods: Always use cls for the first argument to class methods. If a function arguments name clashes with a reserved keyword, it is generally better to append a single trailing underscore rather than use an abbreviation or spelling corruption. Thus class_ is better than clss. (Perhaps better is to avoid such clashes by using a synonym.) Names can be excluded from this rule using the lint.pep8-naming.ignore-names or lint.pep8-naming.extend-ignore-names configuration options. For example, to allow the use of klass as the first argument to class methods, set the lint.pep8-naming.extend-ignore-names option to [\"klass\"]. Example class Example: @classmethod def function(self, data): ... Use instead: class Example: @classmethod def function(cls, data): ... Fix safety This rule's fix is marked as unsafe, as renaming a method parameter can change the behavior of the program.",
    "option": {
      "lint.pep8-naming.classmethod-decorators": {
        "option_description": "A list of decorators that, when applied to a method, indicate that the\nmethod should be treated as a class method (in addition to the builtin\n@classmethod).\n\nFor example, Ruff will expect that any method decorated by a decorator\nin this list takes a cls argument as its first argument.\n\nExpects to receive a list of fully-qualified names (e.g., pydantic.validator,\nrather than validator) or alternatively a plain name which is then matched against\nthe last segment in case the decorator itself consists of a dotted name.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nclassmethod-decorators = [\n    # Allow Pydantic's `@validator` decorator to trigger class method treatment.\n    \"pydantic.validator\",\n    # Allow SQLAlchemy's dynamic decorators, like `@field.expression`, to trigger class method treatment.\n    \"declared_attr\",\n    \"expression\",\n    \"comparator\",\n]\n\n\n\n[lint.pep8-naming]\nclassmethod-decorators = [\n    # Allow Pydantic's `@validator` decorator to trigger class method treatment.\n    \"pydantic.validator\",\n    # Allow SQLAlchemy's dynamic decorators, like `@field.expression`, to trigger class method treatment.\n    \"declared_attr\",\n    \"expression\",\n    \"comparator\",\n]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pep8-naming.staticmethod-decorators": {
        "option_description": "A list of decorators that, when applied to a method, indicate that the\nmethod should be treated as a static method (in addition to the builtin\n@staticmethod).\n\nFor example, Ruff will expect that any method decorated by a decorator\nin this list has no self or cls argument.\n\nExpects to receive a list of fully-qualified names (e.g., belay.Device.teardown,\nrather than teardown) or alternatively a plain name which is then matched against\nthe last segment in case the decorator itself consists of a dotted name.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\n# Allow Belay's `@Device.teardown` decorator to trigger static method treatment.\nstaticmethod-decorators = [\"belay.Device.teardown\"]\n\n\n\n[lint.pep8-naming]\n# Allow Belay's `@Device.teardown` decorator to trigger static method treatment.\nstaticmethod-decorators = [\"belay.Device.teardown\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pep8-naming.ignore-names": {
        "option_description": "A list of names (or patterns) to ignore when considering pep8-naming violations.\n\nSupports glob patterns. For example, to ignore all names starting with test_\nor ending with _test, you could use ignore-names = [\"test_*\", \"*_test\"].\nFor more information on the glob syntax, refer to the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nignore-names = [\"callMethod\"]\n\n\n\n[lint.pep8-naming]\nignore-names = [\"callMethod\"]",
        "option_data_type": "list[str]",
        "default_value": "[\"setUp\", \"tearDown\", \"setUpClass\", \"tearDownClass\", \"setUpModule\", \"tearDownModule\", \"asyncSetUp\", \"asyncTearDown\", \"setUpTestData\", \"failureException\", \"longMessage\", \"maxDiff\"]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pep8-naming.extend-ignore-names": {
        "option_description": "Additional names (or patterns) to ignore when considering pep8-naming violations,\nin addition to those included in ignore-names.\n\nSupports glob patterns. For example, to ignore all names starting with test_\nor ending with _test, you could use ignore-names = [\"test_*\", \"*_test\"].\nFor more information on the glob syntax, refer to the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nextend-ignore-names = [\"callMethod\"]\n\n\n\n[lint.pep8-naming]\nextend-ignore-names = [\"callMethod\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "N805": {
    "description": "[Status: stable] [Name: invalid-first-argument-name-for-method] [Message: First argument of a method should be named <code>self</code>] What it does Checks for instance methods that use a name other than self for their first argument. Why is this bad? PEP 8 recommends the use of self as first argument for all instance methods: Always use self for the first argument to instance methods. If a function arguments name clashes with a reserved keyword, it is generally better to append a single trailing underscore rather than use an abbreviation or spelling corruption. Thus class_ is better than clss. (Perhaps better is to avoid such clashes by using a synonym.) Names can be excluded from this rule using the lint.pep8-naming.ignore-names or lint.pep8-naming.extend-ignore-names configuration options. For example, to allow the use of this as the first argument to instance methods, set the lint.pep8-naming.extend-ignore-names option to [\"this\"]. Example class Example: def function(cls, data): ... Use instead: class Example: def function(self, data): ... Fix safety This rule's fix is marked as unsafe, as renaming a method parameter can change the behavior of the program.",
    "option": {
      "lint.pep8-naming.classmethod-decorators": {
        "option_description": "A list of decorators that, when applied to a method, indicate that the\nmethod should be treated as a class method (in addition to the builtin\n@classmethod).\n\nFor example, Ruff will expect that any method decorated by a decorator\nin this list takes a cls argument as its first argument.\n\nExpects to receive a list of fully-qualified names (e.g., pydantic.validator,\nrather than validator) or alternatively a plain name which is then matched against\nthe last segment in case the decorator itself consists of a dotted name.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nclassmethod-decorators = [\n    # Allow Pydantic's `@validator` decorator to trigger class method treatment.\n    \"pydantic.validator\",\n    # Allow SQLAlchemy's dynamic decorators, like `@field.expression`, to trigger class method treatment.\n    \"declared_attr\",\n    \"expression\",\n    \"comparator\",\n]\n\n\n\n[lint.pep8-naming]\nclassmethod-decorators = [\n    # Allow Pydantic's `@validator` decorator to trigger class method treatment.\n    \"pydantic.validator\",\n    # Allow SQLAlchemy's dynamic decorators, like `@field.expression`, to trigger class method treatment.\n    \"declared_attr\",\n    \"expression\",\n    \"comparator\",\n]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pep8-naming.staticmethod-decorators": {
        "option_description": "A list of decorators that, when applied to a method, indicate that the\nmethod should be treated as a static method (in addition to the builtin\n@staticmethod).\n\nFor example, Ruff will expect that any method decorated by a decorator\nin this list has no self or cls argument.\n\nExpects to receive a list of fully-qualified names (e.g., belay.Device.teardown,\nrather than teardown) or alternatively a plain name which is then matched against\nthe last segment in case the decorator itself consists of a dotted name.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\n# Allow Belay's `@Device.teardown` decorator to trigger static method treatment.\nstaticmethod-decorators = [\"belay.Device.teardown\"]\n\n\n\n[lint.pep8-naming]\n# Allow Belay's `@Device.teardown` decorator to trigger static method treatment.\nstaticmethod-decorators = [\"belay.Device.teardown\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pep8-naming.ignore-names": {
        "option_description": "A list of names (or patterns) to ignore when considering pep8-naming violations.\n\nSupports glob patterns. For example, to ignore all names starting with test_\nor ending with _test, you could use ignore-names = [\"test_*\", \"*_test\"].\nFor more information on the glob syntax, refer to the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nignore-names = [\"callMethod\"]\n\n\n\n[lint.pep8-naming]\nignore-names = [\"callMethod\"]",
        "option_data_type": "list[str]",
        "default_value": "[\"setUp\", \"tearDown\", \"setUpClass\", \"tearDownClass\", \"setUpModule\", \"tearDownModule\", \"asyncSetUp\", \"asyncTearDown\", \"setUpTestData\", \"failureException\", \"longMessage\", \"maxDiff\"]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pep8-naming.extend-ignore-names": {
        "option_description": "Additional names (or patterns) to ignore when considering pep8-naming violations,\nin addition to those included in ignore-names.\n\nSupports glob patterns. For example, to ignore all names starting with test_\nor ending with _test, you could use ignore-names = [\"test_*\", \"*_test\"].\nFor more information on the glob syntax, refer to the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nextend-ignore-names = [\"callMethod\"]\n\n\n\n[lint.pep8-naming]\nextend-ignore-names = [\"callMethod\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "N806": {
    "description": "[Status: stable] [Name: non-lowercase-variable-in-function] [Message: Variable <code>{name}</code> in function should be lowercase] What it does Checks for the use of non-lowercase variable names in functions. Why is this bad? PEP 8 recommends that all function variables use lowercase names: Function names should be lowercase, with words separated by underscores as necessary to improve readability. Variable names follow the same convention as function names. mixedCase is allowed only in contexts where that's already the prevailing style (e.g. threading.py), to retain backwards compatibility. Example def my_function(a): B = a + 3 return B Use instead: def my_function(a): b = a + 3 return b",
    "option": {
      "lint.pep8-naming.ignore-names": {
        "option_description": "A list of names (or patterns) to ignore when considering pep8-naming violations.\n\nSupports glob patterns. For example, to ignore all names starting with test_\nor ending with _test, you could use ignore-names = [\"test_*\", \"*_test\"].\nFor more information on the glob syntax, refer to the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nignore-names = [\"callMethod\"]\n\n\n\n[lint.pep8-naming]\nignore-names = [\"callMethod\"]",
        "option_data_type": "list[str]",
        "default_value": "[\"setUp\", \"tearDown\", \"setUpClass\", \"tearDownClass\", \"setUpModule\", \"tearDownModule\", \"asyncSetUp\", \"asyncTearDown\", \"setUpTestData\", \"failureException\", \"longMessage\", \"maxDiff\"]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pep8-naming.extend-ignore-names": {
        "option_description": "Additional names (or patterns) to ignore when considering pep8-naming violations,\nin addition to those included in ignore-names.\n\nSupports glob patterns. For example, to ignore all names starting with test_\nor ending with _test, you could use ignore-names = [\"test_*\", \"*_test\"].\nFor more information on the glob syntax, refer to the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nextend-ignore-names = [\"callMethod\"]\n\n\n\n[lint.pep8-naming]\nextend-ignore-names = [\"callMethod\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "N807": {
    "description": "[Status: stable] [Name: dunder-function-name] [Message: Function name should not start and end with <code>__</code>] What it does Checks for functions with \"dunder\" names (that is, names with two leading and trailing underscores) that are not documented. Why is this bad? PEP 8 recommends that only documented \"dunder\" methods are used: ...\"magic\" objects or attributes that live in user-controlled namespaces. E.g. __init__, __import__ or __file__. Never invent such names; only use them as documented. Example def __my_function__(): pass Use instead: def my_function(): pass",
    "option": {
      "lint.pep8-naming.ignore-names": {
        "option_description": "A list of names (or patterns) to ignore when considering pep8-naming violations.\n\nSupports glob patterns. For example, to ignore all names starting with test_\nor ending with _test, you could use ignore-names = [\"test_*\", \"*_test\"].\nFor more information on the glob syntax, refer to the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nignore-names = [\"callMethod\"]\n\n\n\n[lint.pep8-naming]\nignore-names = [\"callMethod\"]",
        "option_data_type": "list[str]",
        "default_value": "[\"setUp\", \"tearDown\", \"setUpClass\", \"tearDownClass\", \"setUpModule\", \"tearDownModule\", \"asyncSetUp\", \"asyncTearDown\", \"setUpTestData\", \"failureException\", \"longMessage\", \"maxDiff\"]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pep8-naming.extend-ignore-names": {
        "option_description": "Additional names (or patterns) to ignore when considering pep8-naming violations,\nin addition to those included in ignore-names.\n\nSupports glob patterns. For example, to ignore all names starting with test_\nor ending with _test, you could use ignore-names = [\"test_*\", \"*_test\"].\nFor more information on the glob syntax, refer to the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nextend-ignore-names = [\"callMethod\"]\n\n\n\n[lint.pep8-naming]\nextend-ignore-names = [\"callMethod\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "N811": {
    "description": "[Status: stable] [Name: constant-imported-as-non-constant] [Message: Constant <code>{name}</code> imported as non-constant <code>{asname}</code>] What it does Checks for constant imports that are aliased to non-constant-style names. Why is this bad? PEP 8 recommends naming conventions for classes, functions, constants, and more. The use of inconsistent naming styles between import and alias names may lead readers to expect an import to be of another type (e.g., confuse a Python class with a constant). Import aliases should thus follow the same naming style as the member being imported. Example from example import CONSTANT_VALUE as ConstantValue Use instead: from example import CONSTANT_VALUE Note Identifiers consisting of a single uppercase character are ambiguous under the rules of PEP 8, which specifies CamelCase for classes and ALL_CAPS_SNAKE_CASE for constants. Without a second character, it is not possible to reliably guess whether the identifier is intended to be part of a CamelCase string for a class or an ALL_CAPS_SNAKE_CASE string for a constant, since both conventions will produce the same output when given a single input character. Therefore, this lint rule does not apply to cases where the imported identifier consists of a single uppercase character. A common example of a single uppercase character being used for a class name can be found in Django's django.db.models.Q class.",
    "option": {
      "lint.pep8-naming.ignore-names": {
        "option_description": "A list of names (or patterns) to ignore when considering pep8-naming violations.\n\nSupports glob patterns. For example, to ignore all names starting with test_\nor ending with _test, you could use ignore-names = [\"test_*\", \"*_test\"].\nFor more information on the glob syntax, refer to the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nignore-names = [\"callMethod\"]\n\n\n\n[lint.pep8-naming]\nignore-names = [\"callMethod\"]",
        "option_data_type": "list[str]",
        "default_value": "[\"setUp\", \"tearDown\", \"setUpClass\", \"tearDownClass\", \"setUpModule\", \"tearDownModule\", \"asyncSetUp\", \"asyncTearDown\", \"setUpTestData\", \"failureException\", \"longMessage\", \"maxDiff\"]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pep8-naming.extend-ignore-names": {
        "option_description": "Additional names (or patterns) to ignore when considering pep8-naming violations,\nin addition to those included in ignore-names.\n\nSupports glob patterns. For example, to ignore all names starting with test_\nor ending with _test, you could use ignore-names = [\"test_*\", \"*_test\"].\nFor more information on the glob syntax, refer to the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nextend-ignore-names = [\"callMethod\"]\n\n\n\n[lint.pep8-naming]\nextend-ignore-names = [\"callMethod\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "N812": {
    "description": "[Status: stable] [Name: lowercase-imported-as-non-lowercase] [Message: Lowercase <code>{name}</code> imported as non-lowercase <code>{asname}</code>] What it does Checks for lowercase imports that are aliased to non-lowercase names. Why is this bad? PEP 8 recommends naming conventions for classes, functions, constants, and more. The use of inconsistent naming styles between import and alias names may lead readers to expect an import to be of another type (e.g., confuse a Python class with a constant). Import aliases should thus follow the same naming style as the member being imported. Example from example import myclassname as MyClassName Use instead: from example import myclassname",
    "option": {
      "lint.pep8-naming.ignore-names": {
        "option_description": "A list of names (or patterns) to ignore when considering pep8-naming violations.\n\nSupports glob patterns. For example, to ignore all names starting with test_\nor ending with _test, you could use ignore-names = [\"test_*\", \"*_test\"].\nFor more information on the glob syntax, refer to the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nignore-names = [\"callMethod\"]\n\n\n\n[lint.pep8-naming]\nignore-names = [\"callMethod\"]",
        "option_data_type": "list[str]",
        "default_value": "[\"setUp\", \"tearDown\", \"setUpClass\", \"tearDownClass\", \"setUpModule\", \"tearDownModule\", \"asyncSetUp\", \"asyncTearDown\", \"setUpTestData\", \"failureException\", \"longMessage\", \"maxDiff\"]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pep8-naming.extend-ignore-names": {
        "option_description": "Additional names (or patterns) to ignore when considering pep8-naming violations,\nin addition to those included in ignore-names.\n\nSupports glob patterns. For example, to ignore all names starting with test_\nor ending with _test, you could use ignore-names = [\"test_*\", \"*_test\"].\nFor more information on the glob syntax, refer to the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nextend-ignore-names = [\"callMethod\"]\n\n\n\n[lint.pep8-naming]\nextend-ignore-names = [\"callMethod\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "N813": {
    "description": "[Status: stable] [Name: camelcase-imported-as-lowercase] [Message: Camelcase <code>{name}</code> imported as lowercase <code>{asname}</code>] What it does Checks for CamelCase imports that are aliased to lowercase names. Why is this bad? PEP 8 recommends naming conventions for classes, functions, constants, and more. The use of inconsistent naming styles between import and alias names may lead readers to expect an import to be of another type (e.g., confuse a Python class with a constant). Import aliases should thus follow the same naming style as the member being imported. Example from example import MyClassName as myclassname Use instead: from example import MyClassName",
    "option": {
      "lint.pep8-naming.ignore-names": {
        "option_description": "A list of names (or patterns) to ignore when considering pep8-naming violations.\n\nSupports glob patterns. For example, to ignore all names starting with test_\nor ending with _test, you could use ignore-names = [\"test_*\", \"*_test\"].\nFor more information on the glob syntax, refer to the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nignore-names = [\"callMethod\"]\n\n\n\n[lint.pep8-naming]\nignore-names = [\"callMethod\"]",
        "option_data_type": "list[str]",
        "default_value": "[\"setUp\", \"tearDown\", \"setUpClass\", \"tearDownClass\", \"setUpModule\", \"tearDownModule\", \"asyncSetUp\", \"asyncTearDown\", \"setUpTestData\", \"failureException\", \"longMessage\", \"maxDiff\"]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pep8-naming.extend-ignore-names": {
        "option_description": "Additional names (or patterns) to ignore when considering pep8-naming violations,\nin addition to those included in ignore-names.\n\nSupports glob patterns. For example, to ignore all names starting with test_\nor ending with _test, you could use ignore-names = [\"test_*\", \"*_test\"].\nFor more information on the glob syntax, refer to the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nextend-ignore-names = [\"callMethod\"]\n\n\n\n[lint.pep8-naming]\nextend-ignore-names = [\"callMethod\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "N814": {
    "description": "[Status: stable] [Name: camelcase-imported-as-constant] [Message: Camelcase <code>{name}</code> imported as constant <code>{asname}</code>] What it does Checks for CamelCase imports that are aliased to constant-style names. Why is this bad? PEP 8 recommends naming conventions for classes, functions, constants, and more. The use of inconsistent naming styles between import and alias names may lead readers to expect an import to be of another type (e.g., confuse a Python class with a constant). Import aliases should thus follow the same naming style as the member being imported. Example from example import MyClassName as MY_CLASS_NAME Use instead: from example import MyClassName Note Identifiers consisting of a single uppercase character are ambiguous under the rules of PEP 8, which specifies CamelCase for classes and ALL_CAPS_SNAKE_CASE for constants. Without a second character, it is not possible to reliably guess whether the identifier is intended to be part of a CamelCase string for a class or an ALL_CAPS_SNAKE_CASE string for a constant, since both conventions will produce the same output when given a single input character. Therefore, this lint rule does not apply to cases where the alias for the imported identifier consists of a single uppercase character. A common example of a single uppercase character being used for a class name can be found in Django's django.db.models.Q class.",
    "option": {
      "lint.pep8-naming.ignore-names": {
        "option_description": "A list of names (or patterns) to ignore when considering pep8-naming violations.\n\nSupports glob patterns. For example, to ignore all names starting with test_\nor ending with _test, you could use ignore-names = [\"test_*\", \"*_test\"].\nFor more information on the glob syntax, refer to the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nignore-names = [\"callMethod\"]\n\n\n\n[lint.pep8-naming]\nignore-names = [\"callMethod\"]",
        "option_data_type": "list[str]",
        "default_value": "[\"setUp\", \"tearDown\", \"setUpClass\", \"tearDownClass\", \"setUpModule\", \"tearDownModule\", \"asyncSetUp\", \"asyncTearDown\", \"setUpTestData\", \"failureException\", \"longMessage\", \"maxDiff\"]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pep8-naming.extend-ignore-names": {
        "option_description": "Additional names (or patterns) to ignore when considering pep8-naming violations,\nin addition to those included in ignore-names.\n\nSupports glob patterns. For example, to ignore all names starting with test_\nor ending with _test, you could use ignore-names = [\"test_*\", \"*_test\"].\nFor more information on the glob syntax, refer to the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nextend-ignore-names = [\"callMethod\"]\n\n\n\n[lint.pep8-naming]\nextend-ignore-names = [\"callMethod\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "N815": {
    "description": "[Status: stable] [Name: mixed-case-variable-in-class-scope] [Message: Variable <code>{name}</code> in class scope should not be mixedCase] What it does Checks for class variable names that follow the mixedCase convention. Why is this bad? PEP 8 recommends that variable names should be lower case and separated by underscores (also known as snake_case). Function names should be lowercase, with words separated by underscores as necessary to improve readability. Variable names follow the same convention as function names. mixedCase is allowed only in contexts where thats already the prevailing style (e.g. threading.py), to retain backwards compatibility. Example class MyClass: myVariable = \"hello\" another_variable = \"world\" Use instead: class MyClass: my_variable = \"hello\" another_variable = \"world\"",
    "option": {
      "lint.pep8-naming.ignore-names": {
        "option_description": "A list of names (or patterns) to ignore when considering pep8-naming violations.\n\nSupports glob patterns. For example, to ignore all names starting with test_\nor ending with _test, you could use ignore-names = [\"test_*\", \"*_test\"].\nFor more information on the glob syntax, refer to the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nignore-names = [\"callMethod\"]\n\n\n\n[lint.pep8-naming]\nignore-names = [\"callMethod\"]",
        "option_data_type": "list[str]",
        "default_value": "[\"setUp\", \"tearDown\", \"setUpClass\", \"tearDownClass\", \"setUpModule\", \"tearDownModule\", \"asyncSetUp\", \"asyncTearDown\", \"setUpTestData\", \"failureException\", \"longMessage\", \"maxDiff\"]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pep8-naming.extend-ignore-names": {
        "option_description": "Additional names (or patterns) to ignore when considering pep8-naming violations,\nin addition to those included in ignore-names.\n\nSupports glob patterns. For example, to ignore all names starting with test_\nor ending with _test, you could use ignore-names = [\"test_*\", \"*_test\"].\nFor more information on the glob syntax, refer to the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nextend-ignore-names = [\"callMethod\"]\n\n\n\n[lint.pep8-naming]\nextend-ignore-names = [\"callMethod\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "N816": {
    "description": "[Status: stable] [Name: mixed-case-variable-in-global-scope] [Message: Variable <code>{name}</code> in global scope should not be mixedCase] What it does Checks for global variable names that follow the mixedCase convention. Why is this bad? PEP 8 recommends that global variable names should be lower case and separated by underscores (also known as snake_case). Global Variable Names (Lets hope that these variables are meant for use inside one module only.) The conventions are about the same as those for functions. Modules that are designed for use via from M import * should use the __all__ mechanism to prevent exporting globals, or use the older convention of prefixing such globals with an underscore (which you might want to do to indicate these globals are module non-public). Function and Variable Names Function names should be lowercase, with words separated by underscores as necessary to improve readability. Variable names follow the same convention as function names. mixedCase is allowed only in contexts where thats already the prevailing style (e.g. threading.py), to retain backwards compatibility. Example myVariable = \"hello\" another_variable = \"world\" yet_anotherVariable = \"foo\" Use instead: my_variable = \"hello\" another_variable = \"world\" yet_another_variable = \"foo\"",
    "option": {
      "lint.pep8-naming.ignore-names": {
        "option_description": "A list of names (or patterns) to ignore when considering pep8-naming violations.\n\nSupports glob patterns. For example, to ignore all names starting with test_\nor ending with _test, you could use ignore-names = [\"test_*\", \"*_test\"].\nFor more information on the glob syntax, refer to the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nignore-names = [\"callMethod\"]\n\n\n\n[lint.pep8-naming]\nignore-names = [\"callMethod\"]",
        "option_data_type": "list[str]",
        "default_value": "[\"setUp\", \"tearDown\", \"setUpClass\", \"tearDownClass\", \"setUpModule\", \"tearDownModule\", \"asyncSetUp\", \"asyncTearDown\", \"setUpTestData\", \"failureException\", \"longMessage\", \"maxDiff\"]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pep8-naming.extend-ignore-names": {
        "option_description": "Additional names (or patterns) to ignore when considering pep8-naming violations,\nin addition to those included in ignore-names.\n\nSupports glob patterns. For example, to ignore all names starting with test_\nor ending with _test, you could use ignore-names = [\"test_*\", \"*_test\"].\nFor more information on the glob syntax, refer to the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nextend-ignore-names = [\"callMethod\"]\n\n\n\n[lint.pep8-naming]\nextend-ignore-names = [\"callMethod\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "N817": {
    "description": "[Status: stable] [Name: camelcase-imported-as-acronym] [Message: CamelCase <code>{name}</code> imported as acronym <code>{asname}</code>] What it does Checks for CamelCase imports that are aliased as acronyms. Why is this bad? PEP 8 recommends naming conventions for classes, functions, constants, and more. The use of inconsistent naming styles between import and alias names may lead readers to expect an import to be of another type (e.g., confuse a Python class with a constant). Import aliases should thus follow the same naming style as the member being imported. Note that this rule is distinct from camelcase-imported-as-constant to accommodate selective enforcement. Also note that import aliases following an import convention according to the lint.flake8-import-conventions.aliases option are allowed. Example from example import MyClassName as MCN Use instead: from example import MyClassName",
    "option": {
      "lint.flake8-import-conventions.aliases": {
        "option_description": "The conventional aliases for imports. These aliases can be extended by\nthe extend-aliases option.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-import-conventions.aliases]\n# Declare the default aliases.\naltair = \"alt\"\n\"matplotlib.pyplot\" = \"plt\"\nnumpy = \"np\"\npandas = \"pd\"\nseaborn = \"sns\"\nscipy = \"sp\"\n\n\n\n[lint.flake8-import-conventions.aliases]\n# Declare the default aliases.\naltair = \"alt\"\n\"matplotlib.pyplot\" = \"plt\"\nnumpy = \"np\"\npandas = \"pd\"\nseaborn = \"sns\"\nscipy = \"sp\"",
        "option_data_type": "dict[str, str]",
        "default_value": "{\"altair\": \"alt\", \"matplotlib\": \"mpl\", \"matplotlib.pyplot\": \"plt\", \"numpy\": \"np\", \"numpy.typing\": \"npt\", \"pandas\": \"pd\", \"seaborn\": \"sns\", \"tensorflow\": \"tf\", \"tkinter\":  \"tk\", \"holoviews\": \"hv\", \"panel\": \"pn\", \"plotly.express\": \"px\", \"polars\": \"pl\", \"pyarrow\": \"pa\", \"xml.etree.ElementTree\": \"ET\"}",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pep8-naming.ignore-names": {
        "option_description": "A list of names (or patterns) to ignore when considering pep8-naming violations.\n\nSupports glob patterns. For example, to ignore all names starting with test_\nor ending with _test, you could use ignore-names = [\"test_*\", \"*_test\"].\nFor more information on the glob syntax, refer to the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nignore-names = [\"callMethod\"]\n\n\n\n[lint.pep8-naming]\nignore-names = [\"callMethod\"]",
        "option_data_type": "list[str]",
        "default_value": "[\"setUp\", \"tearDown\", \"setUpClass\", \"tearDownClass\", \"setUpModule\", \"tearDownModule\", \"asyncSetUp\", \"asyncTearDown\", \"setUpTestData\", \"failureException\", \"longMessage\", \"maxDiff\"]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pep8-naming.extend-ignore-names": {
        "option_description": "Additional names (or patterns) to ignore when considering pep8-naming violations,\nin addition to those included in ignore-names.\n\nSupports glob patterns. For example, to ignore all names starting with test_\nor ending with _test, you could use ignore-names = [\"test_*\", \"*_test\"].\nFor more information on the glob syntax, refer to the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nextend-ignore-names = [\"callMethod\"]\n\n\n\n[lint.pep8-naming]\nextend-ignore-names = [\"callMethod\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "N818": {
    "description": "[Status: stable] [Name: error-suffix-on-exception-name] [Message: Exception name <code>{name}</code> should be named with an Error suffix] What it does Checks for custom exception definitions that omit the Error suffix. Why is this bad? The Error suffix is recommended by PEP 8: Because exceptions should be classes, the class naming convention applies here. However, you should use the suffix \"Error\" on your exception names (if the exception actually is an error). Example class Validation(Exception): ... Use instead: class ValidationError(Exception): ...",
    "option": {
      "lint.pep8-naming.ignore-names": {
        "option_description": "A list of names (or patterns) to ignore when considering pep8-naming violations.\n\nSupports glob patterns. For example, to ignore all names starting with test_\nor ending with _test, you could use ignore-names = [\"test_*\", \"*_test\"].\nFor more information on the glob syntax, refer to the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nignore-names = [\"callMethod\"]\n\n\n\n[lint.pep8-naming]\nignore-names = [\"callMethod\"]",
        "option_data_type": "list[str]",
        "default_value": "[\"setUp\", \"tearDown\", \"setUpClass\", \"tearDownClass\", \"setUpModule\", \"tearDownModule\", \"asyncSetUp\", \"asyncTearDown\", \"setUpTestData\", \"failureException\", \"longMessage\", \"maxDiff\"]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pep8-naming.extend-ignore-names": {
        "option_description": "Additional names (or patterns) to ignore when considering pep8-naming violations,\nin addition to those included in ignore-names.\n\nSupports glob patterns. For example, to ignore all names starting with test_\nor ending with _test, you could use ignore-names = [\"test_*\", \"*_test\"].\nFor more information on the glob syntax, refer to the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nextend-ignore-names = [\"callMethod\"]\n\n\n\n[lint.pep8-naming]\nextend-ignore-names = [\"callMethod\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "N999": {
    "description": "[Status: stable] [Name: invalid-module-name] [Message: Invalid module name: '{name}'] What it does Checks for module names that do not follow the snake_case naming convention or are otherwise invalid. Why is this bad? PEP 8 recommends the use of the snake_case naming convention for module names: Modules should have short, all-lowercase names. Underscores can be used in the module name if it improves readability. Python packages should also have short, all-lowercase names, although the use of underscores is discouraged. When an extension module written in C or C++ has an accompanying Python module that provides a higher level (e.g. more object-oriented) interface, the C/C++ module has a leading underscore (e.g. _socket). Further, in order for Python modules to be importable, they must be valid identifiers. As such, they cannot start with a digit, or collide with hard keywords, like import or class. Example Instead of example-module-name or example module name, use example_module_name. Instead of ExampleModule, use example_module.",
    "option": {
      "lint.pep8-naming.ignore-names": {
        "option_description": "A list of names (or patterns) to ignore when considering pep8-naming violations.\n\nSupports glob patterns. For example, to ignore all names starting with test_\nor ending with _test, you could use ignore-names = [\"test_*\", \"*_test\"].\nFor more information on the glob syntax, refer to the globset documentation.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nignore-names = [\"callMethod\"]\n\n\n\n[lint.pep8-naming]\nignore-names = [\"callMethod\"]",
        "option_data_type": "list[str]",
        "default_value": "[\"setUp\", \"tearDown\", \"setUpClass\", \"tearDownClass\", \"setUpModule\", \"tearDownModule\", \"asyncSetUp\", \"asyncTearDown\", \"setUpTestData\", \"failureException\", \"longMessage\", \"maxDiff\"]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PERF101": {
    "description": "[Status: stable] [Name: unnecessary-list-cast] [Message: Do not cast an iterable to <code>list</code> before iterating over it] What it does Checks for explicit casts to list on for-loop iterables. Why is this bad? Using a list() call to eagerly iterate over an already-iterable type (like a tuple, list, or set) is inefficient, as it forces Python to create a new list unnecessarily. Removing the list() call will not change the behavior of the code, but may improve performance. Note that, as with all perflint rules, this is only intended as a micro-optimization, and will have a negligible impact on performance in most cases. Example items = (1, 2, 3) for i in list(items): print(i) Use instead: items = (1, 2, 3) for i in items: print(i) Fix safety This rule's fix is marked as unsafe if there's comments in the list() call, as comments may be removed. For example, the fix would be marked as unsafe in the following case: items = (1, 2, 3) for i in list( # comment items ): print(i)",
    "option": {}
  },
  "PERF102": {
    "description": "[Status: stable] [Name: incorrect-dict-iterator] [Message: When using only the {subset} of a dict use the <code>{subset}()</code> method] What it does Checks for uses of dict.items() that discard either the key or the value when iterating over the dictionary. Why is this bad? If you only need the keys or values of a dictionary, you should use dict.keys() or dict.values() respectively, instead of dict.items(). These specialized methods are more efficient than dict.items(), as they avoid allocating tuples for every item in the dictionary. They also communicate the intent of the code more clearly. Note that, as with all perflint rules, this is only intended as a micro-optimization, and will have a negligible impact on performance in most cases. Example obj = {\"a\": 1, \"b\": 2} for key, value in obj.items(): print(value) Use instead: obj = {\"a\": 1, \"b\": 2} for value in obj.values(): print(value) Fix safety The fix does not perform any type analysis and, as such, may suggest an incorrect fix if the object in question does not duck-type as a mapping (e.g., if it is missing a .keys() or .values() method, or if those methods behave differently than they do on standard mapping types).",
    "option": {}
  },
  "PERF203": {
    "description": "[Status: stable] [Name: try-except-in-loop] [Message: <code>try</code>-<code>except</code> within a loop incurs performance overhead] What it does Checks for uses of except handling via try-except within for and while loops. Why is this bad? Exception handling via try-except blocks incurs some performance overhead, regardless of whether an exception is raised. To optimize your code, two techniques are possible: Refactor your code to put the entire loop into the try-except block, rather than wrapping each iteration in a separate try-except block. Use \"Look Before You Leap\" idioms that attempt to avoid exceptions being raised in the first place, avoiding the need to use try-except blocks in the first place. This rule is only enforced for Python versions prior to 3.11, which introduced \"zero-cost\" exception handling. However, note that even on Python 3.11 and newer, refactoring your code to avoid exception handling in tight loops can provide a significant speedup in some cases, as zero-cost exception handling is only zero-cost in the \"happy path\" where no exception is raised in the try-except block. As with all perflint rules, this is only intended as a micro-optimization. In many cases, it will have a negligible impact on performance. Example string_numbers: list[str] = [\"1\", \"2\", \"three\", \"4\", \"5\"] # `try`/`except` that could be moved out of the loop: int_numbers: list[int] = [] for num in string_numbers: try: int_numbers.append(int(num)) except ValueError as e: print(f\"Couldn't convert to integer: {e}\") break # `try`/`except` used when \"look before you leap\" idioms could be used: number_names: dict[int, str] = {1: \"one\", 3: \"three\", 4: \"four\"} for number in range(5): try: name = number_names[number] except KeyError: continue else: print(f\"The name of {number} is {name}\") Use instead: string_numbers: list[str] = [\"1\", \"2\", \"three\", \"4\", \"5\"] int_numbers: list[int] = [] try: for num in string_numbers: int_numbers.append(int(num)) except ValueError as e: print(f\"Couldn't convert to integer: {e}\") number_names: dict[int, str] = {1: \"one\", 3: \"three\", 4: \"four\"} for number in range(5): name = number_names.get(number) if name is not None: print(f\"The name of {number} is {name}\")",
    "option": {
      "target-version": {
        "option_description": "The minimum Python version to target, e.g., when considering automatic\ncode upgrades, like rewriting type annotations. Ruff will not propose\nchanges using features that are not available in the given version.\n\nFor example, to represent supporting Python >=3.11 or ==3.11\nspecify target-version = \"py311\".\n\nIf you're already using a pyproject.toml file, we recommend\nproject.requires-python instead, as it's based on Python packaging\nstandards, and will be respected by other tools. For example, Ruff\ntreats the following as identical to target-version = \"py38\":\n\n[project]\nrequires-python = \">=3.8\"\n\nIf both are specified, target-version takes precedence over\nrequires-python. See Inferring the Python version\nfor a complete description of how the target-version is determined\nwhen left unspecified.\n\nNote that a stub file can sometimes make use of a typing feature\nbefore it is available at runtime, as long as the stub does not make\nuse of new syntax. For example, a type checker will understand\nint | str in a stub as being a Union type annotation, even if the\ntype checker is run using Python 3.9, despite the fact that the |\noperator can only be used to create union types at runtime on Python\n3.10+. As such, Ruff will often recommend newer features in a stub\nfile than it would for an equivalent runtime file with the same target\nversion.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"\n\n\n\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"",
        "option_data_type": "\"py37\" | \"py38\" | \"py39\" | \"py310\" | \"py311\" | \"py312\" | \"py313\" | \"py314\"",
        "default_value": "\"py310\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PERF401": {
    "description": "[Status: stable] [Name: manual-list-comprehension] [Message: Use {message_str} to create a transformed list] What it does Checks for for loops that can be replaced by a list comprehension. Why is this bad? When creating a transformed list from an existing list using a for-loop, prefer a list comprehension. List comprehensions are more readable and more performant. Using the below as an example, the list comprehension is ~10% faster on Python 3.11, and ~25% faster on Python 3.10. Note that, as with all perflint rules, this is only intended as a micro-optimization, and will have a negligible impact on performance in most cases. Example original = list(range(10000)) filtered = [] for i in original: if i % 2: filtered.append(i) Use instead: original = list(range(10000)) filtered = [x for x in original if x % 2] If you're appending to an existing list, use the extend method instead: original = list(range(10000)) filtered.extend(x for x in original if x % 2)",
    "option": {}
  },
  "PERF402": {
    "description": "[Status: stable] [Name: manual-list-copy] [Message: Use <code>list</code> or <code>list.copy</code> to create a copy of a list] What it does Checks for for loops that can be replaced by a making a copy of a list. Why is this bad? When creating a copy of an existing list using a for-loop, prefer list or list.copy instead. Making a direct copy is more readable and more performant. Using the below as an example, the list-based copy is ~2x faster on Python 3.11. Note that, as with all perflint rules, this is only intended as a micro-optimization, and will have a negligible impact on performance in most cases. Example original = list(range(10000)) filtered = [] for i in original: filtered.append(i) Use instead: original = list(range(10000)) filtered = list(original)",
    "option": {}
  },
  "PERF403": {
    "description": "[Status: stable] [Name: manual-dict-comprehension] [Message: Use a dictionary comprehension instead of {modifier} for-loop] What it does Checks for for loops that can be replaced by a dictionary comprehension. Why is this bad? When creating or extending a dictionary in a for-loop, prefer a dictionary comprehension. Comprehensions are more readable and more performant. For example, when comparing {x: x for x in list(range(1000))} to the for loop version, the comprehension is ~10% faster on Python 3.11. Note that, as with all perflint rules, this is only intended as a micro-optimization, and will have a negligible impact on performance in most cases. Example pairs = ((\"a\", 1), (\"b\", 2)) result = {} for x, y in pairs: if y % 2: result[x] = y Use instead: pairs = ((\"a\", 1), (\"b\", 2)) result = {x: y for x, y in pairs if y % 2} If you're appending to an existing dictionary, use the update method instead: pairs = ((\"a\", 1), (\"b\", 2)) result.update({x: y for x, y in pairs if y % 2})",
    "option": {}
  },
  "E101": {
    "description": "[Status: stable] [Name: mixed-spaces-and-tabs] [Message: Indentation contains mixed spaces and tabs] What it does Checks for mixed tabs and spaces in indentation. Why is this bad? Never mix tabs and spaces. The most popular way of indenting Python is with spaces only. The second-most popular way is with tabs only. Code indented with a mixture of tabs and spaces should be converted to using spaces exclusively. Example if a == 0:\\n a = 1\\n\\tb = 1 Use instead: if a == 0:\\n a = 1\\n b = 1",
    "option": {}
  },
  "E111": {
    "description": "[Status: preview] [Name: indentation-with-invalid-multiple] [Message: Indentation is not a multiple of {indent_width}] What it does Checks for indentation with a non-multiple of 4 spaces. Why is this bad? According to PEP 8, 4 spaces per indentation level should be preferred. Example if True: a = 1 Use instead: if True: a = 1 Formatter compatibility We recommend against using this rule alongside the formatter. The formatter enforces consistent indentation, making the rule redundant. The rule is also incompatible with the formatter when using indent-width with a value other than 4.",
    "option": {
      "indent-width": {
        "option_description": "The number of spaces per indentation level (tab).\n\nUsed by the formatter and when enforcing long-line violations (like E501) to determine the visual\nwidth of a tab.\n\nThis option changes the number of spaces the formatter inserts when\nusing soft-tabs (indent-style = space).\n\nPEP 8 recommends using 4 spaces per indentation level.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\nindent-width = 2\n\n\n\nindent-width = 2",
        "option_data_type": "int",
        "default_value": "4",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "E112": {
    "description": "[Status: preview] [Name: no-indented-block] [Message: Expected an indented block] What it does Checks for indented blocks that are lacking indentation. Why is this bad? All indented blocks should be indented; otherwise, they are not valid Python syntax. Example for item in items: pass Use instead: for item in items: pass",
    "option": {}
  },
  "E113": {
    "description": "[Status: preview] [Name: unexpected-indentation] [Message: Unexpected indentation] What it does Checks for unexpected indentation. Why is this bad? Indentation outside of a code block is not valid Python syntax. Example a = 1 b = 2 Use instead: a = 1 b = 2",
    "option": {}
  },
  "E114": {
    "description": "[Status: preview] [Name: indentation-with-invalid-multiple-comment] [Message: Indentation is not a multiple of {indent_width} (comment)] What it does Checks for indentation of comments with a non-multiple of 4 spaces. Why is this bad? According to PEP 8, 4 spaces per indentation level should be preferred. Example if True: # a = 1 ... Use instead: if True: # a = 1 ... Formatter compatibility We recommend against using this rule alongside the formatter. The formatter enforces consistent indentation, making the rule redundant. The rule is also incompatible with the formatter when using indent-width with a value other than 4.",
    "option": {
      "indent-width": {
        "option_description": "The number of spaces per indentation level (tab).\n\nUsed by the formatter and when enforcing long-line violations (like E501) to determine the visual\nwidth of a tab.\n\nThis option changes the number of spaces the formatter inserts when\nusing soft-tabs (indent-style = space).\n\nPEP 8 recommends using 4 spaces per indentation level.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\nindent-width = 2\n\n\n\nindent-width = 2",
        "option_data_type": "int",
        "default_value": "4",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "E115": {
    "description": "[Status: preview] [Name: no-indented-block-comment] [Message: Expected an indented block (comment)] What it does Checks for comments in a code blocks that are lacking indentation. Why is this bad? Comments within an indented block should themselves be indented, to indicate that they are part of the block. Example for item in items: # Hi pass Use instead: for item in items: # Hi pass",
    "option": {}
  },
  "E116": {
    "description": "[Status: preview] [Name: unexpected-indentation-comment] [Message: Unexpected indentation (comment)] What it does Checks for unexpected indentation of comment. Why is this bad? Comments should match the indentation of the containing code block. Example a = 1 # b = 2 Use instead: a = 1 # b = 2",
    "option": {}
  },
  "E117": {
    "description": "[Status: preview] [Name: over-indented] [Message: Over-indented (comment)] What it does Checks for over-indented code. Why is this bad? According to PEP 8, 4 spaces per indentation level should be preferred. Increased indentation can lead to inconsistent formatting, which can hurt readability. Example for item in items: pass Use instead: for item in items: pass Formatter compatibility We recommend against using this rule alongside the formatter. The formatter enforces consistent indentation, making the rule redundant.",
    "option": {}
  },
  "E201": {
    "description": "[Status: preview] [Name: whitespace-after-open-bracket] [Message: Whitespace after '{symbol}'] What it does Checks for the use of extraneous whitespace after \"(\", \"[\" or \"{\". Why is this bad? PEP 8 recommends the omission of whitespace in the following cases: \"Immediately inside parentheses, brackets or braces.\" \"Immediately before a comma, semicolon, or colon.\" Example spam( ham[1], {eggs: 2}) spam(ham[ 1], {eggs: 2}) spam(ham[1], { eggs: 2}) Use instead: spam(ham[1], {eggs: 2})",
    "option": {}
  },
  "E202": {
    "description": "[Status: preview] [Name: whitespace-before-close-bracket] [Message: Whitespace before '{symbol}'] What it does Checks for the use of extraneous whitespace before \")\", \"]\" or \"}\". Why is this bad? PEP 8 recommends the omission of whitespace in the following cases: \"Immediately inside parentheses, brackets or braces.\" \"Immediately before a comma, semicolon, or colon.\" Example spam(ham[1], {eggs: 2} ) spam(ham[1 ], {eggs: 2}) spam(ham[1], {eggs: 2 }) Use instead: spam(ham[1], {eggs: 2})",
    "option": {}
  },
  "E203": {
    "description": "[Status: preview] [Name: whitespace-before-punctuation] [Message: Whitespace before '{symbol}'] What it does Checks for the use of extraneous whitespace before \",\", \";\" or \":\". Why is this bad? PEP 8 recommends the omission of whitespace in the following cases: \"Immediately inside parentheses, brackets or braces.\" \"Immediately before a comma, semicolon, or colon.\" Example if x == 4: print(x, y); x, y = y , x Use instead: if x == 4: print(x, y); x, y = y, x",
    "option": {}
  },
  "E204": {
    "description": "[Status: preview] [Name: whitespace-after-decorator] [Message: Whitespace after decorator] What it does Checks for trailing whitespace after a decorator's opening @. Why is this bad? Including whitespace after the @ symbol is not compliant with PEP 8. Example @ decorator def func(): pass Use instead: @decorator def func(): pass",
    "option": {}
  },
  "E211": {
    "description": "[Status: preview] [Name: whitespace-before-parameters] [Message: Whitespace before '{bracket}'] What it does Checks for extraneous whitespace immediately preceding an open parenthesis or bracket. Why is this bad? According to PEP 8, open parentheses and brackets should not be preceded by any trailing whitespace. Example spam (1) Use instead: spam(1)",
    "option": {}
  },
  "E221": {
    "description": "[Status: preview] [Name: multiple-spaces-before-operator] [Message: Multiple spaces before operator] What it does Checks for extraneous whitespace before an operator. Why is this bad? According to PEP 8, operators should be surrounded by at most a single space on either side. Example a = 4 + 5 Use instead: a = 4 + 5",
    "option": {}
  },
  "E222": {
    "description": "[Status: preview] [Name: multiple-spaces-after-operator] [Message: Multiple spaces after operator] What it does Checks for extraneous whitespace after an operator. Why is this bad? According to PEP 8, operators should be surrounded by at most a single space on either side. Example a = 4 + 5 Use instead: a = 4 + 5",
    "option": {}
  },
  "E223": {
    "description": "[Status: preview] [Name: tab-before-operator] [Message: Tab before operator] What it does Checks for extraneous tabs before an operator. Why is this bad? According to PEP 8, operators should be surrounded by at most a single space on either side. Example a = 4\\t+ 5 Use instead: a = 4 + 5",
    "option": {}
  },
  "E224": {
    "description": "[Status: preview] [Name: tab-after-operator] [Message: Tab after operator] What it does Checks for extraneous tabs after an operator. Why is this bad? According to PEP 8, operators should be surrounded by at most a single space on either side. Example a = 4 +\\t5 Use instead: a = 4 + 5",
    "option": {}
  },
  "E225": {
    "description": "[Status: preview] [Name: missing-whitespace-around-operator] [Message: Missing whitespace around operator] What it does Checks for missing whitespace around all operators. Why is this bad? According to PEP 8, there should be one space before and after all assignment (=), augmented assignment (+=, -=, etc.), comparison, and Booleans operators. Example if number==42: print('you have found the meaning of life') Use instead: if number == 42: print('you have found the meaning of life')",
    "option": {}
  },
  "E226": {
    "description": "[Status: preview] [Name: missing-whitespace-around-arithmetic-operator] [Message: Missing whitespace around arithmetic operator] What it does Checks for missing whitespace arithmetic operators. Why is this bad? PEP 8 recommends never using more than one space, and always having the same amount of whitespace on both sides of a binary operator. For consistency, this rule enforces one space before and after an arithmetic operator (+, -, /, and *). (Note that PEP 8 suggests only adding whitespace around the operator with the lowest precedence, but that authors should \"use [their] own judgment\".) Example number = 40+2 Use instead: number = 40 + 2",
    "option": {}
  },
  "E227": {
    "description": "[Status: preview] [Name: missing-whitespace-around-bitwise-or-shift-operator] [Message: Missing whitespace around bitwise or shift operator] What it does Checks for missing whitespace around bitwise and shift operators. Why is this bad? PEP 8 recommends never using more than one space, and always having the same amount of whitespace on both sides of a binary operator. For consistency, this rule enforces one space before and after bitwise and shift operators (<<, >>, &, |, ^). (Note that PEP 8 suggests only adding whitespace around the operator with the lowest precedence, but that authors should \"use [their] own judgment\".) Example x = 128<<1 Use instead: x = 128 << 1",
    "option": {}
  },
  "E228": {
    "description": "[Status: preview] [Name: missing-whitespace-around-modulo-operator] [Message: Missing whitespace around modulo operator] What it does Checks for missing whitespace around the modulo operator. Why is this bad? PEP 8 recommends never using more than one space, and always having the same amount of whitespace on both sides of a binary operator. For consistency, this rule enforces one space before and after a modulo operator (%). (Note that PEP 8 suggests only adding whitespace around the operator with the lowest precedence, but that authors should \"use [their] own judgment\".) Example remainder = 10%2 Use instead: remainder = 10 % 2",
    "option": {}
  },
  "E231": {
    "description": "[Status: preview] [Name: missing-whitespace] [Message: Missing whitespace after {}] What it does Checks for missing whitespace after ,, ;, and :. Why is this bad? Missing whitespace after ,, ;, and : makes the code harder to read. Example a = (1,2) Use instead: a = (1, 2)",
    "option": {}
  },
  "E241": {
    "description": "[Status: preview] [Name: multiple-spaces-after-comma] [Message: Multiple spaces after comma] What it does Checks for extraneous whitespace after a comma. Why is this bad? Consistency is good. This rule helps ensure you have a consistent formatting style across your project. Example a = 4, 5 Use instead: a = 4, 5",
    "option": {}
  },
  "E242": {
    "description": "[Status: preview] [Name: tab-after-comma] [Message: Tab after comma] What it does Checks for extraneous tabs after a comma. Why is this bad? Commas should be followed by one space, never tabs. Example a = 4,\\t5 Use instead: a = 4, 5",
    "option": {}
  },
  "E251": {
    "description": "[Status: preview] [Name: unexpected-spaces-around-keyword-parameter-equals] [Message: Unexpected spaces around keyword / parameter equals] What it does Checks for missing whitespace around the equals sign in an unannotated function keyword parameter. Why is this bad? According to PEP 8, there should be no spaces around the equals sign in a keyword parameter, if it is unannotated: Dont use spaces around the = sign when used to indicate a keyword argument, or when used to indicate a default value for an unannotated function parameter. Example def add(a = 0) -> int: return a + 1 Use instead: def add(a=0) -> int: return a + 1",
    "option": {}
  },
  "E252": {
    "description": "[Status: preview] [Name: missing-whitespace-around-parameter-equals] [Message: Missing whitespace around parameter equals] What it does Checks for missing whitespace around the equals sign in an annotated function keyword parameter. Why is this bad? According to PEP 8, the spaces around the equals sign in a keyword parameter should only be omitted when the parameter is unannotated: Dont use spaces around the = sign when used to indicate a keyword argument, or when used to indicate a default value for an unannotated function parameter. Example def add(a: int=0) -> int: return a + 1 Use instead: def add(a: int = 0) -> int: return a + 1",
    "option": {}
  },
  "E261": {
    "description": "[Status: preview] [Name: too-few-spaces-before-inline-comment] [Message: Insert at least two spaces before an inline comment] What it does Checks if inline comments are separated by at least two spaces. Why is this bad? An inline comment is a comment on the same line as a statement. Per PEP 8, inline comments should be separated by at least two spaces from the preceding statement. Example x = x + 1 # Increment x Use instead: x = x + 1 # Increment x x = x + 1 # Increment x",
    "option": {}
  },
  "E262": {
    "description": "[Status: preview] [Name: no-space-after-inline-comment] [Message: Inline comment should start with <code>#</code>] What it does Checks if one space is used after inline comments. Why is this bad? An inline comment is a comment on the same line as a statement. Per PEP 8, inline comments should start with a # and a single space. Example x = x + 1 #Increment x x = x + 1 # Increment x x = x + 1 # \\xa0Increment x Use instead: x = x + 1 # Increment x x = x + 1 # Increment x",
    "option": {}
  },
  "E265": {
    "description": "[Status: preview] [Name: no-space-after-block-comment] [Message: Block comment should start with <code>#</code>] What it does Checks for block comments that lack a single space after the leading # character. Why is this bad? Per PEP 8, \"Block comments generally consist of one or more paragraphs built out of complete sentences, with each sentence ending in a period.\" Block comments should start with a # followed by a single space. Shebangs (lines starting with #!, at the top of a file) are exempt from this rule. Example #Block comment Use instead: # Block comment",
    "option": {}
  },
  "E266": {
    "description": "[Status: preview] [Name: multiple-leading-hashes-for-block-comment] [Message: Too many leading <code>#</code> before block comment] What it does Checks for block comments that start with multiple leading # characters. Why is this bad? Per PEP 8, \"Block comments generally consist of one or more paragraphs built out of complete sentences, with each sentence ending in a period.\" Each line of a block comment should start with a # followed by a single space. Shebangs (lines starting with #!, at the top of a file) are exempt from this rule. Example ### Block comment Use instead: # Block comment Alternatively, this rule makes an exception for comments that consist solely of # characters, as in: ############## # Block header ##############",
    "option": {}
  },
  "E271": {
    "description": "[Status: preview] [Name: multiple-spaces-after-keyword] [Message: Multiple spaces after keyword] What it does Checks for extraneous whitespace after keywords. Why is this bad? Example True and False Use instead: True and False",
    "option": {}
  },
  "E272": {
    "description": "[Status: preview] [Name: multiple-spaces-before-keyword] [Message: Multiple spaces before keyword] What it does Checks for extraneous whitespace before keywords. Why is this bad? Example x and y Use instead: x and y",
    "option": {}
  },
  "E273": {
    "description": "[Status: preview] [Name: tab-after-keyword] [Message: Tab after keyword] What it does Checks for extraneous tabs after keywords. Why is this bad? Example True and\\tFalse Use instead: True and False",
    "option": {}
  },
  "E274": {
    "description": "[Status: preview] [Name: tab-before-keyword] [Message: Tab before keyword] What it does Checks for extraneous tabs before keywords. Why is this bad? Example True\\tand False Use instead: True and False",
    "option": {}
  },
  "E275": {
    "description": "[Status: preview] [Name: missing-whitespace-after-keyword] [Message: Missing whitespace after keyword] What it does Checks for missing whitespace after keywords. Why is this bad? Missing whitespace after keywords makes the code harder to read. Example if(True): pass Use instead: if (True): pass",
    "option": {}
  },
  "E301": {
    "description": "[Status: preview] [Name: blank-line-between-methods] [Message: Expected {BLANK_LINES_NESTED_LEVEL:?} blank line, found 0] What it does Checks for missing blank lines between methods of a class. Why is this bad? PEP 8 recommends exactly one blank line between methods of a class. Example class MyClass(object): def func1(): pass def func2(): pass Use instead: class MyClass(object): def func1(): pass def func2(): pass Typing stub files (.pyi) The typing style guide recommends to not use blank lines between methods except to group them. That's why this rule is not enabled in typing stub files.",
    "option": {}
  },
  "E302": {
    "description": "[Status: preview] [Name: blank-lines-top-level] [Message: Expected {expected_blank_lines:?} blank lines, found {actual_blank_lines}] What it does Checks for missing blank lines between top level functions and classes. Why is this bad? PEP 8 recommends exactly two blank lines between top level functions and classes. The rule respects the lint.isort.lines-after-imports setting when determining the required number of blank lines between top-level import statements and function or class definitions for compatibility with isort. Example def func1(): pass def func2(): pass Use instead: def func1(): pass def func2(): pass Typing stub files (.pyi) The typing style guide recommends to not use blank lines between classes and functions except to group them. That's why this rule is not enabled in typing stub files.",
    "option": {
      "lint.isort.lines-after-imports": {
        "option_description": "The number of blank lines to place after imports.\nUse -1 for automatic determination.\n\nRuff uses at most one blank line after imports in typing stub files (files with .pyi extension) in accordance to\nthe typing style recommendations (source).\n\nWhen using the formatter, only the values -1, 1, and 2 are compatible because\nit enforces at least one empty and at most two empty lines after imports.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.isort]\n# Use a single line after each import block.\nlines-after-imports = 1\n\n\n\n[lint.isort]\n# Use a single line after each import block.\nlines-after-imports = 1",
        "option_data_type": "int",
        "default_value": "-1",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "E303": {
    "description": "[Status: preview] [Name: too-many-blank-lines] [Message: Too many blank lines ({actual_blank_lines})] What it does Checks for extraneous blank lines. Why is this bad? PEP 8 recommends using blank lines as follows: No more than two blank lines between top-level statements. No more than one blank line between non-top-level statements. Example def func1(): pass def func2(): pass Use instead: def func1(): pass def func2(): pass Typing stub files (.pyi) The rule allows at most one blank line in typing stub files in accordance to the typing style guide recommendation. Note: The rule respects the following isort settings when determining the maximum number of blank lines allowed between two statements: lint.isort.lines-after-imports: For top-level statements directly following an import statement. lint.isort.lines-between-types: For import statements directly following a from ... import ... statement or vice versa.",
    "option": {
      "lint.isort.lines-after-imports": {
        "option_description": "The number of blank lines to place after imports.\nUse -1 for automatic determination.\n\nRuff uses at most one blank line after imports in typing stub files (files with .pyi extension) in accordance to\nthe typing style recommendations (source).\n\nWhen using the formatter, only the values -1, 1, and 2 are compatible because\nit enforces at least one empty and at most two empty lines after imports.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.isort]\n# Use a single line after each import block.\nlines-after-imports = 1\n\n\n\n[lint.isort]\n# Use a single line after each import block.\nlines-after-imports = 1",
        "option_data_type": "int",
        "default_value": "-1",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.isort.lines-between-types": {
        "option_description": "The number of lines to place between \"direct\" and import from imports.\n\nWhen using the formatter, only the values 0 and 1 are compatible because\nit preserves up to one empty line after imports in nested blocks.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.isort]\n# Use a single line between direct and from import.\nlines-between-types = 1\n\n\n\n[lint.isort]\n# Use a single line between direct and from import.\nlines-between-types = 1",
        "option_data_type": "int",
        "default_value": "0",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "E304": {
    "description": "[Status: preview] [Name: blank-line-after-decorator] [Message: Blank lines found after function decorator ({lines})] What it does Checks for extraneous blank line(s) after function decorators. Why is this bad? There should be no blank lines between a decorator and the object it is decorating. Example class User(object): @property def name(self): pass Use instead: class User(object): @property def name(self): pass",
    "option": {}
  },
  "E305": {
    "description": "[Status: preview] [Name: blank-lines-after-function-or-class] [Message: Expected 2 blank lines after class or function definition, found ({blank_lines})] What it does Checks for missing blank lines after the end of function or class. Why is this bad? PEP 8 recommends using blank lines as follows: Two blank lines are expected between functions and classes One blank line is expected between methods of a class. Example class User(object): pass user = User() Use instead: class User(object): pass user = User() Typing stub files (.pyi) The typing style guide recommends to not use blank lines between statements except to group them. That's why this rule is not enabled in typing stub files.",
    "option": {}
  },
  "E306": {
    "description": "[Status: preview] [Name: blank-lines-before-nested-definition] [Message: Expected 1 blank line before a nested definition, found 0] What it does Checks for 1 blank line between nested function or class definitions. Why is this bad? PEP 8 recommends using blank lines as follows: Two blank lines are expected between functions and classes One blank line is expected between methods of a class. Example def outer(): def inner(): pass def inner2(): pass Use instead: def outer(): def inner(): pass def inner2(): pass Typing stub files (.pyi) The typing style guide recommends to not use blank lines between classes and functions except to group them. That's why this rule is not enabled in typing stub files.",
    "option": {}
  },
  "E401": {
    "description": "[Status: stable] [Name: multiple-imports-on-one-line] [Message: Multiple imports on one line] What it does Check for multiple imports on one line. Why is this bad? According to PEP 8, \"imports should usually be on separate lines.\" Example import sys, os Use instead: import os import sys",
    "option": {}
  },
  "E402": {
    "description": "[Status: stable] [Name: module-import-not-at-top-of-file] [Message: Module level import not at top of cell] What it does Checks for imports that are not at the top of the file. Why is this bad? According to PEP 8, \"imports are always put at the top of the file, just after any module comments and docstrings, and before module globals and constants.\" This rule makes an exception for both sys.path modifications (allowing for sys.path.insert, sys.path.append, etc.) and os.environ modifications between imports. Example \"One string\" \"Two string\" a = 1 import os from sys import x Use instead: import os from sys import x \"One string\" \"Two string\" a = 1 Notebook behavior For Jupyter notebooks, this rule checks for imports that are not at the top of a cell.",
    "option": {}
  },
  "E501": {
    "description": "[Status: stable] [Name: line-too-long] [Message: Line too long ({width} > {limit})] What it does Checks for lines that exceed the specified maximum character length. Why is this bad? Overlong lines can hurt readability. PEP 8, for example, recommends limiting lines to 79 characters. By default, this rule enforces a limit of 88 characters for compatibility with Black and the Ruff formatter, though that limit is configurable via the line-length setting. In the interest of pragmatism, this rule makes a few exceptions when determining whether a line is overlong. Namely, it: Ignores lines that consist of a single \"word\" (i.e., without any whitespace between its characters). Ignores lines that end with a URL, as long as the URL starts before the line-length threshold. Ignores line that end with a pragma comment (e.g., # type: ignore or # noqa), as long as the pragma comment starts before the line-length threshold. That is, a line will not be flagged as overlong if a pragma comment causes it to exceed the line length. (This behavior aligns with that of the Ruff formatter.) Ignores SPDX license identifiers and copyright notices (e.g., # SPDX-License-Identifier: MIT), which are machine-readable and should not wrap over multiple lines. If lint.pycodestyle.ignore-overlong-task-comments is true, this rule will also ignore comments that start with any of the specified lint.task-tags (e.g., # TODO:). Example my_function(param1, param2, param3, param4, param5, param6, param7, param8, param9, param10) Use instead: my_function( param1, param2, param3, param4, param5, param6, param7, param8, param9, param10 ) Error suppression Hint: when suppressing E501 errors within multi-line strings (like docstrings), the noqa directive should come at the end of the string (after the closing triple quote), and will apply to the entire string, like so: \"\"\"Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor. \"\"\" # noqa: E501",
    "option": {
      "line-length": {
        "option_description": "The line length to use when enforcing long-lines violations (like E501)\nand at which isort and the formatter prefers to wrap lines.\n\nThe length is determined by the number of characters per line, except for lines containing East Asian characters or emojis.\nFor these lines, the unicode width of each character is added up to determine the length.\n\nThe value must be greater than 0 and less than or equal to 320.\n\nNote: While the formatter will attempt to format lines such that they remain\nwithin the line-length, it isn't a hard upper bound, and formatted lines may\nexceed the line-length.\n\nSee pycodestyle.max-line-length to configure different lengths for E501 and the formatter.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\n# Allow lines to be as long as 120.\nline-length = 120\n\n\n\n# Allow lines to be as long as 120.\nline-length = 120",
        "option_data_type": "int",
        "default_value": "88",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.task-tags": {
        "option_description": "A list of task tags to recognize (e.g., \"TODO\", \"FIXME\", \"XXX\").\n\nComments starting with these tags will be ignored by commented-out code\ndetection (ERA), and skipped by line-length rules (E501) if\nignore-overlong-task-comments is set to true.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\ntask-tags = [\"HACK\"]\n\n\n\n[lint]\ntask-tags = [\"HACK\"]",
        "option_data_type": "list[str]",
        "default_value": "[\"TODO\", \"FIXME\", \"XXX\"]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pycodestyle.ignore-overlong-task-comments": {
        "option_description": "Whether line-length violations (E501) should be triggered for\ncomments starting with task-tags (by default: \"TODO\", \"FIXME\",\nand \"XXX\").\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pycodestyle]\nignore-overlong-task-comments = true\n\n\n\n[lint.pycodestyle]\nignore-overlong-task-comments = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pycodestyle.max-line-length": {
        "option_description": "The maximum line length to allow for line-too-long violations. By default,\nthis is set to the value of the line-length option.\n\nUse this option when you want to detect extra-long lines that the formatter can't automatically split by setting\npycodestyle.line-length to a value larger than line-length.\n\n# The formatter wraps lines at a length of 88.\nline-length = 88\n\n[pycodestyle]\n# E501 reports lines that exceed the length of 100.\nmax-line-length = 100\n\nThe length is determined by the number of characters per line, except for lines containing East Asian characters or emojis.\nFor these lines, the unicode width of each character is added up to determine the length.\n\nSee the line-too-long rule for more information.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pycodestyle]\nmax-line-length = 100\n\n\n\n[lint.pycodestyle]\nmax-line-length = 100",
        "option_data_type": "int",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "E502": {
    "description": "[Status: preview] [Name: redundant-backslash] [Message: Redundant backslash] What it does Checks for redundant backslashes between brackets. Why is this bad? Explicit line joins using a backslash are redundant between brackets. Example x = (2 + \\ 2) Use instead: x = (2 + 2)",
    "option": {}
  },
  "E701": {
    "description": "[Status: stable] [Name: multiple-statements-on-one-line-colon] [Message: Multiple statements on one line (colon)] What it does Checks for compound statements (multiple statements on the same line). Why is this bad? According to PEP 8, \"compound statements are generally discouraged\". Example if foo == \"blah\": do_blah_thing() Use instead: if foo == \"blah\": do_blah_thing()",
    "option": {}
  },
  "E702": {
    "description": "[Status: stable] [Name: multiple-statements-on-one-line-semicolon] [Message: Multiple statements on one line (semicolon)] What it does Checks for multiline statements on one line. Why is this bad? According to PEP 8, including multi-clause statements on the same line is discouraged. Example do_one(); do_two(); do_three() Use instead: do_one() do_two() do_three()",
    "option": {}
  },
  "E703": {
    "description": "[Status: stable] [Name: useless-semicolon] [Message: Statement ends with an unnecessary semicolon] What it does Checks for statements that end with an unnecessary semicolon. Why is this bad? A trailing semicolon is unnecessary and should be removed. Example do_four(); # useless semicolon Use instead: do_four()",
    "option": {}
  },
  "E711": {
    "description": "[Status: stable] [Name: none-comparison] [Message: Comparison to <code>None</code> should be <code>cond is None</code>] What it does Checks for comparisons to None which are not using the is operator. Why is this bad? According to PEP 8, \"Comparisons to singletons like None should always be done with is or is not, never the equality operators.\" Example if arg != None: pass if None == arg: pass Use instead: if arg is not None: pass Fix safety This rule's fix is marked as unsafe, as it may alter runtime behavior when used with libraries that override the ==/__eq__ or !=/__ne__ operators. In these cases, is/is not may not be equivalent to ==/!=. For more information, see this issue.",
    "option": {}
  },
  "E712": {
    "description": "[Status: stable] [Name: true-false-comparison] [Message: Avoid equality comparisons to <code>True</code>; use <code>{cond}:</code> for truth checks] What it does Checks for equality comparisons to boolean literals. Why is this bad? PEP 8 recommends against using the equality operators == and != to compare values to True or False. Instead, use if cond: or if not cond: to check for truth values. If you intend to check if a value is the boolean literal True or False, consider using is or is not to check for identity instead. Example if foo == True: ... if bar == False: ... Use instead: if foo: ... if not bar: ... Fix safety This rule's fix is marked as unsafe, as it may alter runtime behavior when used with libraries that override the ==/__eq__ or !=/__ne__ operators. In these cases, is/is not may not be equivalent to ==/!=. For more information, see this issue.",
    "option": {}
  },
  "E713": {
    "description": "[Status: stable] [Name: not-in-test] [Message: Test for membership should be <code>not in</code>] What it does Checks for membership tests using not {element} in {collection}. Why is this bad? Testing membership with {element} not in {collection} is more readable. Example Z = not X in Y if not X.B in Y: pass Use instead: Z = X not in Y if X.B not in Y: pass",
    "option": {}
  },
  "E714": {
    "description": "[Status: stable] [Name: not-is-test] [Message: Test for object identity should be <code>is not</code>] What it does Checks for identity comparisons using not {foo} is {bar}. Why is this bad? According to PEP8, testing for an object's identity with is not is more readable. Example if not X is Y: pass Z = not X.B is Y Use instead: if X is not Y: pass Z = X.B is not Y",
    "option": {}
  },
  "E721": {
    "description": "[Status: stable] [Name: type-comparison] [Message: Use <code>is</code> and <code>is not</code> for type comparisons, or <code>isinstance()</code> for isinstance checks] What it does Checks for object type comparisons using == and other comparison operators. Why is this bad? Unlike a direct type comparison, isinstance will also check if an object is an instance of a class or a subclass thereof. If you want to check for an exact type match, use is or is not. Known problems When using libraries that override the == (__eq__) operator (such as NumPy, Pandas, and SQLAlchemy), this rule may produce false positives, as converting from == to is or is not will change the behavior of the code. For example, the following operations are not equivalent: import numpy as np np.array([True, False]) == False # array([False, True]) np.array([True, False]) is False # False Example if type(obj) == type(1): pass if type(obj) == int: pass Use instead: if isinstance(obj, int): pass",
    "option": {}
  },
  "E722": {
    "description": "[Status: stable] [Name: bare-except] [Message: Do not use bare <code>except</code>] What it does Checks for bare except catches in try-except statements. Why is this bad? A bare except catches BaseException which includes KeyboardInterrupt, SystemExit, Exception, and others. Catching BaseException can make it hard to interrupt the program (e.g., with Ctrl-C) and can disguise other problems. Example try: raise KeyboardInterrupt(\"You probably don't mean to break CTRL-C.\") except: print(\"But a bare `except` will ignore keyboard interrupts.\") Use instead: try: do_something_that_might_break() except MoreSpecificException as e: handle_error(e) If you actually need to catch an unknown error, use Exception which will catch regular program errors but not important system exceptions. def run_a_function(some_other_fn): try: some_other_fn() except Exception as e: print(f\"How exceptional! {e}\")",
    "option": {}
  },
  "E731": {
    "description": "[Status: stable] [Name: lambda-assignment] [Message: Do not assign a <code>lambda</code> expression, use a <code>def</code>] What it does Checks for lambda expressions which are assigned to a variable. Why is this bad? Per PEP 8, you should \"Always use a def statement instead of an assignment statement that binds a lambda expression directly to an identifier.\" Using a def statement leads to better tracebacks, and the assignment itself negates the primary benefit of using a lambda expression (i.e., that it can be embedded inside another expression). Example f = lambda x: 2 * x Use instead: def f(x): return 2 * x",
    "option": {}
  },
  "E741": {
    "description": "[Status: stable] [Name: ambiguous-variable-name] [Message: Ambiguous variable name: <code>{name}</code>] What it does Checks for the use of the characters 'l', 'O', or 'I' as variable names. Note: This rule is automatically disabled for all stub files (files with .pyi extensions). The rule has little relevance for authors of stubs: a well-written stub should aim to faithfully represent the interface of the equivalent .py file as it exists at runtime, including any ambiguously named variables in the runtime module. Why is this bad? In some fonts, these characters are indistinguishable from the numerals one and zero. When tempted to use 'l', use 'L' instead. Example l = 0 O = 123 I = 42 Use instead: L = 0 o = 123 i = 42",
    "option": {}
  },
  "E742": {
    "description": "[Status: stable] [Name: ambiguous-class-name] [Message: Ambiguous class name: <code>{name}</code>] What it does Checks for the use of the characters 'l', 'O', or 'I' as class names. Why is this bad? In some fonts, these characters are indistinguishable from the numerals one and zero. When tempted to use 'l', use 'L' instead. Example class I(object): ... Use instead: class Integer(object): ...",
    "option": {}
  },
  "E743": {
    "description": "[Status: stable] [Name: ambiguous-function-name] [Message: Ambiguous function name: <code>{name}</code>] What it does Checks for the use of the characters 'l', 'O', or 'I' as function names. Why is this bad? In some fonts, these characters are indistinguishable from the numerals one and zero. When tempted to use 'l', use 'L' instead. Example def l(x): ... Use instead: def long_name(x): ...",
    "option": {}
  },
  "E902": {
    "description": "[Status: stable] [Name: io-error] [Message: {message}] What it does This is not a regular diagnostic; instead, it's raised when a file cannot be read from disk. Why is this bad? An IOError indicates an error in the development setup. For example, the user may not have permissions to read a given file, or the filesystem may contain a broken symlink. Example On Linux or macOS: $ echo 'print(\"hello world!\")' > a.py $ chmod 000 a.py $ ruff a.py a.py:1:1: E902 Permission denied (os error 13) Found 1 error.",
    "option": {}
  },
  "W191": {
    "description": "[Status: stable] [Name: tab-indentation] [Message: Indentation contains tabs] What it does Checks for indentation that uses tabs. Why is this bad? According to PEP 8, spaces are preferred over tabs (unless used to remain consistent with code that is already indented with tabs). Formatter compatibility We recommend against using this rule alongside the formatter. The formatter enforces consistent indentation, making the rule redundant. The rule is also incompatible with the formatter when using format.indent-style=\"tab\".",
    "option": {}
  },
  "W291": {
    "description": "[Status: stable] [Name: trailing-whitespace] [Message: Trailing whitespace] What it does Checks for superfluous trailing whitespace. Why is this bad? According to PEP 8, \"avoid trailing whitespace anywhere. Because its usually invisible, it can be confusing\" Example spam(1) \\n# Use instead: spam(1)\\n# Fix safety This fix is marked unsafe if the whitespace is inside a multiline string, as removing it changes the string's content.",
    "option": {}
  },
  "W292": {
    "description": "[Status: stable] [Name: missing-newline-at-end-of-file] [Message: No newline at end of file] What it does Checks for files missing a new line at the end of the file. Why is this bad? Trailing blank lines in a file are superfluous. However, the last line of the file should end with a newline. Example spam(1) Use instead: spam(1)\\n",
    "option": {}
  },
  "W293": {
    "description": "[Status: stable] [Name: blank-line-with-whitespace] [Message: Blank line contains whitespace] What it does Checks for superfluous whitespace in blank lines. Why is this bad? According to PEP 8, \"avoid trailing whitespace anywhere. Because its usually invisible, it can be confusing\" Example class Foo(object):\\n \\n bang = 12 Use instead: class Foo(object):\\n\\n bang = 12 Fix safety This fix is marked unsafe if the whitespace is inside a multiline string, as removing it changes the string's content.",
    "option": {}
  },
  "W391": {
    "description": "[Status: preview] [Name: too-many-newlines-at-end-of-file] [Message: Too many newlines at end of {domain}] What it does Checks for files with multiple trailing blank lines. In the case of notebooks, this check is applied to each cell separately. Why is this bad? Trailing blank lines in a file are superfluous. However, the last line of the file should end with a newline. Example spam(1)\\n\\n\\n Use instead: spam(1)\\n",
    "option": {}
  },
  "W505": {
    "description": "[Status: stable] [Name: doc-line-too-long] [Message: Doc line too long ({width} > {limit})] What it does Checks for doc lines that exceed the specified maximum character length. Why is this bad? For flowing long blocks of text (docstrings or comments), overlong lines can hurt readability. PEP 8, for example, recommends that such lines be limited to 72 characters, while this rule enforces the limit specified by the lint.pycodestyle.max-doc-length setting. (If no value is provided, this rule will be ignored, even if it's added to your --select list.) In the context of this rule, a \"doc line\" is defined as a line consisting of either a standalone comment or a standalone string, like a docstring. In the interest of pragmatism, this rule makes a few exceptions when determining whether a line is overlong. Namely, it: Ignores lines that consist of a single \"word\" (i.e., without any whitespace between its characters). Ignores lines that end with a URL, as long as the URL starts before the line-length threshold. Ignores line that end with a pragma comment (e.g., # type: ignore or # noqa), as long as the pragma comment starts before the line-length threshold. That is, a line will not be flagged as overlong if a pragma comment causes it to exceed the line length. (This behavior aligns with that of the Ruff formatter.) If lint.pycodestyle.ignore-overlong-task-comments is true, this rule will also ignore comments that start with any of the specified lint.task-tags (e.g., # TODO:). Example def function(x): \"\"\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis auctor purus ut ex fermentum, at maximus est hendrerit.\"\"\" Use instead: def function(x): \"\"\" Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis auctor purus ut ex fermentum, at maximus est hendrerit. \"\"\" Error suppression Hint: when suppressing W505 errors within multi-line strings (like docstrings), the noqa directive should come at the end of the string (after the closing triple quote), and will apply to the entire string, like so: \"\"\"Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor. \"\"\" # noqa: W505",
    "option": {
      "lint.task-tags": {
        "option_description": "A list of task tags to recognize (e.g., \"TODO\", \"FIXME\", \"XXX\").\n\nComments starting with these tags will be ignored by commented-out code\ndetection (ERA), and skipped by line-length rules (E501) if\nignore-overlong-task-comments is set to true.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\ntask-tags = [\"HACK\"]\n\n\n\n[lint]\ntask-tags = [\"HACK\"]",
        "option_data_type": "list[str]",
        "default_value": "[\"TODO\", \"FIXME\", \"XXX\"]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pycodestyle.max-doc-length": {
        "option_description": "The maximum line length to allow for doc-line-too-long violations within\ndocumentation (W505), including standalone comments. By default,\nthis is set to null which disables reporting violations.\n\nThe length is determined by the number of characters per line, except for lines containing Asian characters or emojis.\nFor these lines, the unicode width of each character is added up to determine the length.\n\nSee the doc-line-too-long rule for more information.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pycodestyle]\nmax-doc-length = 88\n\n\n\n[lint.pycodestyle]\nmax-doc-length = 88",
        "option_data_type": "int",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pycodestyle.ignore-overlong-task-comments": {
        "option_description": "Whether line-length violations (E501) should be triggered for\ncomments starting with task-tags (by default: \"TODO\", \"FIXME\",\nand \"XXX\").\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pycodestyle]\nignore-overlong-task-comments = true\n\n\n\n[lint.pycodestyle]\nignore-overlong-task-comments = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "W605": {
    "description": "[Status: stable] [Name: invalid-escape-sequence] [Message: Invalid escape sequence: <code>\\{ch}</code>] What it does Checks for invalid escape sequences. Why is this bad? Invalid escape sequences are deprecated in Python 3.6. Example regex = \"\\.png$\" Use instead: regex = r\"\\.png$\" Or, if the string already contains a valid escape sequence: value = \"new line\\nand invalid escape \\_ here\" Use instead: value = \"new line\\nand invalid escape \\\\_ here\"",
    "option": {}
  },
  "DOC102": {
    "description": "[Status: preview] [Name: docstring-extraneous-parameter] [Message: Documented parameter <code>{id}</code> is not in the function's signature] What it does Checks for function docstrings that include parameters which are not in the function signature. Why is this bad? If a docstring documents a parameter which is not in the function signature, it can be misleading to users and/or a sign of incomplete documentation or refactors. Example def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Args: distance: Distance traveled. time: Time spent traveling. acceleration: Rate of change of speed. Returns: Speed as distance divided by time. \"\"\" return distance / time Use instead: def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Args: distance: Distance traveled. time: Time spent traveling. Returns: Speed as distance divided by time. \"\"\" return distance / time",
    "option": {
      "lint.pydoclint.ignore-one-line-docstrings": {
        "option_description": "Skip docstrings which fit on a single line.\n\nNote: The corresponding setting in pydoclint\nis named skip-checking-short-docstrings.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydoclint]\n# Skip docstrings which fit on a single line.\nignore-one-line-docstrings = true\n\n\n\n[lint.pydoclint]\n# Skip docstrings which fit on a single line.\nignore-one-line-docstrings = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pydocstyle.convention": {
        "option_description": "Whether to use Google-style, NumPy-style conventions, or the PEP 257\ndefaults when analyzing docstring sections.\n\nEnabling a convention will disable all rules that are not included in\nthe specified convention. As such, the intended workflow is to enable a\nconvention and then selectively enable or disable any additional rules\non top of it.\n\nFor example, to use Google-style conventions but avoid requiring\ndocumentation for every function parameter:\n\n[tool.ruff.lint]\n# Enable all `pydocstyle` rules, limiting to those that adhere to the\n# Google convention via `convention = \"google\"`, below.\nselect = [\"D\"]\n\n# On top of the Google convention, disable `D417`, which requires\n# documentation for every function parameter.\nignore = [\"D417\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\nTo enable an additional rule that's excluded from the convention,\nselect the desired rule via its fully qualified rule code (e.g.,\nD400 instead of D4 or D40):\n\n[tool.ruff.lint]\n# Enable D400 on top of the Google convention.\nextend-select = [\"D400\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"\n\n\n\n[lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"",
        "option_data_type": "\"google\" | \"numpy\" | \"pep257\"",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "DOC201": {
    "description": "[Status: preview] [Name: docstring-missing-returns] [Message: <code>return</code> is not documented in docstring] What it does Checks for functions with return statements that do not have \"Returns\" sections in their docstrings. Why is this bad? A missing \"Returns\" section is a sign of incomplete documentation. This rule is not enforced for abstract methods or functions that only return None. It is also ignored for \"stub functions\": functions where the body only consists of pass, ..., raise NotImplementedError, or similar. Example def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Args: distance: Distance traveled. time: Time spent traveling. \"\"\" return distance / time Use instead: def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Args: distance: Distance traveled. time: Time spent traveling. Returns: Speed as distance divided by time. \"\"\" return distance / time",
    "option": {
      "lint.pydoclint.ignore-one-line-docstrings": {
        "option_description": "Skip docstrings which fit on a single line.\n\nNote: The corresponding setting in pydoclint\nis named skip-checking-short-docstrings.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydoclint]\n# Skip docstrings which fit on a single line.\nignore-one-line-docstrings = true\n\n\n\n[lint.pydoclint]\n# Skip docstrings which fit on a single line.\nignore-one-line-docstrings = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pydocstyle.convention": {
        "option_description": "Whether to use Google-style, NumPy-style conventions, or the PEP 257\ndefaults when analyzing docstring sections.\n\nEnabling a convention will disable all rules that are not included in\nthe specified convention. As such, the intended workflow is to enable a\nconvention and then selectively enable or disable any additional rules\non top of it.\n\nFor example, to use Google-style conventions but avoid requiring\ndocumentation for every function parameter:\n\n[tool.ruff.lint]\n# Enable all `pydocstyle` rules, limiting to those that adhere to the\n# Google convention via `convention = \"google\"`, below.\nselect = [\"D\"]\n\n# On top of the Google convention, disable `D417`, which requires\n# documentation for every function parameter.\nignore = [\"D417\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\nTo enable an additional rule that's excluded from the convention,\nselect the desired rule via its fully qualified rule code (e.g.,\nD400 instead of D4 or D40):\n\n[tool.ruff.lint]\n# Enable D400 on top of the Google convention.\nextend-select = [\"D400\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"\n\n\n\n[lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"",
        "option_data_type": "\"google\" | \"numpy\" | \"pep257\"",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pydocstyle.property-decorators": {
        "option_description": "A list of decorators that, when applied to a method, indicate that the\nmethod should be treated as a property (in addition to the builtin\n@property and standard-library @functools.cached_property).\n\nFor example, Ruff will expect that any method decorated by a decorator\nin this list can use a non-imperative summary line.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\nproperty-decorators = [\"gi.repository.GObject.Property\"]\n\n\n\n[lint.pydocstyle]\nproperty-decorators = [\"gi.repository.GObject.Property\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "DOC202": {
    "description": "[Status: preview] [Name: docstring-extraneous-returns] [Message: Docstring should not have a returns section because the function doesn't return anything] What it does Checks for function docstrings with unnecessary \"Returns\" sections. Why is this bad? A function without an explicit return statement should not have a \"Returns\" section in its docstring. This rule is not enforced for abstract methods. It is also ignored for \"stub functions\": functions where the body only consists of pass, ..., raise NotImplementedError, or similar. Example def say_hello(n: int) -> None: \"\"\"Says hello to the user. Args: n: Number of times to say hello. Returns: Doesn't return anything. \"\"\" for _ in range(n): print(\"Hello!\") Use instead: def say_hello(n: int) -> None: \"\"\"Says hello to the user. Args: n: Number of times to say hello. \"\"\" for _ in range(n): print(\"Hello!\")",
    "option": {
      "lint.pydoclint.ignore-one-line-docstrings": {
        "option_description": "Skip docstrings which fit on a single line.\n\nNote: The corresponding setting in pydoclint\nis named skip-checking-short-docstrings.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydoclint]\n# Skip docstrings which fit on a single line.\nignore-one-line-docstrings = true\n\n\n\n[lint.pydoclint]\n# Skip docstrings which fit on a single line.\nignore-one-line-docstrings = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pydocstyle.convention": {
        "option_description": "Whether to use Google-style, NumPy-style conventions, or the PEP 257\ndefaults when analyzing docstring sections.\n\nEnabling a convention will disable all rules that are not included in\nthe specified convention. As such, the intended workflow is to enable a\nconvention and then selectively enable or disable any additional rules\non top of it.\n\nFor example, to use Google-style conventions but avoid requiring\ndocumentation for every function parameter:\n\n[tool.ruff.lint]\n# Enable all `pydocstyle` rules, limiting to those that adhere to the\n# Google convention via `convention = \"google\"`, below.\nselect = [\"D\"]\n\n# On top of the Google convention, disable `D417`, which requires\n# documentation for every function parameter.\nignore = [\"D417\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\nTo enable an additional rule that's excluded from the convention,\nselect the desired rule via its fully qualified rule code (e.g.,\nD400 instead of D4 or D40):\n\n[tool.ruff.lint]\n# Enable D400 on top of the Google convention.\nextend-select = [\"D400\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"\n\n\n\n[lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"",
        "option_data_type": "\"google\" | \"numpy\" | \"pep257\"",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "DOC402": {
    "description": "[Status: preview] [Name: docstring-missing-yields] [Message: <code>yield</code> is not documented in docstring] What it does Checks for functions with yield statements that do not have \"Yields\" sections in their docstrings. Why is this bad? A missing \"Yields\" section is a sign of incomplete documentation. This rule is not enforced for abstract methods or functions that only yield None. It is also ignored for \"stub functions\": functions where the body only consists of pass, ..., raise NotImplementedError, or similar. Example def count_to_n(n: int) -> int: \"\"\"Generate integers up to *n*. Args: n: The number at which to stop counting. \"\"\" for i in range(1, n + 1): yield i Use instead: def count_to_n(n: int) -> int: \"\"\"Generate integers up to *n*. Args: n: The number at which to stop counting. Yields: int: The number we're at in the count. \"\"\" for i in range(1, n + 1): yield i",
    "option": {
      "lint.pydoclint.ignore-one-line-docstrings": {
        "option_description": "Skip docstrings which fit on a single line.\n\nNote: The corresponding setting in pydoclint\nis named skip-checking-short-docstrings.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydoclint]\n# Skip docstrings which fit on a single line.\nignore-one-line-docstrings = true\n\n\n\n[lint.pydoclint]\n# Skip docstrings which fit on a single line.\nignore-one-line-docstrings = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pydocstyle.convention": {
        "option_description": "Whether to use Google-style, NumPy-style conventions, or the PEP 257\ndefaults when analyzing docstring sections.\n\nEnabling a convention will disable all rules that are not included in\nthe specified convention. As such, the intended workflow is to enable a\nconvention and then selectively enable or disable any additional rules\non top of it.\n\nFor example, to use Google-style conventions but avoid requiring\ndocumentation for every function parameter:\n\n[tool.ruff.lint]\n# Enable all `pydocstyle` rules, limiting to those that adhere to the\n# Google convention via `convention = \"google\"`, below.\nselect = [\"D\"]\n\n# On top of the Google convention, disable `D417`, which requires\n# documentation for every function parameter.\nignore = [\"D417\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\nTo enable an additional rule that's excluded from the convention,\nselect the desired rule via its fully qualified rule code (e.g.,\nD400 instead of D4 or D40):\n\n[tool.ruff.lint]\n# Enable D400 on top of the Google convention.\nextend-select = [\"D400\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"\n\n\n\n[lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"",
        "option_data_type": "\"google\" | \"numpy\" | \"pep257\"",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "DOC403": {
    "description": "[Status: preview] [Name: docstring-extraneous-yields] [Message: Docstring has a \"Yields\" section but the function doesn't yield anything] What it does Checks for function docstrings with unnecessary \"Yields\" sections. Why is this bad? A function that doesn't yield anything should not have a \"Yields\" section in its docstring. This rule is not enforced for abstract methods. It is also ignored for \"stub functions\": functions where the body only consists of pass, ..., raise NotImplementedError, or similar. Example def say_hello(n: int) -> None: \"\"\"Says hello to the user. Args: n: Number of times to say hello. Yields: Doesn't yield anything. \"\"\" for _ in range(n): print(\"Hello!\") Use instead: def say_hello(n: int) -> None: \"\"\"Says hello to the user. Args: n: Number of times to say hello. \"\"\" for _ in range(n): print(\"Hello!\")",
    "option": {
      "lint.pydoclint.ignore-one-line-docstrings": {
        "option_description": "Skip docstrings which fit on a single line.\n\nNote: The corresponding setting in pydoclint\nis named skip-checking-short-docstrings.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydoclint]\n# Skip docstrings which fit on a single line.\nignore-one-line-docstrings = true\n\n\n\n[lint.pydoclint]\n# Skip docstrings which fit on a single line.\nignore-one-line-docstrings = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pydocstyle.convention": {
        "option_description": "Whether to use Google-style, NumPy-style conventions, or the PEP 257\ndefaults when analyzing docstring sections.\n\nEnabling a convention will disable all rules that are not included in\nthe specified convention. As such, the intended workflow is to enable a\nconvention and then selectively enable or disable any additional rules\non top of it.\n\nFor example, to use Google-style conventions but avoid requiring\ndocumentation for every function parameter:\n\n[tool.ruff.lint]\n# Enable all `pydocstyle` rules, limiting to those that adhere to the\n# Google convention via `convention = \"google\"`, below.\nselect = [\"D\"]\n\n# On top of the Google convention, disable `D417`, which requires\n# documentation for every function parameter.\nignore = [\"D417\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\nTo enable an additional rule that's excluded from the convention,\nselect the desired rule via its fully qualified rule code (e.g.,\nD400 instead of D4 or D40):\n\n[tool.ruff.lint]\n# Enable D400 on top of the Google convention.\nextend-select = [\"D400\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"\n\n\n\n[lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"",
        "option_data_type": "\"google\" | \"numpy\" | \"pep257\"",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "DOC501": {
    "description": "[Status: preview] [Name: docstring-missing-exception] [Message: Raised exception <code>{id}</code> missing from docstring] What it does Checks for function docstrings that do not document all explicitly raised exceptions. Why is this bad? A function should document all exceptions that are directly raised in some circumstances. Failing to document an exception that could be raised can be misleading to users and/or a sign of incomplete documentation. This rule is not enforced for abstract methods. It is also ignored for \"stub functions\": functions where the body only consists of pass, ..., raise NotImplementedError, or similar. Example class FasterThanLightError(ArithmeticError): ... def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Args: distance: Distance traveled. time: Time spent traveling. Returns: Speed as distance divided by time. \"\"\" try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc Use instead: class FasterThanLightError(ArithmeticError): ... def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Args: distance: Distance traveled. time: Time spent traveling. Returns: Speed as distance divided by time. Raises: FasterThanLightError: If speed is greater than the speed of light. \"\"\" try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc",
    "option": {
      "lint.pydoclint.ignore-one-line-docstrings": {
        "option_description": "Skip docstrings which fit on a single line.\n\nNote: The corresponding setting in pydoclint\nis named skip-checking-short-docstrings.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydoclint]\n# Skip docstrings which fit on a single line.\nignore-one-line-docstrings = true\n\n\n\n[lint.pydoclint]\n# Skip docstrings which fit on a single line.\nignore-one-line-docstrings = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pydocstyle.convention": {
        "option_description": "Whether to use Google-style, NumPy-style conventions, or the PEP 257\ndefaults when analyzing docstring sections.\n\nEnabling a convention will disable all rules that are not included in\nthe specified convention. As such, the intended workflow is to enable a\nconvention and then selectively enable or disable any additional rules\non top of it.\n\nFor example, to use Google-style conventions but avoid requiring\ndocumentation for every function parameter:\n\n[tool.ruff.lint]\n# Enable all `pydocstyle` rules, limiting to those that adhere to the\n# Google convention via `convention = \"google\"`, below.\nselect = [\"D\"]\n\n# On top of the Google convention, disable `D417`, which requires\n# documentation for every function parameter.\nignore = [\"D417\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\nTo enable an additional rule that's excluded from the convention,\nselect the desired rule via its fully qualified rule code (e.g.,\nD400 instead of D4 or D40):\n\n[tool.ruff.lint]\n# Enable D400 on top of the Google convention.\nextend-select = [\"D400\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"\n\n\n\n[lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"",
        "option_data_type": "\"google\" | \"numpy\" | \"pep257\"",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "DOC502": {
    "description": "[Status: preview] [Name: docstring-extraneous-exception] [Message: Raised exception is not explicitly raised: <code>{id}</code>] What it does Checks for function docstrings that state that exceptions could be raised even though they are not directly raised in the function body. Why is this bad? Some conventions prefer non-explicit exceptions be omitted from the docstring. This rule is not enforced for abstract methods. It is also ignored for \"stub functions\": functions where the body only consists of pass, ..., raise NotImplementedError, or similar. Example def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Args: distance: Distance traveled. time: Time spent traveling. Returns: Speed as distance divided by time. Raises: ZeroDivisionError: Divided by zero. \"\"\" return distance / time Use instead: def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Args: distance: Distance traveled. time: Time spent traveling. Returns: Speed as distance divided by time. \"\"\" return distance / time Known issues It may often be desirable to document all exceptions that a function could possibly raise, even those which are not explicitly raised using raise statements in the function body.",
    "option": {
      "lint.pydoclint.ignore-one-line-docstrings": {
        "option_description": "Skip docstrings which fit on a single line.\n\nNote: The corresponding setting in pydoclint\nis named skip-checking-short-docstrings.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydoclint]\n# Skip docstrings which fit on a single line.\nignore-one-line-docstrings = true\n\n\n\n[lint.pydoclint]\n# Skip docstrings which fit on a single line.\nignore-one-line-docstrings = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pydocstyle.convention": {
        "option_description": "Whether to use Google-style, NumPy-style conventions, or the PEP 257\ndefaults when analyzing docstring sections.\n\nEnabling a convention will disable all rules that are not included in\nthe specified convention. As such, the intended workflow is to enable a\nconvention and then selectively enable or disable any additional rules\non top of it.\n\nFor example, to use Google-style conventions but avoid requiring\ndocumentation for every function parameter:\n\n[tool.ruff.lint]\n# Enable all `pydocstyle` rules, limiting to those that adhere to the\n# Google convention via `convention = \"google\"`, below.\nselect = [\"D\"]\n\n# On top of the Google convention, disable `D417`, which requires\n# documentation for every function parameter.\nignore = [\"D417\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\nTo enable an additional rule that's excluded from the convention,\nselect the desired rule via its fully qualified rule code (e.g.,\nD400 instead of D4 or D40):\n\n[tool.ruff.lint]\n# Enable D400 on top of the Google convention.\nextend-select = [\"D400\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"\n\n\n\n[lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"",
        "option_data_type": "\"google\" | \"numpy\" | \"pep257\"",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D100": {
    "description": "[Status: stable] [Name: undocumented-public-module] [Message: Missing docstring in public module] What it does Checks for undocumented public module definitions. Why is this bad? Public modules should be documented via docstrings to outline their purpose and contents. Generally, module docstrings should describe the purpose of the module and list the classes, exceptions, functions, and other objects that are exported by the module, alongside a one-line summary of each. If the module is a script, the docstring should be usable as its \"usage\" message. If the codebase adheres to a standard format for module docstrings, follow that format for consistency. Example class FasterThanLightError(ZeroDivisionError): ... def calculate_speed(distance: float, time: float) -> float: ... Use instead: \"\"\"Utility functions and classes for calculating speed. This module provides: - FasterThanLightError: exception when FTL speed is calculated; - calculate_speed: calculate speed given distance and time. \"\"\" class FasterThanLightError(ZeroDivisionError): ... def calculate_speed(distance: float, time: float) -> float: ... Notebook behavior This rule is ignored for Jupyter Notebooks.",
    "option": {
      "lint.pydocstyle.ignore-decorators": {
        "option_description": "Ignore docstrings for functions or methods decorated with the\nspecified fully-qualified decorators.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]\n\n\n\n[lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D101": {
    "description": "[Status: stable] [Name: undocumented-public-class] [Message: Missing docstring in public class] What it does Checks for undocumented public class definitions. Why is this bad? Public classes should be documented via docstrings to outline their purpose and behavior. Generally, a class docstring should describe the class's purpose and list its public attributes and methods. If the codebase adheres to a standard format for class docstrings, follow that format for consistency. Example class Player: def __init__(self, name: str, points: int = 0) -> None: self.name: str = name self.points: int = points def add_points(self, points: int) -> None: self.points += points Use instead (in the NumPy docstring format): class Player: \"\"\"A player in the game. Attributes ---------- name : str The name of the player. points : int The number of points the player has. Methods ------- add_points(points: int) -> None Add points to the player's score. \"\"\" def __init__(self, name: str, points: int = 0) -> None: self.name: str = name self.points: int = points def add_points(self, points: int) -> None: self.points += points Or (in the Google docstring format): class Player: \"\"\"A player in the game. Attributes: name: The name of the player. points: The number of points the player has. \"\"\" def __init__(self, name: str, points: int = 0) -> None: self.name: str = name self.points: int = points def add_points(self, points: int) -> None: self.points += points",
    "option": {
      "lint.pydocstyle.ignore-decorators": {
        "option_description": "Ignore docstrings for functions or methods decorated with the\nspecified fully-qualified decorators.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]\n\n\n\n[lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D102": {
    "description": "[Status: stable] [Name: undocumented-public-method] [Message: Missing docstring in public method] What it does Checks for undocumented public method definitions. Why is this bad? Public methods should be documented via docstrings to outline their purpose and behavior. Generally, a method docstring should describe the method's behavior, arguments, side effects, exceptions, return values, and any other information that may be relevant to the user. If the codebase adheres to a standard format for method docstrings, follow that format for consistency. This rule exempts methods decorated with @typing.override, since it is a common practice to document a method on a superclass but not on an overriding method in a subclass. Example class Cat(Animal): def greet(self, happy: bool = True): if happy: print(\"Meow!\") else: raise ValueError(\"Tried to greet an unhappy cat.\") Use instead (in the NumPy docstring format): class Cat(Animal): def greet(self, happy: bool = True): \"\"\"Print a greeting from the cat. Parameters ---------- happy : bool, optional Whether the cat is happy, is True by default. Raises ------ ValueError If the cat is not happy. \"\"\" if happy: print(\"Meow!\") else: raise ValueError(\"Tried to greet an unhappy cat.\") Or (in the Google docstring format): class Cat(Animal): def greet(self, happy: bool = True): \"\"\"Print a greeting from the cat. Args: happy: Whether the cat is happy, is True by default. Raises: ValueError: If the cat is not happy. \"\"\" if happy: print(\"Meow!\") else: raise ValueError(\"Tried to greet an unhappy cat.\")",
    "option": {
      "lint.pydocstyle.ignore-decorators": {
        "option_description": "Ignore docstrings for functions or methods decorated with the\nspecified fully-qualified decorators.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]\n\n\n\n[lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D103": {
    "description": "[Status: stable] [Name: undocumented-public-function] [Message: Missing docstring in public function] What it does Checks for undocumented public function definitions. Why is this bad? Public functions should be documented via docstrings to outline their purpose and behavior. Generally, a function docstring should describe the function's behavior, arguments, side effects, exceptions, return values, and any other information that may be relevant to the user. If the codebase adheres to a standard format for function docstrings, follow that format for consistency. Example def calculate_speed(distance: float, time: float) -> float: try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc Use instead (using the NumPy docstring format): def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Parameters ---------- distance : float Distance traveled. time : float Time spent traveling. Returns ------- float Speed as distance divided by time. Raises ------ FasterThanLightError If speed is greater than the speed of light. \"\"\" try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc Or, using the Google docstring format: def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Args: distance: Distance traveled. time: Time spent traveling. Returns: Speed as distance divided by time. Raises: FasterThanLightError: If speed is greater than the speed of light. \"\"\" try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc",
    "option": {
      "lint.pydocstyle.ignore-decorators": {
        "option_description": "Ignore docstrings for functions or methods decorated with the\nspecified fully-qualified decorators.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]\n\n\n\n[lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D104": {
    "description": "[Status: stable] [Name: undocumented-public-package] [Message: Missing docstring in public package] What it does Checks for undocumented public package definitions. Why is this bad? Public packages should be documented via docstrings to outline their purpose and contents. Generally, package docstrings should list the modules and subpackages that are exported by the package. If the codebase adheres to a standard format for package docstrings, follow that format for consistency. Example __all__ = [\"Player\", \"Game\"] Use instead: \"\"\"Game and player management package. This package provides classes for managing players and games. \"\"\" __all__ = [\"player\", \"game\"]",
    "option": {
      "lint.pydocstyle.ignore-decorators": {
        "option_description": "Ignore docstrings for functions or methods decorated with the\nspecified fully-qualified decorators.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]\n\n\n\n[lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D105": {
    "description": "[Status: stable] [Name: undocumented-magic-method] [Message: Missing docstring in magic method] What it does Checks for undocumented magic method definitions. Why is this bad? Magic methods (methods with names that start and end with double underscores) are used to implement operator overloading and other special behavior. Such methods should be documented via docstrings to outline their behavior. Generally, magic method docstrings should describe the method's behavior, arguments, side effects, exceptions, return values, and any other information that may be relevant to the user. If the codebase adheres to a standard format for method docstrings, follow that format for consistency. Example class Cat(Animal): def __str__(self) -> str: return f\"Cat: {self.name}\" cat = Cat(\"Dusty\") print(cat) # \"Cat: Dusty\" Use instead: class Cat(Animal): def __str__(self) -> str: \"\"\"Return a string representation of the cat.\"\"\" return f\"Cat: {self.name}\" cat = Cat(\"Dusty\") print(cat) # \"Cat: Dusty\"",
    "option": {
      "lint.pydocstyle.ignore-decorators": {
        "option_description": "Ignore docstrings for functions or methods decorated with the\nspecified fully-qualified decorators.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]\n\n\n\n[lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D106": {
    "description": "[Status: stable] [Name: undocumented-public-nested-class] [Message: Missing docstring in public nested class] What it does Checks for undocumented public class definitions, for nested classes. Why is this bad? Public classes should be documented via docstrings to outline their purpose and behavior. Nested classes do not inherit the docstring of their enclosing class, so they should have their own docstrings. If the codebase adheres to a standard format for class docstrings, follow that format for consistency. Example class Foo: \"\"\"Class Foo.\"\"\" class Bar: ... bar = Foo.Bar() bar.__doc__ # None Use instead: class Foo: \"\"\"Class Foo.\"\"\" class Bar: \"\"\"Class Bar.\"\"\" bar = Foo.Bar() bar.__doc__ # \"Class Bar.\"",
    "option": {
      "lint.pydocstyle.ignore-decorators": {
        "option_description": "Ignore docstrings for functions or methods decorated with the\nspecified fully-qualified decorators.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]\n\n\n\n[lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D107": {
    "description": "[Status: stable] [Name: undocumented-public-init] [Message: Missing docstring in <code>__init__</code>] What it does Checks for public __init__ method definitions that are missing docstrings. Why is this bad? Public __init__ methods are used to initialize objects. __init__ methods should be documented via docstrings to describe the method's behavior, arguments, side effects, exceptions, and any other information that may be relevant to the user. If the codebase adheres to a standard format for __init__ method docstrings, follow that format for consistency. Example class City: def __init__(self, name: str, population: int) -> None: self.name: str = name self.population: int = population Use instead: class City: def __init__(self, name: str, population: int) -> None: \"\"\"Initialize a city with a name and population.\"\"\" self.name: str = name self.population: int = population",
    "option": {
      "lint.pydocstyle.ignore-decorators": {
        "option_description": "Ignore docstrings for functions or methods decorated with the\nspecified fully-qualified decorators.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]\n\n\n\n[lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D200": {
    "description": "[Status: stable] [Name: unnecessary-multiline-docstring] [Message: One-line docstring should fit on one line] What it does Checks for single-line docstrings that are broken across multiple lines. Why is this bad? PEP 257 recommends that docstrings that can fit on one line should be formatted on a single line, for consistency and readability. Example def average(values: list[float]) -> float: \"\"\" Return the mean of the given values. \"\"\" Use instead: def average(values: list[float]) -> float: \"\"\"Return the mean of the given values.\"\"\" Fix safety The fix is marked as unsafe because it could affect tools that parse docstrings, documentation generators, or custom introspection utilities that rely on specific docstring formatting.",
    "option": {
      "lint.pydocstyle.ignore-decorators": {
        "option_description": "Ignore docstrings for functions or methods decorated with the\nspecified fully-qualified decorators.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]\n\n\n\n[lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D201": {
    "description": "[Status: stable] [Name: blank-line-before-function] [Message: No blank lines allowed before function docstring (found {num_lines})] What it does Checks for docstrings on functions that are separated by one or more blank lines from the function definition. Why is this bad? Remove any blank lines between the function definition and its docstring, for consistency. Example def average(values: list[float]) -> float: \"\"\"Return the mean of the given values.\"\"\" Use instead: def average(values: list[float]) -> float: \"\"\"Return the mean of the given values.\"\"\"",
    "option": {
      "lint.pydocstyle.ignore-decorators": {
        "option_description": "Ignore docstrings for functions or methods decorated with the\nspecified fully-qualified decorators.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]\n\n\n\n[lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D202": {
    "description": "[Status: stable] [Name: blank-line-after-function] [Message: No blank lines allowed after function docstring (found {num_lines})] What it does Checks for docstrings on functions that are separated by one or more blank lines from the function body. Why is this bad? Remove any blank lines between the function body and the function docstring, for consistency. Example def average(values: list[float]) -> float: \"\"\"Return the mean of the given values.\"\"\" return sum(values) / len(values) Use instead: def average(values: list[float]) -> float: \"\"\"Return the mean of the given values.\"\"\" return sum(values) / len(values)",
    "option": {
      "lint.pydocstyle.ignore-decorators": {
        "option_description": "Ignore docstrings for functions or methods decorated with the\nspecified fully-qualified decorators.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]\n\n\n\n[lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D203": {
    "description": "[Status: stable] [Name: incorrect-blank-line-before-class] [Message: 1 blank line required before class docstring] What it does Checks for docstrings on class definitions that are not preceded by a blank line. Why is this bad? Use a blank line to separate the docstring from the class definition, for consistency. This rule may not apply to all projects; its applicability is a matter of convention. By default, this rule is disabled when using the google, numpy, and pep257 conventions. For an alternative, see D211. Example class PhotoMetadata: \"\"\"Metadata about a photo.\"\"\" Use instead: class PhotoMetadata: \"\"\"Metadata about a photo.\"\"\"",
    "option": {
      "lint.pydocstyle.convention": {
        "option_description": "Whether to use Google-style, NumPy-style conventions, or the PEP 257\ndefaults when analyzing docstring sections.\n\nEnabling a convention will disable all rules that are not included in\nthe specified convention. As such, the intended workflow is to enable a\nconvention and then selectively enable or disable any additional rules\non top of it.\n\nFor example, to use Google-style conventions but avoid requiring\ndocumentation for every function parameter:\n\n[tool.ruff.lint]\n# Enable all `pydocstyle` rules, limiting to those that adhere to the\n# Google convention via `convention = \"google\"`, below.\nselect = [\"D\"]\n\n# On top of the Google convention, disable `D417`, which requires\n# documentation for every function parameter.\nignore = [\"D417\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\nTo enable an additional rule that's excluded from the convention,\nselect the desired rule via its fully qualified rule code (e.g.,\nD400 instead of D4 or D40):\n\n[tool.ruff.lint]\n# Enable D400 on top of the Google convention.\nextend-select = [\"D400\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"\n\n\n\n[lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"",
        "option_data_type": "\"google\" | \"numpy\" | \"pep257\"",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D204": {
    "description": "[Status: stable] [Name: incorrect-blank-line-after-class] [Message: 1 blank line required after class docstring] What it does Checks for class methods that are not separated from the class's docstring by a blank line. Why is this bad? PEP 257 recommends the use of a blank line to separate a class's docstring from its methods. This rule may not apply to all projects; its applicability is a matter of convention. By default, this rule is enabled when using the numpy and pep257 conventions, and disabled when using the google convention. Example class PhotoMetadata: \"\"\"Metadata about a photo.\"\"\" def __init__(self, file: Path): ... Use instead: class PhotoMetadata: \"\"\"Metadata about a photo.\"\"\" def __init__(self, file: Path): ...",
    "option": {
      "lint.pydocstyle.convention": {
        "option_description": "Whether to use Google-style, NumPy-style conventions, or the PEP 257\ndefaults when analyzing docstring sections.\n\nEnabling a convention will disable all rules that are not included in\nthe specified convention. As such, the intended workflow is to enable a\nconvention and then selectively enable or disable any additional rules\non top of it.\n\nFor example, to use Google-style conventions but avoid requiring\ndocumentation for every function parameter:\n\n[tool.ruff.lint]\n# Enable all `pydocstyle` rules, limiting to those that adhere to the\n# Google convention via `convention = \"google\"`, below.\nselect = [\"D\"]\n\n# On top of the Google convention, disable `D417`, which requires\n# documentation for every function parameter.\nignore = [\"D417\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\nTo enable an additional rule that's excluded from the convention,\nselect the desired rule via its fully qualified rule code (e.g.,\nD400 instead of D4 or D40):\n\n[tool.ruff.lint]\n# Enable D400 on top of the Google convention.\nextend-select = [\"D400\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"\n\n\n\n[lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"",
        "option_data_type": "\"google\" | \"numpy\" | \"pep257\"",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D205": {
    "description": "[Status: stable] [Name: missing-blank-line-after-summary] [Message: 1 blank line required between summary line and description] What it does Checks for docstring summary lines that are not separated from the docstring description by one blank line. Why is this bad? PEP 257 recommends that multi-line docstrings consist of \"a summary line just like a one-line docstring, followed by a blank line, followed by a more elaborate description.\" Example def sort_list(l: list[int]) -> list[int]: \"\"\"Return a sorted copy of the list. Sort the list in ascending order and return a copy of the result using the bubble sort algorithm. \"\"\" Use instead: def sort_list(l: list[int]) -> list[int]: \"\"\"Return a sorted copy of the list. Sort the list in ascending order and return a copy of the result using the bubble sort algorithm. \"\"\"",
    "option": {
      "lint.pydocstyle.ignore-decorators": {
        "option_description": "Ignore docstrings for functions or methods decorated with the\nspecified fully-qualified decorators.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]\n\n\n\n[lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D206": {
    "description": "[Status: stable] [Name: docstring-tab-indentation] [Message: Docstring should be indented with spaces, not tabs] What it does Checks for docstrings that are indented with tabs. Why is this bad? PEP 8 recommends using spaces over tabs for indentation. Example def sort_list(l: list[int]) -> list[int]: \"\"\"Return a sorted copy of the list. Sort the list in ascending order and return a copy of the result using the bubble sort algorithm. \"\"\" Use instead: def sort_list(l: list[int]) -> list[int]: \"\"\"Return a sorted copy of the list. Sort the list in ascending order and return a copy of the result using the bubble sort algorithm. \"\"\" Formatter compatibility We recommend against using this rule alongside the formatter. The formatter enforces consistent indentation, making the rule redundant. The rule is also incompatible with the formatter when using format.indent-style=\"tab\".",
    "option": {
      "lint.pydocstyle.ignore-decorators": {
        "option_description": "Ignore docstrings for functions or methods decorated with the\nspecified fully-qualified decorators.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]\n\n\n\n[lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D207": {
    "description": "[Status: stable] [Name: under-indentation] [Message: Docstring is under-indented] What it does Checks for under-indented docstrings. Why is this bad? PEP 257 recommends that docstrings be indented to the same level as their opening quotes. Avoid under-indenting docstrings, for consistency. Example def sort_list(l: list[int]) -> list[int]: \"\"\"Return a sorted copy of the list. Sort the list in ascending order and return a copy of the result using the bubble sort algorithm. \"\"\" Use instead: def sort_list(l: list[int]) -> list[int]: \"\"\"Return a sorted copy of the list. Sort the list in ascending order and return a copy of the result using the bubble sort algorithm. \"\"\" Formatter compatibility We recommend against using this rule alongside the formatter. The formatter enforces consistent indentation, making the rule redundant.",
    "option": {
      "lint.pydocstyle.ignore-decorators": {
        "option_description": "Ignore docstrings for functions or methods decorated with the\nspecified fully-qualified decorators.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]\n\n\n\n[lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D208": {
    "description": "[Status: stable] [Name: over-indentation] [Message: Docstring is over-indented] What it does Checks for over-indented docstrings. Why is this bad? PEP 257 recommends that docstrings be indented to the same level as their opening quotes. Avoid over-indenting docstrings, for consistency. Example def sort_list(l: list[int]) -> list[int]: \"\"\"Return a sorted copy of the list. Sort the list in ascending order and return a copy of the result using the bubble sort algorithm. \"\"\" Use instead: def sort_list(l: list[int]) -> list[int]: \"\"\"Return a sorted copy of the list. Sort the list in ascending order and return a copy of the result using the bubble sort algorithm. \"\"\" Formatter compatibility We recommend against using this rule alongside the formatter. The formatter enforces consistent indentation, making the rule redundant.",
    "option": {
      "lint.pydocstyle.ignore-decorators": {
        "option_description": "Ignore docstrings for functions or methods decorated with the\nspecified fully-qualified decorators.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]\n\n\n\n[lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D209": {
    "description": "[Status: stable] [Name: new-line-after-last-paragraph] [Message: Multi-line docstring closing quotes should be on a separate line] What it does Checks for multi-line docstrings whose closing quotes are not on their own line. Why is this bad? PEP 257 recommends that the closing quotes of a multi-line docstring be on their own line, for consistency and compatibility with documentation tools that may need to parse the docstring. Example def sort_list(l: List[int]) -> List[int]: \"\"\"Return a sorted copy of the list. Sort the list in ascending order and return a copy of the result using the bubble sort algorithm.\"\"\" Use instead: def sort_list(l: List[int]) -> List[int]: \"\"\"Return a sorted copy of the list. Sort the list in ascending order and return a copy of the result using the bubble sort algorithm. \"\"\"",
    "option": {
      "lint.pydocstyle.ignore-decorators": {
        "option_description": "Ignore docstrings for functions or methods decorated with the\nspecified fully-qualified decorators.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]\n\n\n\n[lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D210": {
    "description": "[Status: stable] [Name: surrounding-whitespace] [Message: No whitespaces allowed surrounding docstring text] What it does Checks for surrounding whitespace in docstrings. Why is this bad? Remove surrounding whitespace from the docstring, for consistency. Example def factorial(n: int) -> int: \"\"\" Return the factorial of n. \"\"\" Use instead: def factorial(n: int) -> int: \"\"\"Return the factorial of n.\"\"\"",
    "option": {
      "lint.pydocstyle.ignore-decorators": {
        "option_description": "Ignore docstrings for functions or methods decorated with the\nspecified fully-qualified decorators.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]\n\n\n\n[lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D211": {
    "description": "[Status: stable] [Name: blank-line-before-class] [Message: No blank lines allowed before class docstring] What it does Checks for docstrings on class definitions that are preceded by a blank line. Why is this bad? Avoid introducing any blank lines between a class definition and its docstring, for consistency. This rule may not apply to all projects; its applicability is a matter of convention. By default, this rule is enabled when using the google, numpy, and pep257 conventions. For an alternative, see D203. Example class PhotoMetadata: \"\"\"Metadata about a photo.\"\"\" Use instead: class PhotoMetadata: \"\"\"Metadata about a photo.\"\"\"",
    "option": {
      "lint.pydocstyle.convention": {
        "option_description": "Whether to use Google-style, NumPy-style conventions, or the PEP 257\ndefaults when analyzing docstring sections.\n\nEnabling a convention will disable all rules that are not included in\nthe specified convention. As such, the intended workflow is to enable a\nconvention and then selectively enable or disable any additional rules\non top of it.\n\nFor example, to use Google-style conventions but avoid requiring\ndocumentation for every function parameter:\n\n[tool.ruff.lint]\n# Enable all `pydocstyle` rules, limiting to those that adhere to the\n# Google convention via `convention = \"google\"`, below.\nselect = [\"D\"]\n\n# On top of the Google convention, disable `D417`, which requires\n# documentation for every function parameter.\nignore = [\"D417\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\nTo enable an additional rule that's excluded from the convention,\nselect the desired rule via its fully qualified rule code (e.g.,\nD400 instead of D4 or D40):\n\n[tool.ruff.lint]\n# Enable D400 on top of the Google convention.\nextend-select = [\"D400\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"\n\n\n\n[lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"",
        "option_data_type": "\"google\" | \"numpy\" | \"pep257\"",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D212": {
    "description": "[Status: stable] [Name: multi-line-summary-first-line] [Message: Multi-line docstring summary should start at the first line] What it does Checks for docstring summary lines that are not positioned on the first physical line of the docstring. Why is this bad? PEP 257 recommends that multi-line docstrings consist of \"a summary line just like a one-line docstring, followed by a blank line, followed by a more elaborate description.\" The summary line should be located on the first physical line of the docstring, immediately after the opening quotes. This rule may not apply to all projects; its applicability is a matter of convention. By default, this rule is enabled when using the google convention, and disabled when using the numpy and pep257 conventions. For an alternative, see D213. Example def sort_list(l: list[int]) -> list[int]: \"\"\" Return a sorted copy of the list. Sort the list in ascending order and return a copy of the result using the bubble sort algorithm. \"\"\" Use instead: def sort_list(l: list[int]) -> list[int]: \"\"\"Return a sorted copy of the list. Sort the list in ascending order and return a copy of the result using the bubble sort algorithm. \"\"\"",
    "option": {
      "lint.pydocstyle.convention": {
        "option_description": "Whether to use Google-style, NumPy-style conventions, or the PEP 257\ndefaults when analyzing docstring sections.\n\nEnabling a convention will disable all rules that are not included in\nthe specified convention. As such, the intended workflow is to enable a\nconvention and then selectively enable or disable any additional rules\non top of it.\n\nFor example, to use Google-style conventions but avoid requiring\ndocumentation for every function parameter:\n\n[tool.ruff.lint]\n# Enable all `pydocstyle` rules, limiting to those that adhere to the\n# Google convention via `convention = \"google\"`, below.\nselect = [\"D\"]\n\n# On top of the Google convention, disable `D417`, which requires\n# documentation for every function parameter.\nignore = [\"D417\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\nTo enable an additional rule that's excluded from the convention,\nselect the desired rule via its fully qualified rule code (e.g.,\nD400 instead of D4 or D40):\n\n[tool.ruff.lint]\n# Enable D400 on top of the Google convention.\nextend-select = [\"D400\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"\n\n\n\n[lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"",
        "option_data_type": "\"google\" | \"numpy\" | \"pep257\"",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D213": {
    "description": "[Status: stable] [Name: multi-line-summary-second-line] [Message: Multi-line docstring summary should start at the second line] What it does Checks for docstring summary lines that are not positioned on the second physical line of the docstring. Why is this bad? PEP 257 recommends that multi-line docstrings consist of \"a summary line just like a one-line docstring, followed by a blank line, followed by a more elaborate description.\" The summary line should be located on the second physical line of the docstring, immediately after the opening quotes and the blank line. This rule may not apply to all projects; its applicability is a matter of convention. By default, this rule is disabled when using the google, numpy, and pep257 conventions. For an alternative, see D212. Example def sort_list(l: list[int]) -> list[int]: \"\"\"Return a sorted copy of the list. Sort the list in ascending order and return a copy of the result using the bubble sort algorithm. \"\"\" Use instead: def sort_list(l: list[int]) -> list[int]: \"\"\" Return a sorted copy of the list. Sort the list in ascending order and return a copy of the result using the bubble sort algorithm. \"\"\"",
    "option": {
      "lint.pydocstyle.convention": {
        "option_description": "Whether to use Google-style, NumPy-style conventions, or the PEP 257\ndefaults when analyzing docstring sections.\n\nEnabling a convention will disable all rules that are not included in\nthe specified convention. As such, the intended workflow is to enable a\nconvention and then selectively enable or disable any additional rules\non top of it.\n\nFor example, to use Google-style conventions but avoid requiring\ndocumentation for every function parameter:\n\n[tool.ruff.lint]\n# Enable all `pydocstyle` rules, limiting to those that adhere to the\n# Google convention via `convention = \"google\"`, below.\nselect = [\"D\"]\n\n# On top of the Google convention, disable `D417`, which requires\n# documentation for every function parameter.\nignore = [\"D417\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\nTo enable an additional rule that's excluded from the convention,\nselect the desired rule via its fully qualified rule code (e.g.,\nD400 instead of D4 or D40):\n\n[tool.ruff.lint]\n# Enable D400 on top of the Google convention.\nextend-select = [\"D400\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"\n\n\n\n[lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"",
        "option_data_type": "\"google\" | \"numpy\" | \"pep257\"",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D214": {
    "description": "[Status: stable] [Name: overindented-section] [Message: Section is over-indented (\"{name}\")] What it does Checks for over-indented sections in docstrings. Why is this bad? This rule enforces a consistent style for docstrings with multiple sections. Multiline docstrings are typically composed of a summary line, followed by a blank line, followed by a series of sections, each with a section header and a section body. The convention is that all sections should use consistent indentation. In each section, the header should match the indentation of the docstring's opening quotes, and the body should be indented one level further. This rule is enabled when using the numpy and google conventions, and disabled when using the pep257 convention. Example def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Args: distance: Distance traveled. time: Time spent traveling. Returns: Speed as distance divided by time. Raises: FasterThanLightError: If speed is greater than the speed of light. \"\"\" try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc Use instead: def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Args: distance: Distance traveled. time: Time spent traveling. Returns: Speed as distance divided by time. Raises: FasterThanLightError: If speed is greater than the speed of light. \"\"\" try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc",
    "option": {
      "lint.pydocstyle.convention": {
        "option_description": "Whether to use Google-style, NumPy-style conventions, or the PEP 257\ndefaults when analyzing docstring sections.\n\nEnabling a convention will disable all rules that are not included in\nthe specified convention. As such, the intended workflow is to enable a\nconvention and then selectively enable or disable any additional rules\non top of it.\n\nFor example, to use Google-style conventions but avoid requiring\ndocumentation for every function parameter:\n\n[tool.ruff.lint]\n# Enable all `pydocstyle` rules, limiting to those that adhere to the\n# Google convention via `convention = \"google\"`, below.\nselect = [\"D\"]\n\n# On top of the Google convention, disable `D417`, which requires\n# documentation for every function parameter.\nignore = [\"D417\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\nTo enable an additional rule that's excluded from the convention,\nselect the desired rule via its fully qualified rule code (e.g.,\nD400 instead of D4 or D40):\n\n[tool.ruff.lint]\n# Enable D400 on top of the Google convention.\nextend-select = [\"D400\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"\n\n\n\n[lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"",
        "option_data_type": "\"google\" | \"numpy\" | \"pep257\"",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D215": {
    "description": "[Status: stable] [Name: overindented-section-underline] [Message: Section underline is over-indented (\"{name}\")] What it does Checks for over-indented section underlines in docstrings. Why is this bad? This rule enforces a consistent style for multiline numpy-style docstrings, and helps prevent incorrect syntax in docstrings using reStructuredText. Multiline numpy-style docstrings are typically composed of a summary line, followed by a blank line, followed by a series of sections. Each section has a section header and a section body, and there should be a series of underline characters in the line following the header. The underline should have the same indentation as the header. This rule enforces a consistent style for multiline numpy-style docstrings with sections. If your docstring uses reStructuredText, the rule also helps protect against incorrect reStructuredText syntax, which would cause errors if you tried to use a tool such as Sphinx to generate documentation from the docstring. This rule is enabled when using the numpy convention, and disabled when using the google or pep257 conventions. Example def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Parameters ---------- distance : float Distance traveled. time : float Time spent traveling. Returns ------- float Speed as distance divided by time. Raises ------ FasterThanLightError If speed is greater than the speed of light. \"\"\" try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc Use instead: def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Parameters ---------- distance : float Distance traveled. time : float Time spent traveling. Returns ------- float Speed as distance divided by time. Raises ------ FasterThanLightError If speed is greater than the speed of light. \"\"\" try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc",
    "option": {
      "lint.pydocstyle.convention": {
        "option_description": "Whether to use Google-style, NumPy-style conventions, or the PEP 257\ndefaults when analyzing docstring sections.\n\nEnabling a convention will disable all rules that are not included in\nthe specified convention. As such, the intended workflow is to enable a\nconvention and then selectively enable or disable any additional rules\non top of it.\n\nFor example, to use Google-style conventions but avoid requiring\ndocumentation for every function parameter:\n\n[tool.ruff.lint]\n# Enable all `pydocstyle` rules, limiting to those that adhere to the\n# Google convention via `convention = \"google\"`, below.\nselect = [\"D\"]\n\n# On top of the Google convention, disable `D417`, which requires\n# documentation for every function parameter.\nignore = [\"D417\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\nTo enable an additional rule that's excluded from the convention,\nselect the desired rule via its fully qualified rule code (e.g.,\nD400 instead of D4 or D40):\n\n[tool.ruff.lint]\n# Enable D400 on top of the Google convention.\nextend-select = [\"D400\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"\n\n\n\n[lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"",
        "option_data_type": "\"google\" | \"numpy\" | \"pep257\"",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D300": {
    "description": "[Status: stable] [Name: triple-single-quotes] [Message: Use triple double quotes <code>\"\"\"</code>] What it does Checks for docstrings that use '''triple single quotes''' instead of \"\"\"triple double quotes\"\"\". Why is this bad? PEP 257 recommends the use of \"\"\"triple double quotes\"\"\" for docstrings, to ensure consistency. Example def kos_root(): '''Return the pathname of the KOS root directory.''' Use instead: def kos_root(): \"\"\"Return the pathname of the KOS root directory.\"\"\" Formatter compatibility We recommend against using this rule alongside the formatter. The formatter enforces consistent quotes, making the rule redundant.",
    "option": {
      "lint.pydocstyle.ignore-decorators": {
        "option_description": "Ignore docstrings for functions or methods decorated with the\nspecified fully-qualified decorators.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]\n\n\n\n[lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D301": {
    "description": "[Status: stable] [Name: escape-sequence-in-docstring] [Message: Use <code>r\"\"\"</code> if any backslashes in a docstring] What it does Checks for docstrings that include backslashes, but are not defined as raw string literals. Why is this bad? In Python, backslashes are typically used to escape characters in strings. In raw strings (those prefixed with an r), however, backslashes are treated as literal characters. PEP 257 recommends the use of raw strings (i.e., r\"\"\"raw triple double quotes\"\"\") for docstrings that include backslashes. The use of a raw string ensures that any backslashes are treated as literal characters, and not as escape sequences, which avoids confusion. Example def foobar(): \"\"\"Docstring for foo\\bar.\"\"\" foobar.__doc__ # \"Docstring for foar.\" Use instead: def foobar(): r\"\"\"Docstring for foo\\bar.\"\"\" foobar.__doc__ # \"Docstring for foo\\bar.\"",
    "option": {
      "lint.pydocstyle.ignore-decorators": {
        "option_description": "Ignore docstrings for functions or methods decorated with the\nspecified fully-qualified decorators.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]\n\n\n\n[lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D400": {
    "description": "[Status: stable] [Name: missing-trailing-period] [Message: First line should end with a period] What it does Checks for docstrings in which the first line does not end in a period. Why is this bad? PEP 257 recommends that the first line of a docstring is written in the form of a command, ending in a period. This rule may not apply to all projects; its applicability is a matter of convention. By default, this rule is enabled when using the numpy and pep257 conventions, and disabled when using the google convention. Example def average(values: list[float]) -> float: \"\"\"Return the mean of the given values\"\"\" Use instead: def average(values: list[float]) -> float: \"\"\"Return the mean of the given values.\"\"\"",
    "option": {
      "lint.pydocstyle.convention": {
        "option_description": "Whether to use Google-style, NumPy-style conventions, or the PEP 257\ndefaults when analyzing docstring sections.\n\nEnabling a convention will disable all rules that are not included in\nthe specified convention. As such, the intended workflow is to enable a\nconvention and then selectively enable or disable any additional rules\non top of it.\n\nFor example, to use Google-style conventions but avoid requiring\ndocumentation for every function parameter:\n\n[tool.ruff.lint]\n# Enable all `pydocstyle` rules, limiting to those that adhere to the\n# Google convention via `convention = \"google\"`, below.\nselect = [\"D\"]\n\n# On top of the Google convention, disable `D417`, which requires\n# documentation for every function parameter.\nignore = [\"D417\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\nTo enable an additional rule that's excluded from the convention,\nselect the desired rule via its fully qualified rule code (e.g.,\nD400 instead of D4 or D40):\n\n[tool.ruff.lint]\n# Enable D400 on top of the Google convention.\nextend-select = [\"D400\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"\n\n\n\n[lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"",
        "option_data_type": "\"google\" | \"numpy\" | \"pep257\"",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D401": {
    "description": "[Status: stable] [Name: non-imperative-mood] [Message: First line of docstring should be in imperative mood: \"{first_line}\"] What it does Checks for docstring first lines that are not in an imperative mood. Why is this bad? PEP 257 recommends that the first line of a docstring be written in the imperative mood, for consistency. Hint: to rewrite the docstring in the imperative, phrase the first line as if it were a command. This rule may not apply to all projects; its applicability is a matter of convention. By default, this rule is enabled when using the numpy and pep257 conventions, and disabled when using the google conventions. Example def average(values: list[float]) -> float: \"\"\"Returns the mean of the given values.\"\"\" Use instead: def average(values: list[float]) -> float: \"\"\"Return the mean of the given values.\"\"\"",
    "option": {
      "lint.pydocstyle.convention": {
        "option_description": "Whether to use Google-style, NumPy-style conventions, or the PEP 257\ndefaults when analyzing docstring sections.\n\nEnabling a convention will disable all rules that are not included in\nthe specified convention. As such, the intended workflow is to enable a\nconvention and then selectively enable or disable any additional rules\non top of it.\n\nFor example, to use Google-style conventions but avoid requiring\ndocumentation for every function parameter:\n\n[tool.ruff.lint]\n# Enable all `pydocstyle` rules, limiting to those that adhere to the\n# Google convention via `convention = \"google\"`, below.\nselect = [\"D\"]\n\n# On top of the Google convention, disable `D417`, which requires\n# documentation for every function parameter.\nignore = [\"D417\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\nTo enable an additional rule that's excluded from the convention,\nselect the desired rule via its fully qualified rule code (e.g.,\nD400 instead of D4 or D40):\n\n[tool.ruff.lint]\n# Enable D400 on top of the Google convention.\nextend-select = [\"D400\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"\n\n\n\n[lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"",
        "option_data_type": "\"google\" | \"numpy\" | \"pep257\"",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pydocstyle.property-decorators": {
        "option_description": "A list of decorators that, when applied to a method, indicate that the\nmethod should be treated as a property (in addition to the builtin\n@property and standard-library @functools.cached_property).\n\nFor example, Ruff will expect that any method decorated by a decorator\nin this list can use a non-imperative summary line.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\nproperty-decorators = [\"gi.repository.GObject.Property\"]\n\n\n\n[lint.pydocstyle]\nproperty-decorators = [\"gi.repository.GObject.Property\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pydocstyle.ignore-decorators": {
        "option_description": "Ignore docstrings for functions or methods decorated with the\nspecified fully-qualified decorators.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]\n\n\n\n[lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D402": {
    "description": "[Status: stable] [Name: signature-in-docstring] [Message: First line should not be the function's signature] What it does Checks for function docstrings that include the function's signature in the summary line. Why is this bad? PEP 257 recommends against including a function's signature in its docstring. Instead, consider using type annotations as a form of documentation for the function's parameters and return value. This rule may not apply to all projects; its applicability is a matter of convention. By default, this rule is enabled when using the google and pep257 conventions, and disabled when using the numpy convention. Example def foo(a, b): \"\"\"foo(a: int, b: int) -> list[int]\"\"\" Use instead: def foo(a: int, b: int) -> list[int]: \"\"\"Return a list of a and b.\"\"\"",
    "option": {
      "lint.pydocstyle.convention": {
        "option_description": "Whether to use Google-style, NumPy-style conventions, or the PEP 257\ndefaults when analyzing docstring sections.\n\nEnabling a convention will disable all rules that are not included in\nthe specified convention. As such, the intended workflow is to enable a\nconvention and then selectively enable or disable any additional rules\non top of it.\n\nFor example, to use Google-style conventions but avoid requiring\ndocumentation for every function parameter:\n\n[tool.ruff.lint]\n# Enable all `pydocstyle` rules, limiting to those that adhere to the\n# Google convention via `convention = \"google\"`, below.\nselect = [\"D\"]\n\n# On top of the Google convention, disable `D417`, which requires\n# documentation for every function parameter.\nignore = [\"D417\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\nTo enable an additional rule that's excluded from the convention,\nselect the desired rule via its fully qualified rule code (e.g.,\nD400 instead of D4 or D40):\n\n[tool.ruff.lint]\n# Enable D400 on top of the Google convention.\nextend-select = [\"D400\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"\n\n\n\n[lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"",
        "option_data_type": "\"google\" | \"numpy\" | \"pep257\"",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D403": {
    "description": "[Status: stable] [Name: first-word-uncapitalized] [Message: First word of the docstring should be capitalized: <code>{}</code> -> <code>{}</code>] What it does Checks for docstrings that do not start with a capital letter. Why is this bad? The first non-whitespace character in a docstring should be capitalized for grammatical correctness and consistency. Example def average(values: list[float]) -> float: \"\"\"return the mean of the given values.\"\"\" Use instead: def average(values: list[float]) -> float: \"\"\"Return the mean of the given values.\"\"\"",
    "option": {
      "lint.pydocstyle.ignore-decorators": {
        "option_description": "Ignore docstrings for functions or methods decorated with the\nspecified fully-qualified decorators.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]\n\n\n\n[lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D404": {
    "description": "[Status: stable] [Name: docstring-starts-with-this] [Message: First word of the docstring should not be \"This\"] What it does Checks for docstrings that start with This. Why is this bad? PEP 257 recommends that the first line of a docstring be written in the imperative mood, for consistency. Hint: to rewrite the docstring in the imperative, phrase the first line as if it were a command. This rule may not apply to all projects; its applicability is a matter of convention. By default, this rule is enabled when using the numpy convention,, and disabled when using the google and pep257 conventions. Example def average(values: list[float]) -> float: \"\"\"This function returns the mean of the given values.\"\"\" Use instead: def average(values: list[float]) -> float: \"\"\"Return the mean of the given values.\"\"\"",
    "option": {
      "lint.pydocstyle.convention": {
        "option_description": "Whether to use Google-style, NumPy-style conventions, or the PEP 257\ndefaults when analyzing docstring sections.\n\nEnabling a convention will disable all rules that are not included in\nthe specified convention. As such, the intended workflow is to enable a\nconvention and then selectively enable or disable any additional rules\non top of it.\n\nFor example, to use Google-style conventions but avoid requiring\ndocumentation for every function parameter:\n\n[tool.ruff.lint]\n# Enable all `pydocstyle` rules, limiting to those that adhere to the\n# Google convention via `convention = \"google\"`, below.\nselect = [\"D\"]\n\n# On top of the Google convention, disable `D417`, which requires\n# documentation for every function parameter.\nignore = [\"D417\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\nTo enable an additional rule that's excluded from the convention,\nselect the desired rule via its fully qualified rule code (e.g.,\nD400 instead of D4 or D40):\n\n[tool.ruff.lint]\n# Enable D400 on top of the Google convention.\nextend-select = [\"D400\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"\n\n\n\n[lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"",
        "option_data_type": "\"google\" | \"numpy\" | \"pep257\"",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D405": {
    "description": "[Status: stable] [Name: non-capitalized-section-name] [Message: Section name should be properly capitalized (\"{name}\")] What it does Checks for section headers in docstrings that do not begin with capital letters. Why is this bad? For stylistic consistency, all section headers in a docstring should be capitalized. Multiline docstrings are typically composed of a summary line, followed by a blank line, followed by a series of sections. Each section typically has a header and a body. This rule is enabled when using the numpy and google conventions, and disabled when using the pep257 convention. Example def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. args: distance: Distance traveled. time: Time spent traveling. returns: Speed as distance divided by time. raises: FasterThanLightError: If speed is greater than the speed of light. \"\"\" try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc Use instead: def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Args: distance: Distance traveled. time: Time spent traveling. Returns: Speed as distance divided by time. Raises: FasterThanLightError: If speed is greater than the speed of light. \"\"\" try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc",
    "option": {
      "lint.pydocstyle.convention": {
        "option_description": "Whether to use Google-style, NumPy-style conventions, or the PEP 257\ndefaults when analyzing docstring sections.\n\nEnabling a convention will disable all rules that are not included in\nthe specified convention. As such, the intended workflow is to enable a\nconvention and then selectively enable or disable any additional rules\non top of it.\n\nFor example, to use Google-style conventions but avoid requiring\ndocumentation for every function parameter:\n\n[tool.ruff.lint]\n# Enable all `pydocstyle` rules, limiting to those that adhere to the\n# Google convention via `convention = \"google\"`, below.\nselect = [\"D\"]\n\n# On top of the Google convention, disable `D417`, which requires\n# documentation for every function parameter.\nignore = [\"D417\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\nTo enable an additional rule that's excluded from the convention,\nselect the desired rule via its fully qualified rule code (e.g.,\nD400 instead of D4 or D40):\n\n[tool.ruff.lint]\n# Enable D400 on top of the Google convention.\nextend-select = [\"D400\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"\n\n\n\n[lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"",
        "option_data_type": "\"google\" | \"numpy\" | \"pep257\"",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D406": {
    "description": "[Status: stable] [Name: missing-new-line-after-section-name] [Message: Section name should end with a newline (\"{name}\")] What it does Checks for section headers in docstrings that are followed by non-newline characters. Why is this bad? This rule enforces a consistent style for multiline numpy-style docstrings. Multiline numpy-style docstrings are typically composed of a summary line, followed by a blank line, followed by a series of sections. Each section has a section header and a section body. The section header should be followed by a newline, rather than by some other character (like a colon). This rule is enabled when using the numpy convention, and disabled when using the google or pep257 conventions. Example # The `Parameters`, `Returns` and `Raises` section headers are all followed # by a colon in this function's docstring: def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Parameters: ----------- distance : float Distance traveled. time : float Time spent traveling. Returns: -------- float Speed as distance divided by time. Raises: ------- FasterThanLightError If speed is greater than the speed of light. \"\"\" try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc Use instead: def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Parameters ---------- distance : float Distance traveled. time : float Time spent traveling. Returns ------- float Speed as distance divided by time. Raises ------ FasterThanLightError If speed is greater than the speed of light. \"\"\" try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc",
    "option": {
      "lint.pydocstyle.convention": {
        "option_description": "Whether to use Google-style, NumPy-style conventions, or the PEP 257\ndefaults when analyzing docstring sections.\n\nEnabling a convention will disable all rules that are not included in\nthe specified convention. As such, the intended workflow is to enable a\nconvention and then selectively enable or disable any additional rules\non top of it.\n\nFor example, to use Google-style conventions but avoid requiring\ndocumentation for every function parameter:\n\n[tool.ruff.lint]\n# Enable all `pydocstyle` rules, limiting to those that adhere to the\n# Google convention via `convention = \"google\"`, below.\nselect = [\"D\"]\n\n# On top of the Google convention, disable `D417`, which requires\n# documentation for every function parameter.\nignore = [\"D417\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\nTo enable an additional rule that's excluded from the convention,\nselect the desired rule via its fully qualified rule code (e.g.,\nD400 instead of D4 or D40):\n\n[tool.ruff.lint]\n# Enable D400 on top of the Google convention.\nextend-select = [\"D400\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"\n\n\n\n[lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"",
        "option_data_type": "\"google\" | \"numpy\" | \"pep257\"",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D407": {
    "description": "[Status: stable] [Name: missing-dashed-underline-after-section] [Message: Missing dashed underline after section (\"{name}\")] What it does Checks for section headers in docstrings that are not followed by underlines. Why is this bad? This rule enforces a consistent style for multiline numpy-style docstrings, and helps prevent incorrect syntax in docstrings using reStructuredText. Multiline numpy-style docstrings are typically composed of a summary line, followed by a blank line, followed by a series of sections. Each section has a section header and a section body, and the header should be followed by a series of underline characters in the following line. This rule enforces a consistent style for multiline numpy-style docstrings with sections. If your docstring uses reStructuredText, the rule also helps protect against incorrect reStructuredText syntax, which would cause errors if you tried to use a tool such as Sphinx to generate documentation from the docstring. This rule is enabled when using the numpy convention, and disabled when using the google or pep257 conventions. Example def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Parameters distance : float Distance traveled. time : float Time spent traveling. Returns float Speed as distance divided by time. Raises FasterThanLightError If speed is greater than the speed of light. \"\"\" try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc Use instead: def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Parameters ---------- distance : float Distance traveled. time : float Time spent traveling. Returns ------- float Speed as distance divided by time. Raises ------ FasterThanLightError If speed is greater than the speed of light. \"\"\" try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc",
    "option": {
      "lint.pydocstyle.convention": {
        "option_description": "Whether to use Google-style, NumPy-style conventions, or the PEP 257\ndefaults when analyzing docstring sections.\n\nEnabling a convention will disable all rules that are not included in\nthe specified convention. As such, the intended workflow is to enable a\nconvention and then selectively enable or disable any additional rules\non top of it.\n\nFor example, to use Google-style conventions but avoid requiring\ndocumentation for every function parameter:\n\n[tool.ruff.lint]\n# Enable all `pydocstyle` rules, limiting to those that adhere to the\n# Google convention via `convention = \"google\"`, below.\nselect = [\"D\"]\n\n# On top of the Google convention, disable `D417`, which requires\n# documentation for every function parameter.\nignore = [\"D417\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\nTo enable an additional rule that's excluded from the convention,\nselect the desired rule via its fully qualified rule code (e.g.,\nD400 instead of D4 or D40):\n\n[tool.ruff.lint]\n# Enable D400 on top of the Google convention.\nextend-select = [\"D400\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"\n\n\n\n[lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"",
        "option_data_type": "\"google\" | \"numpy\" | \"pep257\"",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D408": {
    "description": "[Status: stable] [Name: missing-section-underline-after-name] [Message: Section underline should be in the line following the section's name (\"{name}\")] What it does Checks for section underlines in docstrings that are not on the line immediately following the section name. Why is this bad? This rule enforces a consistent style for multiline numpy-style docstrings, and helps prevent incorrect syntax in docstrings using reStructuredText. Multiline numpy-style docstrings are typically composed of a summary line, followed by a blank line, followed by a series of sections. Each section has a header and a body. There should be a series of underline characters in the line immediately below the header. This rule enforces a consistent style for multiline numpy-style docstrings with sections. If your docstring uses reStructuredText, the rule also helps protect against incorrect reStructuredText syntax, which would cause errors if you tried to use a tool such as Sphinx to generate documentation from the docstring. This rule is enabled when using the numpy convention, and disabled when using the google or pep257 conventions. Example def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Parameters ---------- distance : float Distance traveled. time : float Time spent traveling. Returns ------- float Speed as distance divided by time. Raises ------ FasterThanLightError If speed is greater than the speed of light. \"\"\" try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc Use instead: def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Parameters ---------- distance : float Distance traveled. time : float Time spent traveling. Returns ------- float Speed as distance divided by time. Raises ------ FasterThanLightError If speed is greater than the speed of light. \"\"\" try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc",
    "option": {
      "lint.pydocstyle.convention": {
        "option_description": "Whether to use Google-style, NumPy-style conventions, or the PEP 257\ndefaults when analyzing docstring sections.\n\nEnabling a convention will disable all rules that are not included in\nthe specified convention. As such, the intended workflow is to enable a\nconvention and then selectively enable or disable any additional rules\non top of it.\n\nFor example, to use Google-style conventions but avoid requiring\ndocumentation for every function parameter:\n\n[tool.ruff.lint]\n# Enable all `pydocstyle` rules, limiting to those that adhere to the\n# Google convention via `convention = \"google\"`, below.\nselect = [\"D\"]\n\n# On top of the Google convention, disable `D417`, which requires\n# documentation for every function parameter.\nignore = [\"D417\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\nTo enable an additional rule that's excluded from the convention,\nselect the desired rule via its fully qualified rule code (e.g.,\nD400 instead of D4 or D40):\n\n[tool.ruff.lint]\n# Enable D400 on top of the Google convention.\nextend-select = [\"D400\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"\n\n\n\n[lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"",
        "option_data_type": "\"google\" | \"numpy\" | \"pep257\"",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D409": {
    "description": "[Status: stable] [Name: mismatched-section-underline-length] [Message: Section underline should match the length of its name (\"{name}\")] What it does Checks for section underlines in docstrings that do not match the length of the corresponding section header. Why is this bad? This rule enforces a consistent style for multiline numpy-style docstrings, and helps prevent incorrect syntax in docstrings using reStructuredText. Multiline numpy-style docstrings are typically composed of a summary line, followed by a blank line, followed by a series of sections. Each section has a section header and a section body, and there should be a series of underline characters in the line following the header. The length of the underline should exactly match the length of the section header. This rule enforces a consistent style for multiline numpy-style docstrings with sections. If your docstring uses reStructuredText, the rule also helps protect against incorrect reStructuredText syntax, which would cause errors if you tried to use a tool such as Sphinx to generate documentation from the docstring. This rule is enabled when using the numpy convention, and disabled when using the google or pep257 conventions. Example def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Parameters --- distance : float Distance traveled. time : float Time spent traveling. Returns --- float Speed as distance divided by time. Raises --- FasterThanLightError If speed is greater than the speed of light. \"\"\" try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc Use instead: def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Parameters ---------- distance : float Distance traveled. time : float Time spent traveling. Returns ------- float Speed as distance divided by time. Raises ------ FasterThanLightError If speed is greater than the speed of light. \"\"\" try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc",
    "option": {
      "lint.pydocstyle.convention": {
        "option_description": "Whether to use Google-style, NumPy-style conventions, or the PEP 257\ndefaults when analyzing docstring sections.\n\nEnabling a convention will disable all rules that are not included in\nthe specified convention. As such, the intended workflow is to enable a\nconvention and then selectively enable or disable any additional rules\non top of it.\n\nFor example, to use Google-style conventions but avoid requiring\ndocumentation for every function parameter:\n\n[tool.ruff.lint]\n# Enable all `pydocstyle` rules, limiting to those that adhere to the\n# Google convention via `convention = \"google\"`, below.\nselect = [\"D\"]\n\n# On top of the Google convention, disable `D417`, which requires\n# documentation for every function parameter.\nignore = [\"D417\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\nTo enable an additional rule that's excluded from the convention,\nselect the desired rule via its fully qualified rule code (e.g.,\nD400 instead of D4 or D40):\n\n[tool.ruff.lint]\n# Enable D400 on top of the Google convention.\nextend-select = [\"D400\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"\n\n\n\n[lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"",
        "option_data_type": "\"google\" | \"numpy\" | \"pep257\"",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D410": {
    "description": "[Status: stable] [Name: no-blank-line-after-section] [Message: Missing blank line after section (\"{name}\")] What it does Checks for docstring sections that are not separated by a single blank line. Why is this bad? This rule enforces consistency in your docstrings, and helps ensure compatibility with documentation tooling. Multiline docstrings are typically composed of a summary line, followed by a blank line, followed by a series of sections, each with a section header and a section body. If a multiline numpy-style or Google-style docstring consists of multiple sections, each section should be separated by a single blank line. This rule is enabled when using the numpy and google conventions, and disabled when using the pep257 convention. Example def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Parameters ---------- distance : float Distance traveled. time : float Time spent traveling. Returns ------- float Speed as distance divided by time. Raises ------ FasterThanLightError If speed is greater than the speed of light. \"\"\" try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc Use instead: def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Parameters ---------- distance : float Distance traveled. time : float Time spent traveling. Returns ------- float Speed as distance divided by time. Raises ------ FasterThanLightError If speed is greater than the speed of light. \"\"\" try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc",
    "option": {
      "lint.pydocstyle.convention": {
        "option_description": "Whether to use Google-style, NumPy-style conventions, or the PEP 257\ndefaults when analyzing docstring sections.\n\nEnabling a convention will disable all rules that are not included in\nthe specified convention. As such, the intended workflow is to enable a\nconvention and then selectively enable or disable any additional rules\non top of it.\n\nFor example, to use Google-style conventions but avoid requiring\ndocumentation for every function parameter:\n\n[tool.ruff.lint]\n# Enable all `pydocstyle` rules, limiting to those that adhere to the\n# Google convention via `convention = \"google\"`, below.\nselect = [\"D\"]\n\n# On top of the Google convention, disable `D417`, which requires\n# documentation for every function parameter.\nignore = [\"D417\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\nTo enable an additional rule that's excluded from the convention,\nselect the desired rule via its fully qualified rule code (e.g.,\nD400 instead of D4 or D40):\n\n[tool.ruff.lint]\n# Enable D400 on top of the Google convention.\nextend-select = [\"D400\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"\n\n\n\n[lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"",
        "option_data_type": "\"google\" | \"numpy\" | \"pep257\"",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D411": {
    "description": "[Status: stable] [Name: no-blank-line-before-section] [Message: Missing blank line before section (\"{name}\")] What it does Checks for docstring sections that are not separated by a blank line. Why is this bad? This rule enforces consistency in numpy-style and Google-style docstrings, and helps ensure compatibility with documentation tooling. Multiline docstrings are typically composed of a summary line, followed by a blank line, followed by a series of sections, each with a section header and a section body. Sections should be separated by a single blank line. This rule is enabled when using the numpy and google conventions, and disabled when using the pep257 convention. Example def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Parameters ---------- distance : float Distance traveled. time : float Time spent traveling. Returns ------- float Speed as distance divided by time. Raises ------ FasterThanLightError If speed is greater than the speed of light. \"\"\" try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc Use instead: def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Parameters ---------- distance : float Distance traveled. time : float Time spent traveling. Returns ------- float Speed as distance divided by time. Raises ------ FasterThanLightError If speed is greater than the speed of light. \"\"\" try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc",
    "option": {
      "lint.pydocstyle.convention": {
        "option_description": "Whether to use Google-style, NumPy-style conventions, or the PEP 257\ndefaults when analyzing docstring sections.\n\nEnabling a convention will disable all rules that are not included in\nthe specified convention. As such, the intended workflow is to enable a\nconvention and then selectively enable or disable any additional rules\non top of it.\n\nFor example, to use Google-style conventions but avoid requiring\ndocumentation for every function parameter:\n\n[tool.ruff.lint]\n# Enable all `pydocstyle` rules, limiting to those that adhere to the\n# Google convention via `convention = \"google\"`, below.\nselect = [\"D\"]\n\n# On top of the Google convention, disable `D417`, which requires\n# documentation for every function parameter.\nignore = [\"D417\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\nTo enable an additional rule that's excluded from the convention,\nselect the desired rule via its fully qualified rule code (e.g.,\nD400 instead of D4 or D40):\n\n[tool.ruff.lint]\n# Enable D400 on top of the Google convention.\nextend-select = [\"D400\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"\n\n\n\n[lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"",
        "option_data_type": "\"google\" | \"numpy\" | \"pep257\"",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D412": {
    "description": "[Status: stable] [Name: blank-lines-between-header-and-content] [Message: No blank lines allowed between a section header and its content (\"{name}\")] What it does Checks for docstring sections that contain blank lines between a section header and a section body. Why is this bad? This rule enforces a consistent style for multiline docstrings. Multiline docstrings are typically composed of a summary line, followed by a blank line, followed by a series of sections, each with a section header and a section body. There should be no blank lines between a section header and a section body. Example def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Args: distance: Distance traveled. time: Time spent traveling. Returns: Speed as distance divided by time. Raises: FasterThanLightError: If speed is greater than the speed of light. \"\"\" try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc Use instead: def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Args: distance: Distance traveled. time: Time spent traveling. Returns: Speed as distance divided by time. Raises: FasterThanLightError: If speed is greater than the speed of light. \"\"\" try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc",
    "option": {
      "lint.pydocstyle.ignore-decorators": {
        "option_description": "Ignore docstrings for functions or methods decorated with the\nspecified fully-qualified decorators.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]\n\n\n\n[lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D413": {
    "description": "[Status: stable] [Name: missing-blank-line-after-last-section] [Message: Missing blank line after last section (\"{name}\")] What it does Checks for missing blank lines after the last section of a multiline docstring. Why is this bad? This rule enforces a consistent style for multiline docstrings. Multiline docstrings are typically composed of a summary line, followed by a blank line, followed by a series of sections, each with a section header and a section body. This rule may not apply to all projects; its applicability is a matter of convention. By default, the rule is disabled when using the google, numpy, and pep257 conventions. Example def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Parameters ---------- distance : float Distance traveled. time : float Time spent traveling. Returns ------- float Speed as distance divided by time. Raises ------ FasterThanLightError If speed is greater than the speed of light. \"\"\" try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc Use instead: def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Parameters ---------- distance : float Distance traveled. time : float Time spent traveling. Returns ------- float Speed as distance divided by time. Raises ------ FasterThanLightError If speed is greater than the speed of light. \"\"\" try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc",
    "option": {
      "lint.pydocstyle.convention": {
        "option_description": "Whether to use Google-style, NumPy-style conventions, or the PEP 257\ndefaults when analyzing docstring sections.\n\nEnabling a convention will disable all rules that are not included in\nthe specified convention. As such, the intended workflow is to enable a\nconvention and then selectively enable or disable any additional rules\non top of it.\n\nFor example, to use Google-style conventions but avoid requiring\ndocumentation for every function parameter:\n\n[tool.ruff.lint]\n# Enable all `pydocstyle` rules, limiting to those that adhere to the\n# Google convention via `convention = \"google\"`, below.\nselect = [\"D\"]\n\n# On top of the Google convention, disable `D417`, which requires\n# documentation for every function parameter.\nignore = [\"D417\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\nTo enable an additional rule that's excluded from the convention,\nselect the desired rule via its fully qualified rule code (e.g.,\nD400 instead of D4 or D40):\n\n[tool.ruff.lint]\n# Enable D400 on top of the Google convention.\nextend-select = [\"D400\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"\n\n\n\n[lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"",
        "option_data_type": "\"google\" | \"numpy\" | \"pep257\"",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D414": {
    "description": "[Status: stable] [Name: empty-docstring-section] [Message: Section has no content (\"{name}\")] What it does Checks for docstrings with empty sections. Why is this bad? An empty section in a multiline docstring likely indicates an unfinished or incomplete docstring. Multiline docstrings are typically composed of a summary line, followed by a blank line, followed by a series of sections, each with a section header and a section body. Each section body should be non-empty; empty sections should either have content added to them, or be removed entirely. Example def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Parameters ---------- distance : float Distance traveled. time : float Time spent traveling. Returns ------- float Speed as distance divided by time. Raises ------ \"\"\" try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc Use instead: def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Parameters ---------- distance : float Distance traveled. time : float Time spent traveling. Returns ------- float Speed as distance divided by time. Raises ------ FasterThanLightError If speed is greater than the speed of light. \"\"\" try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc",
    "option": {
      "lint.pydocstyle.ignore-decorators": {
        "option_description": "Ignore docstrings for functions or methods decorated with the\nspecified fully-qualified decorators.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]\n\n\n\n[lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D415": {
    "description": "[Status: stable] [Name: missing-terminal-punctuation] [Message: First line should end with a period, question mark, or exclamation point] What it does Checks for docstrings in which the first line does not end in a punctuation mark, such as a period, question mark, or exclamation point. Why is this bad? The first line of a docstring should end with a period, question mark, or exclamation point, for grammatical correctness and consistency. This rule may not apply to all projects; its applicability is a matter of convention. By default, this rule is enabled when using the google convention, and disabled when using the numpy and pep257 conventions. Example def average(values: list[float]) -> float: \"\"\"Return the mean of the given values\"\"\" Use instead: def average(values: list[float]) -> float: \"\"\"Return the mean of the given values.\"\"\"",
    "option": {
      "lint.pydocstyle.convention": {
        "option_description": "Whether to use Google-style, NumPy-style conventions, or the PEP 257\ndefaults when analyzing docstring sections.\n\nEnabling a convention will disable all rules that are not included in\nthe specified convention. As such, the intended workflow is to enable a\nconvention and then selectively enable or disable any additional rules\non top of it.\n\nFor example, to use Google-style conventions but avoid requiring\ndocumentation for every function parameter:\n\n[tool.ruff.lint]\n# Enable all `pydocstyle` rules, limiting to those that adhere to the\n# Google convention via `convention = \"google\"`, below.\nselect = [\"D\"]\n\n# On top of the Google convention, disable `D417`, which requires\n# documentation for every function parameter.\nignore = [\"D417\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\nTo enable an additional rule that's excluded from the convention,\nselect the desired rule via its fully qualified rule code (e.g.,\nD400 instead of D4 or D40):\n\n[tool.ruff.lint]\n# Enable D400 on top of the Google convention.\nextend-select = [\"D400\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"\n\n\n\n[lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"",
        "option_data_type": "\"google\" | \"numpy\" | \"pep257\"",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D416": {
    "description": "[Status: stable] [Name: missing-section-name-colon] [Message: Section name should end with a colon (\"{name}\")] What it does Checks for docstring section headers that do not end with a colon. Why is this bad? This rule enforces a consistent style for multiline Google-style docstrings. If a multiline Google-style docstring consists of multiple sections, each section header should end with a colon. Multiline docstrings are typically composed of a summary line, followed by a blank line, followed by a series of sections, each with a section header and a section body. This rule is enabled when using the google convention, and disabled when using the pep257 and numpy conventions. Example def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Args distance: Distance traveled. time: Time spent traveling. Returns Speed as distance divided by time. Raises FasterThanLightError: If speed is greater than the speed of light. \"\"\" try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc Use instead: def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Args: distance: Distance traveled. time: Time spent traveling. Returns: Speed as distance divided by time. Raises: FasterThanLightError: If speed is greater than the speed of light. \"\"\" try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc",
    "option": {
      "lint.pydocstyle.convention": {
        "option_description": "Whether to use Google-style, NumPy-style conventions, or the PEP 257\ndefaults when analyzing docstring sections.\n\nEnabling a convention will disable all rules that are not included in\nthe specified convention. As such, the intended workflow is to enable a\nconvention and then selectively enable or disable any additional rules\non top of it.\n\nFor example, to use Google-style conventions but avoid requiring\ndocumentation for every function parameter:\n\n[tool.ruff.lint]\n# Enable all `pydocstyle` rules, limiting to those that adhere to the\n# Google convention via `convention = \"google\"`, below.\nselect = [\"D\"]\n\n# On top of the Google convention, disable `D417`, which requires\n# documentation for every function parameter.\nignore = [\"D417\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\nTo enable an additional rule that's excluded from the convention,\nselect the desired rule via its fully qualified rule code (e.g.,\nD400 instead of D4 or D40):\n\n[tool.ruff.lint]\n# Enable D400 on top of the Google convention.\nextend-select = [\"D400\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"\n\n\n\n[lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"",
        "option_data_type": "\"google\" | \"numpy\" | \"pep257\"",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D417": {
    "description": "[Status: stable] [Name: undocumented-param] [Message: Missing argument description in the docstring for <code>{definition}</code>: <code>{name}</code>] What it does Checks for function docstrings that do not include documentation for all parameters in the function. Why is this bad? This rule helps prevent you from leaving Google-style docstrings unfinished or incomplete. Multiline Google-style docstrings should describe all parameters for the function they are documenting. Multiline docstrings are typically composed of a summary line, followed by a blank line, followed by a series of sections, each with a section header and a section body. Function docstrings often include a section for function arguments; this rule is concerned with that section only. Note that this rule only checks docstrings with an arguments (e.g. Args) section. This rule is enabled when using the google convention, and disabled when using the pep257 and numpy conventions. Parameters annotated with typing.Unpack are exempt from this rule. This follows the Python typing specification for unpacking keyword arguments. Example def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Args: distance: Distance traveled. Returns: Speed as distance divided by time. Raises: FasterThanLightError: If speed is greater than the speed of light. \"\"\" try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc Use instead: def calculate_speed(distance: float, time: float) -> float: \"\"\"Calculate speed as distance divided by time. Args: distance: Distance traveled. time: Time spent traveling. Returns: Speed as distance divided by time. Raises: FasterThanLightError: If speed is greater than the speed of light. \"\"\" try: return distance / time except ZeroDivisionError as exc: raise FasterThanLightError from exc",
    "option": {
      "lint.pydocstyle.convention": {
        "option_description": "Whether to use Google-style, NumPy-style conventions, or the PEP 257\ndefaults when analyzing docstring sections.\n\nEnabling a convention will disable all rules that are not included in\nthe specified convention. As such, the intended workflow is to enable a\nconvention and then selectively enable or disable any additional rules\non top of it.\n\nFor example, to use Google-style conventions but avoid requiring\ndocumentation for every function parameter:\n\n[tool.ruff.lint]\n# Enable all `pydocstyle` rules, limiting to those that adhere to the\n# Google convention via `convention = \"google\"`, below.\nselect = [\"D\"]\n\n# On top of the Google convention, disable `D417`, which requires\n# documentation for every function parameter.\nignore = [\"D417\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\nTo enable an additional rule that's excluded from the convention,\nselect the desired rule via its fully qualified rule code (e.g.,\nD400 instead of D4 or D40):\n\n[tool.ruff.lint]\n# Enable D400 on top of the Google convention.\nextend-select = [\"D400\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"\n\n\n\n[lint.pydocstyle]\n# Use Google-style docstrings.\nconvention = \"google\"",
        "option_data_type": "\"google\" | \"numpy\" | \"pep257\"",
        "default_value": "null",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pydocstyle.ignore-var-parameters": {
        "option_description": "If set to true, ignore missing documentation for *args and **kwargs parameters.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\nignore-var-parameters = true\n\n\n\n[lint.pydocstyle]\nignore-var-parameters = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D418": {
    "description": "[Status: stable] [Name: overload-with-docstring] [Message: Function decorated with <code>@overload</code> shouldn't contain a docstring] What it does Checks for @overload function definitions that contain a docstring. Why is this bad? The @overload decorator is used to define multiple compatible signatures for a given function, to support type-checking. A series of @overload definitions should be followed by a single non-decorated definition that contains the implementation of the function. @overload function definitions should not contain a docstring; instead, the docstring should be placed on the non-decorated definition that contains the implementation. Example from typing import overload @overload def factorial(n: int) -> int: \"\"\"Return the factorial of n.\"\"\" @overload def factorial(n: float) -> float: \"\"\"Return the factorial of n.\"\"\" def factorial(n): \"\"\"Return the factorial of n.\"\"\" factorial.__doc__ # \"Return the factorial of n.\" Use instead: from typing import overload @overload def factorial(n: int) -> int: ... @overload def factorial(n: float) -> float: ... def factorial(n): \"\"\"Return the factorial of n.\"\"\" factorial.__doc__ # \"Return the factorial of n.\"",
    "option": {
      "lint.pydocstyle.ignore-decorators": {
        "option_description": "Ignore docstrings for functions or methods decorated with the\nspecified fully-qualified decorators.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]\n\n\n\n[lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "D419": {
    "description": "[Status: stable] [Name: empty-docstring] [Message: Docstring is empty] What it does Checks for empty docstrings. Why is this bad? An empty docstring is indicative of incomplete documentation. It should either be removed or replaced with a meaningful docstring. Example def average(values: list[float]) -> float: \"\"\"\"\"\" Use instead: def average(values: list[float]) -> float: \"\"\"Return the mean of the given values.\"\"\"",
    "option": {
      "lint.pydocstyle.ignore-decorators": {
        "option_description": "Ignore docstrings for functions or methods decorated with the\nspecified fully-qualified decorators.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]\n\n\n\n[lint.pydocstyle]\nignore-decorators = [\"typing.overload\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "F401": {
    "description": "[Status: stable] [Name: unused-import] [Message: <code>{name}</code> imported but unused; consider using <code>importlib.util.find_spec</code> to test for availability] What it does Checks for unused imports. Why is this bad? Unused imports add a performance overhead at runtime, and risk creating import cycles. They also increase the cognitive load of reading the code. If an import statement is used to check for the availability or existence of a module, consider using importlib.util.find_spec instead. If an import statement is used to re-export a symbol as part of a module's public interface, consider using a \"redundant\" import alias, which instructs Ruff (and other tools) to respect the re-export, and avoid marking it as unused, as in: from module import member as member Alternatively, you can use __all__ to declare a symbol as part of the module's interface, as in: # __init__.py import some_module __all__ = [\"some_module\"] Preview When preview is enabled (and certain simplifying assumptions are met), we analyze all import statements for a given module when determining whether an import is used, rather than simply the last of these statements. This can result in both different and more import statements being marked as unused. For example, if a module consists of import a import a.b then both statements are marked as unused under preview, whereas only the second is marked as unused under stable behavior. As another example, if a module consists of import a.b import a a.b.foo() then a diagnostic will only be emitted for the first line under preview, whereas a diagnostic would only be emitted for the second line under stable behavior. Note that this behavior is somewhat subjective and is designed to conform to the developer's intuition rather than Python's actual execution. To wit, the statement import a.b automatically executes import a, so in some sense import a is always redundant in the presence of import a.b. Fix safety Fixes to remove unused imports are safe, except in __init__.py files. Applying fixes to __init__.py files is currently in preview. The fix offered depends on the type of the unused import. Ruff will suggest a safe fix to export first-party imports with either a redundant alias or, if already present in the file, an __all__ entry. If multiple __all__ declarations are present, Ruff will not offer a fix. Ruff will suggest an unsafe fix to remove third-party and standard library imports -- the fix is unsafe because the module's interface changes. See this FAQ section for more details on how Ruff determines whether an import is first or third-party. Example import numpy as np # unused import def area(radius): return 3.14 * radius**2 Use instead: def area(radius): return 3.14 * radius**2 To check the availability of a module, use importlib.util.find_spec: from importlib.util import find_spec if find_spec(\"numpy\") is not None: print(\"numpy is installed\") else: print(\"numpy is not installed\")",
    "option": {
      "lint.ignore-init-module-imports": {
        "option_description": "Deprecated\nThis option has been deprecated in 0.4.4. ignore-init-module-imports will be removed in a future version because F401 now recommends appropriate fixes for unused imports in __init__.py (currently in preview mode). See documentation for more information and please update your configuration.\n\nAvoid automatically removing unused imports in __init__.py files. Such\nimports will still be flagged, but with a dedicated message suggesting\nthat the import is either added to the module's __all__ symbol, or\nre-exported with a redundant alias (e.g., import os as os).\n\nThis option is enabled by default, but you can opt-in to removal of imports\nvia an unsafe fix.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\nignore-init-module-imports = false\n\n\n\n[lint]\nignore-init-module-imports = false",
        "option_data_type": "bool",
        "default_value": "true",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pyflakes.allowed-unused-imports": {
        "option_description": "A list of modules to ignore when considering unused imports.\n\nUsed to prevent violations for specific modules that are known to have side effects on\nimport (e.g., hvplot.pandas).\n\nModules in this list are expected to be fully-qualified names (e.g., hvplot.pandas). Any\nsubmodule of a given module will also be ignored (e.g., given hvplot, hvplot.pandas\nwill also be ignored).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pyflakes]\nallowed-unused-imports = [\"hvplot.pandas\"]\n\n\n\n[lint.pyflakes]\nallowed-unused-imports = [\"hvplot.pandas\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "F402": {
    "description": "[Status: stable] [Name: import-shadowed-by-loop-var] [Message: Import <code>{name}</code> from {row} shadowed by loop variable] What it does Checks for import bindings that are shadowed by loop variables. Why is this bad? Shadowing an import with loop variables makes the code harder to read and reason about, as the identify of the imported binding is no longer clear. It's also often indicative of a mistake, as it's unlikely that the loop variable is intended to be used as the imported binding. Consider using a different name for the loop variable. Example from os import path for path in files: print(path) Use instead: from os import path for filename in files: print(filename)",
    "option": {}
  },
  "F403": {
    "description": "[Status: stable] [Name: undefined-local-with-import-star] [Message: <code>from {name} import *</code> used; unable to detect undefined names] What it does Checks for the use of wildcard imports. Why is this bad? Wildcard imports (e.g., from module import *) make it hard to determine which symbols are available in the current namespace, and from which module they were imported. They're also discouraged by PEP 8. Example from math import * def area(radius): return pi * radius**2 Use instead: from math import pi def area(radius): return pi * radius**2",
    "option": {}
  },
  "F404": {
    "description": "[Status: stable] [Name: late-future-import] [Message: <code>from __future__</code> imports must occur at the beginning of the file] What it does Checks for __future__ imports that are not located at the beginning of a file. Why is this bad? Imports from __future__ must be placed the beginning of the file, before any other statements (apart from docstrings). The use of __future__ imports elsewhere is invalid and will result in a SyntaxError. Example from pathlib import Path from __future__ import annotations Use instead: from __future__ import annotations from pathlib import Path",
    "option": {}
  },
  "F405": {
    "description": "[Status: stable] [Name: undefined-local-with-import-star-usage] [Message: <code>{name}</code> may be undefined, or defined from star imports] What it does Checks for names that might be undefined, but may also be defined in a wildcard import. Why is this bad? Wildcard imports (e.g., from module import *) make it hard to determine which symbols are available in the current namespace. If a module contains a wildcard import, and a name in the current namespace has not been explicitly defined or imported, then it's unclear whether the name is undefined or was imported by the wildcard import. If the name is defined in via a wildcard import, that member should be imported explicitly to avoid confusion. If the name is not defined in a wildcard import, it should be defined or imported. Example from math import * def area(radius): return pi * radius**2 Use instead: from math import pi def area(radius): return pi * radius**2",
    "option": {}
  },
  "F406": {
    "description": "[Status: stable] [Name: undefined-local-with-nested-import-star-usage] [Message: <code>from {name} import *</code> only allowed at module level] What it does Check for the use of wildcard imports outside of the module namespace. Why is this bad? The use of wildcard imports outside of the module namespace (e.g., within functions) can lead to confusion, as the import can shadow local variables. Though wildcard imports are discouraged by PEP 8, when necessary, they should be placed in the module namespace (i.e., at the top-level of a module). Example def foo(): from math import * Use instead: from math import * def foo(): ...",
    "option": {}
  },
  "F407": {
    "description": "[Status: stable] [Name: future-feature-not-defined] [Message: Future feature <code>{name}</code> is not defined] What it does Checks for __future__ imports that are not defined in the current Python version. Why is this bad? Importing undefined or unsupported members from the __future__ module is a SyntaxError.",
    "option": {}
  },
  "F501": {
    "description": "[Status: stable] [Name: percent-format-invalid-format] [Message: <code>%</code>-format string has invalid format string: {message}] What it does Checks for invalid printf-style format strings. Why is this bad? Conversion specifiers are required for printf-style format strings. These specifiers must contain a % character followed by a conversion type. Example \"Hello, %\" % \"world\" Use instead: \"Hello, %s\" % \"world\"",
    "option": {}
  },
  "F502": {
    "description": "[Status: stable] [Name: percent-format-expected-mapping] [Message: <code>%</code>-format string expected mapping but got sequence] What it does Checks for named placeholders in printf-style format strings without mapping-type values. Why is this bad? When using named placeholders in printf-style format strings, the values must be a map type (such as a dictionary). Otherwise, the expression will raise a TypeError. Example \"%(greeting)s, %(name)s\" % (\"Hello\", \"World\") Use instead: \"%(greeting)s, %(name)s\" % {\"greeting\": \"Hello\", \"name\": \"World\"} Or: \"%s, %s\" % (\"Hello\", \"World\")",
    "option": {}
  },
  "F503": {
    "description": "[Status: stable] [Name: percent-format-expected-sequence] [Message: <code>%</code>-format string expected sequence but got mapping] What it does Checks for uses of mapping-type values in printf-style format strings without named placeholders. Why is this bad? When using mapping-type values (such as dict) in printf-style format strings, the keys must be named. Otherwise, the expression will raise a TypeError. Example \"%s, %s\" % {\"greeting\": \"Hello\", \"name\": \"World\"} Use instead: \"%(greeting)s, %(name)s\" % {\"greeting\": \"Hello\", \"name\": \"World\"} Or: \"%s, %s\" % (\"Hello\", \"World\")",
    "option": {}
  },
  "F504": {
    "description": "[Status: stable] [Name: percent-format-extra-named-arguments] [Message: <code>%</code>-format string has unused named argument(s): {message}] What it does Checks for unused mapping keys in printf-style format strings. Why is this bad? Unused named placeholders in printf-style format strings are unnecessary, and likely indicative of a mistake. They should be removed. Example \"Hello, %(name)s\" % {\"greeting\": \"Hello\", \"name\": \"World\"} Use instead: \"Hello, %(name)s\" % {\"name\": \"World\"} Fix safety This rule's fix is marked as unsafe for mapping key containing function calls with potential side effects, because removing such arguments could change the behavior of the code. For example, the fix would be marked as unsafe in the following case: \"Hello, %(name)s\" % {\"greeting\": print(1), \"name\": \"World\"}",
    "option": {}
  },
  "F505": {
    "description": "[Status: stable] [Name: percent-format-missing-argument] [Message: <code>%</code>-format string is missing argument(s) for placeholder(s): {message}] What it does Checks for named placeholders in printf-style format strings that are not present in the provided mapping. Why is this bad? Named placeholders that lack a corresponding value in the provided mapping will raise a KeyError. Example \"%(greeting)s, %(name)s\" % {\"name\": \"world\"} Use instead: \"Hello, %(name)s\" % {\"name\": \"world\"}",
    "option": {}
  },
  "F506": {
    "description": "[Status: stable] [Name: percent-format-mixed-positional-and-named] [Message: <code>%</code>-format string has mixed positional and named placeholders] What it does Checks for printf-style format strings that have mixed positional and named placeholders. Why is this bad? Python does not support mixing positional and named placeholders in printf-style format strings. The use of mixed placeholders will raise a TypeError at runtime. Example \"%s, %(name)s\" % (\"Hello\", {\"name\": \"World\"}) Use instead: \"%s, %s\" % (\"Hello\", \"World\") Or: \"%(greeting)s, %(name)s\" % {\"greeting\": \"Hello\", \"name\": \"World\"}",
    "option": {}
  },
  "F507": {
    "description": "[Status: stable] [Name: percent-format-positional-count-mismatch] [Message: <code>%</code>-format string has {wanted} placeholder(s) but {got} substitution(s)] What it does Checks for printf-style format strings that have a mismatch between the number of positional placeholders and the number of substitution values. Why is this bad? When a printf-style format string is provided with too many or too few substitution values, it will raise a TypeError at runtime. Example \"%s, %s\" % (\"Hello\", \"world\", \"!\") Use instead: \"%s, %s\" % (\"Hello\", \"world\")",
    "option": {}
  },
  "F508": {
    "description": "[Status: stable] [Name: percent-format-star-requires-sequence] [Message: <code>%</code>-format string <code>*</code> specifier requires sequence] What it does Checks for printf-style format strings that use the * specifier with non-tuple values. Why is this bad? The use of the * specifier with non-tuple values will raise a TypeError at runtime. Example from math import pi \"%(n).*f\" % {\"n\": (2, pi)} Use instead: from math import pi \"%.*f\" % (2, pi) # 3.14",
    "option": {}
  },
  "F509": {
    "description": "[Status: stable] [Name: percent-format-unsupported-format-character] [Message: <code>%</code>-format string has unsupported format character <code>{char}</code>] What it does Checks for printf-style format strings with invalid format characters. Why is this bad? In printf-style format strings, the % character is used to indicate placeholders. If a % character is not followed by a valid format character, it will raise a ValueError at runtime. Example \"Hello, %S\" % \"world\" Use instead: \"Hello, %s\" % \"world\"",
    "option": {}
  },
  "F521": {
    "description": "[Status: stable] [Name: string-dot-format-invalid-format] [Message: <code>.format</code> call has invalid format string: {message}] What it does Checks for str.format calls with invalid format strings. Why is this bad? Invalid format strings will raise a ValueError. Example \"{\".format(foo) Use instead: \"{}\".format(foo)",
    "option": {}
  },
  "F522": {
    "description": "[Status: stable] [Name: string-dot-format-extra-named-arguments] [Message: <code>.format</code> call has unused named argument(s): {message}] What it does Checks for str.format calls with unused keyword arguments. Why is this bad? Unused keyword arguments are redundant, and often indicative of a mistake. They should be removed. Example \"Hello, {name}\".format(greeting=\"Hello\", name=\"World\") Use instead: \"Hello, {name}\".format(name=\"World\") Fix safety This rule's fix is marked as unsafe if the unused keyword argument contains a function call with potential side effects, because removing such arguments could change the behavior of the code. For example, the fix would be marked as unsafe in the following case: \"Hello, {name}\".format(greeting=print(1), name=\"World\")",
    "option": {}
  },
  "F523": {
    "description": "[Status: stable] [Name: string-dot-format-extra-positional-arguments] [Message: <code>.format</code> call has unused arguments at position(s): {message}] What it does Checks for str.format calls with unused positional arguments. Why is this bad? Unused positional arguments are redundant, and often indicative of a mistake. They should be removed. Example \"Hello, {0}\".format(\"world\", \"!\") Use instead: \"Hello, {0}\".format(\"world\") Fix safety This rule's fix is marked as unsafe if the unused positional argument contains a function call with potential side effects, because removing such arguments could change the behavior of the code. For example, the fix would be marked as unsafe in the following case: \"Hello, {0}\".format(\"world\", print(1))",
    "option": {}
  },
  "F524": {
    "description": "[Status: stable] [Name: string-dot-format-missing-arguments] [Message: <code>.format</code> call is missing argument(s) for placeholder(s): {message}] What it does Checks for str.format calls with placeholders that are missing arguments. Why is this bad? In str.format calls, omitting arguments for placeholders will raise a KeyError at runtime. Example \"{greeting}, {name}\".format(name=\"World\") Use instead: \"{greeting}, {name}\".format(greeting=\"Hello\", name=\"World\")",
    "option": {}
  },
  "F525": {
    "description": "[Status: stable] [Name: string-dot-format-mixing-automatic] [Message: <code>.format</code> string mixes automatic and manual numbering] What it does Checks for str.format calls that mix automatic and manual numbering. Why is this bad? In str.format calls, mixing automatic and manual numbering will raise a ValueError at runtime. Example \"{0}, {}\".format(\"Hello\", \"World\") Use instead: \"{0}, {1}\".format(\"Hello\", \"World\") Or: \"{}, {}\".format(\"Hello\", \"World\")",
    "option": {}
  },
  "F541": {
    "description": "[Status: stable] [Name: f-string-missing-placeholders] [Message: f-string without any placeholders] What it does Checks for f-strings that do not contain any placeholder expressions. Why is this bad? f-strings are a convenient way to format strings, but they are not necessary if there are no placeholder expressions to format. In this case, a regular string should be used instead, as an f-string without placeholders can be confusing for readers, who may expect such a placeholder to be present. An f-string without any placeholders could also indicate that the author forgot to add a placeholder expression. Example f\"Hello, world!\" Use instead: \"Hello, world!\" Note: to maintain compatibility with PyFlakes, this rule only flags f-strings that are part of an implicit concatenation if none of the f-string segments contain placeholder expressions. For example: # Will not be flagged. ( f\"Hello,\" f\" {name}!\" ) # Will be flagged. ( f\"Hello,\" f\" World!\" ) See #10885 for more.",
    "option": {}
  },
  "F601": {
    "description": "[Status: stable] [Name: multi-value-repeated-key-literal] [Message: Dictionary key literal <code>{name}</code> repeated] What it does Checks for dictionary literals that associate multiple values with the same key. Why is this bad? Dictionary keys should be unique. If a key is associated with multiple values, the earlier values will be overwritten. Including multiple values for the same key in a dictionary literal is likely a mistake. Example foo = { \"bar\": 1, \"baz\": 2, \"baz\": 3, } foo[\"baz\"] # 3 Use instead: foo = { \"bar\": 1, \"baz\": 2, } foo[\"baz\"] # 2 Fix safety This rule's fix is marked as unsafe because removing a repeated dictionary key may delete comments that are attached to the removed key-value pair. This can also change the program's behavior if the value expressions have side effects.",
    "option": {}
  },
  "F602": {
    "description": "[Status: stable] [Name: multi-value-repeated-key-variable] [Message: Dictionary key <code>{name}</code> repeated] What it does Checks for dictionary keys that are repeated with different values. Why is this bad? Dictionary keys should be unique. If a key is repeated with a different value, the first values will be overwritten and the key will correspond to the last value. This is likely a mistake. Example foo = { bar: 1, baz: 2, baz: 3, } foo[baz] # 3 Use instead: foo = { bar: 1, baz: 2, } foo[baz] # 2 Fix safety This rule's fix is marked as unsafe because removing a repeated dictionary key may delete comments that are attached to the removed key-value pair. This can also change the program's behavior if the value expressions have side effects.",
    "option": {}
  },
  "F621": {
    "description": "[Status: stable] [Name: expressions-in-star-assignment] [Message: Too many expressions in star-unpacking assignment] What it does Checks for the use of too many expressions in starred assignment statements. Why is this bad? In assignment statements, starred expressions can be used to unpack iterables. In Python 3, no more than 1 << 8 assignments are allowed before a starred expression, and no more than 1 << 24 expressions are allowed after a starred expression.",
    "option": {}
  },
  "F622": {
    "description": "[Status: stable] [Name: multiple-starred-expressions] [Message: Two starred expressions in assignment] What it does Checks for the use of multiple starred expressions in assignment statements. Why is this bad? In assignment statements, starred expressions can be used to unpack iterables. Including more than one starred expression on the left-hand-side of an assignment will cause a SyntaxError, as it is unclear which expression should receive the remaining values. Example *foo, *bar, baz = (1, 2, 3)",
    "option": {}
  },
  "F631": {
    "description": "[Status: stable] [Name: assert-tuple] [Message: Assert test is a non-empty tuple, which is always <code>True</code>] What it does Checks for assert statements that use non-empty tuples as test conditions. Why is this bad? Non-empty tuples are always True, so an assert statement with a non-empty tuple as its test condition will always pass. This is likely a mistake. Example assert (some_condition,) Use instead: assert some_condition",
    "option": {}
  },
  "F632": {
    "description": "[Status: stable] [Name: is-literal] [Message: Use <code>==</code> to compare constant literals] What it does Checks for is and is not comparisons against literals, like integers, strings, or lists. Why is this bad? The is and is not comparators operate on identity, in that they check whether two objects are the same object. If the objects are not the same object, the comparison will always be False. Using is and is not with constant literals often works \"by accident\", but are not guaranteed to produce the expected result. As of Python 3.8, using is and is not with constant literals will produce a SyntaxWarning. This rule will also flag is and is not comparisons against non-constant literals, like lists, sets, and dictionaries. While such comparisons will not raise a SyntaxWarning, they are still likely to be incorrect, as they will compare the identities of the objects instead of their values, which will always evaluate to False. Instead, use == and != to compare literals, which will compare the values of the objects instead of their identities. Example x = 200 if x is 200: print(\"It's 200!\") Use instead: x = 200 if x == 200: print(\"It's 200!\")",
    "option": {}
  },
  "F633": {
    "description": "[Status: stable] [Name: invalid-print-syntax] [Message: Use of <code>&gt;&gt;</code> is invalid with <code>print</code> function] What it does Checks for print statements that use the >> syntax. Why is this bad? In Python 2, the print statement can be used with the >> syntax to print to a file-like object. This print >> sys.stderr syntax no longer exists in Python 3, where print is only a function, not a statement. Instead, use the file keyword argument to the print function, the sys.stderr.write function, or the logging module. Example from __future__ import print_function import sys print >> sys.stderr, \"Hello, world!\" Use instead: print(\"Hello, world!\", file=sys.stderr) Or: import sys sys.stderr.write(\"Hello, world!\\n\") Or: import logging logging.error(\"Hello, world!\")",
    "option": {}
  },
  "F634": {
    "description": "[Status: stable] [Name: if-tuple] [Message: If test is a tuple, which is always <code>True</code>] What it does Checks for if statements that use non-empty tuples as test conditions. Why is this bad? Non-empty tuples are always True, so an if statement with a non-empty tuple as its test condition will always pass. This is likely a mistake. Example if (False,): print(\"This will always run\") Use instead: if False: print(\"This will never run\")",
    "option": {}
  },
  "F701": {
    "description": "[Status: stable] [Name: break-outside-loop] [Message: <code>break</code> outside loop] What it does Checks for break statements outside of loops. Why is this bad? The use of a break statement outside of a for or while loop will raise a SyntaxError. Example def foo(): break",
    "option": {}
  },
  "F702": {
    "description": "[Status: stable] [Name: continue-outside-loop] [Message: <code>continue</code> not properly in loop] What it does Checks for continue statements outside of loops. Why is this bad? The use of a continue statement outside of a for or while loop will raise a SyntaxError. Example def foo(): continue # SyntaxError",
    "option": {}
  },
  "F704": {
    "description": "[Status: stable] [Name: yield-outside-function] [Message: <code>{keyword}</code> statement outside of a function] What it does Checks for yield, yield from, and await usages outside of functions. Why is this bad? The use of yield, yield from, or await outside of a function will raise a SyntaxError. Example class Foo: yield 1 Notebook behavior As an exception, await is allowed at the top level of a Jupyter notebook (see: autoawait).",
    "option": {}
  },
  "F706": {
    "description": "[Status: stable] [Name: return-outside-function] [Message: <code>return</code> statement outside of a function/method] What it does Checks for return statements outside of functions. Why is this bad? The use of a return statement outside of a function will raise a SyntaxError. Example class Foo: return 1",
    "option": {}
  },
  "F707": {
    "description": "[Status: stable] [Name: default-except-not-last] [Message: An <code>except</code> block as not the last exception handler] What it does Checks for except blocks that handle all exceptions, but are not the last except block in a try statement. Why is this bad? When an exception is raised within a try block, the except blocks are evaluated in order, and the first matching block is executed. If an except block handles all exceptions, but isn't the last block, Python will raise a SyntaxError, as the following blocks would never be executed. Example def reciprocal(n): try: reciprocal = 1 / n except: print(\"An exception occurred.\") except ZeroDivisionError: print(\"Cannot divide by zero.\") else: return reciprocal Use instead: def reciprocal(n): try: reciprocal = 1 / n except ZeroDivisionError: print(\"Cannot divide by zero.\") except: print(\"An exception occurred.\") else: return reciprocal",
    "option": {}
  },
  "F722": {
    "description": "[Status: stable] [Name: forward-annotation-syntax-error] [Message: Syntax error in forward annotation: {parse_error}] What it does Checks for forward annotations that include invalid syntax. Why is this bad? In Python, type annotations can be quoted as strings literals to enable references to types that have not yet been defined, known as \"forward references\". However, these quoted annotations must be valid Python expressions. The use of invalid syntax in a quoted annotation won't raise a SyntaxError, but will instead raise an error when type checking is performed. Example def foo() -> \"/\": ...",
    "option": {}
  },
  "F811": {
    "description": "[Status: stable] [Name: redefined-while-unused] [Message: Redefinition of unused <code>{name}</code> from {row}] What it does Checks for variable definitions that redefine (or \"shadow\") unused variables. Why is this bad? Redefinitions of unused names are unnecessary and often indicative of a mistake. Example import foo import bar import foo # Redefinition of unused `foo` from line 1 Use instead: import foo import bar",
    "option": {
      "lint.dummy-variable-rgx": {
        "option_description": "A regular expression used to identify \"dummy\" variables, or those which\nshould be ignored when enforcing (e.g.) unused-variable rules. The\ndefault expression matches _, __, and _var, but not _var_.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\n# Only ignore variables named \"_\".\ndummy-variable-rgx = \"^_$\"\n\n\n\n[lint]\n# Only ignore variables named \"_\".\ndummy-variable-rgx = \"^_$\"",
        "option_data_type": "str",
        "default_value": "\"^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "F821": {
    "description": "[Status: stable] [Name: undefined-name] [Message: Undefined name <code>{name}</code>. {tip}] What it does Checks for uses of undefined names. Why is this bad? An undefined name is likely to raise NameError at runtime. Example def double(): return n * 2 # raises `NameError` if `n` is undefined when `double` is called Use instead: def double(n): return n * 2",
    "option": {
      "target-version": {
        "option_description": "The minimum Python version to target, e.g., when considering automatic\ncode upgrades, like rewriting type annotations. Ruff will not propose\nchanges using features that are not available in the given version.\n\nFor example, to represent supporting Python >=3.11 or ==3.11\nspecify target-version = \"py311\".\n\nIf you're already using a pyproject.toml file, we recommend\nproject.requires-python instead, as it's based on Python packaging\nstandards, and will be respected by other tools. For example, Ruff\ntreats the following as identical to target-version = \"py38\":\n\n[project]\nrequires-python = \">=3.8\"\n\nIf both are specified, target-version takes precedence over\nrequires-python. See Inferring the Python version\nfor a complete description of how the target-version is determined\nwhen left unspecified.\n\nNote that a stub file can sometimes make use of a typing feature\nbefore it is available at runtime, as long as the stub does not make\nuse of new syntax. For example, a type checker will understand\nint | str in a stub as being a Union type annotation, even if the\ntype checker is run using Python 3.9, despite the fact that the |\noperator can only be used to create union types at runtime on Python\n3.10+. As such, Ruff will often recommend newer features in a stub\nfile than it would for an equivalent runtime file with the same target\nversion.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"\n\n\n\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"",
        "option_data_type": "\"py37\" | \"py38\" | \"py39\" | \"py310\" | \"py311\" | \"py312\" | \"py313\" | \"py314\"",
        "default_value": "\"py310\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "F822": {
    "description": "[Status: stable] [Name: undefined-export] [Message: Undefined name <code>{name}</code> in <code>__all__</code>] What it does Checks for undefined names in __all__. Why is this bad? In Python, the __all__ variable is used to define the names that are exported when a module is imported as a wildcard (e.g., from foo import *). The names in __all__ must be defined in the module, but are included as strings. Including an undefined name in __all__ is likely to raise NameError at runtime, when the module is imported. In preview, this rule will flag undefined names in __init__.py file, even if those names implicitly refer to other modules in the package. Users that rely on implicit exports should disable this rule in __init__.py files via lint.per-file-ignores. Example from foo import bar __all__ = [\"bar\", \"baz\"] # undefined name `baz` in `__all__` Use instead: from foo import bar, baz __all__ = [\"bar\", \"baz\"]",
    "option": {}
  },
  "F823": {
    "description": "[Status: stable] [Name: undefined-local] [Message: Local variable <code>{name}</code> referenced before assignment] What it does Checks for undefined local variables. Why is this bad? Referencing a local variable before it has been assigned will raise an UnboundLocalError at runtime. Example x = 1 def foo(): x += 1 Use instead: x = 1 def foo(): global x x += 1",
    "option": {}
  },
  "F841": {
    "description": "[Status: stable] [Name: unused-variable] [Message: Local variable <code>{name}</code> is assigned to but never used] What it does Checks for the presence of unused variables in function scopes. Why is this bad? A variable that is defined but not used is likely a mistake, and should be removed to avoid confusion. If a variable is intentionally defined-but-not-used, it should be prefixed with an underscore, or some other value that adheres to the lint.dummy-variable-rgx pattern. Example def foo(): x = 1 y = 2 return x Use instead: def foo(): x = 1 return x Fix safety This rule's fix is marked as unsafe because removing an unused variable assignment may delete comments that are attached to the assignment. See also This rule does not apply to bindings in unpacked assignments (e.g. x, y = 1, 2). See unused-unpacked-variable for this case.",
    "option": {
      "lint.dummy-variable-rgx": {
        "option_description": "A regular expression used to identify \"dummy\" variables, or those which\nshould be ignored when enforcing (e.g.) unused-variable rules. The\ndefault expression matches _, __, and _var, but not _var_.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\n# Only ignore variables named \"_\".\ndummy-variable-rgx = \"^_$\"\n\n\n\n[lint]\n# Only ignore variables named \"_\".\ndummy-variable-rgx = \"^_$\"",
        "option_data_type": "str",
        "default_value": "\"^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "F842": {
    "description": "[Status: stable] [Name: unused-annotation] [Message: Local variable <code>{name}</code> is annotated but never used] What it does Checks for local variables that are annotated but never used. Why is this bad? Annotations are used to provide type hints to static type checkers. If a variable is annotated but never used, the annotation is unnecessary. Example def foo(): bar: int",
    "option": {
      "lint.dummy-variable-rgx": {
        "option_description": "A regular expression used to identify \"dummy\" variables, or those which\nshould be ignored when enforcing (e.g.) unused-variable rules. The\ndefault expression matches _, __, and _var, but not _var_.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\n# Only ignore variables named \"_\".\ndummy-variable-rgx = \"^_$\"\n\n\n\n[lint]\n# Only ignore variables named \"_\".\ndummy-variable-rgx = \"^_$\"",
        "option_data_type": "str",
        "default_value": "\"^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "F901": {
    "description": "[Status: stable] [Name: raise-not-implemented] [Message: <code>raise NotImplemented</code> should be <code>raise NotImplementedError</code>] What it does Checks for raise statements that raise NotImplemented. Why is this bad? NotImplemented is an exception used by binary special methods to indicate that an operation is not implemented with respect to a particular type. NotImplemented should not be raised directly. Instead, raise NotImplementedError, which is used to indicate that the method is abstract or not implemented in the derived class. Example class Foo: def bar(self): raise NotImplemented Use instead: class Foo: def bar(self): raise NotImplementedError",
    "option": {}
  },
  "PGH003": {
    "description": "[Status: stable] [Name: blanket-type-ignore] [Message: Use specific rule codes when ignoring type issues] What it does Check for type: ignore annotations that suppress all type warnings, as opposed to targeting specific type warnings. Why is this bad? Suppressing all warnings can hide issues in the code. Blanket type: ignore annotations are also more difficult to interpret and maintain, as the annotation does not clarify which warnings are intended to be suppressed. Example from foo import secrets # type: ignore Use instead: from foo import secrets # type: ignore[attr-defined]",
    "option": {}
  },
  "PGH004": {
    "description": "[Status: stable] [Name: blanket-noqa] [Message: Use specific rule codes when using <code>noqa</code>] What it does Check for noqa annotations that suppress all diagnostics, as opposed to targeting specific diagnostics. Why is this bad? Suppressing all diagnostics can hide issues in the code. Blanket noqa annotations are also more difficult to interpret and maintain, as the annotation does not clarify which diagnostics are intended to be suppressed. Example from .base import * # noqa Use instead: from .base import * # noqa: F403 Fix safety This rule will attempt to fix blanket noqa annotations that appear to be unintentional. For example, given # noqa F401, the rule will suggest inserting a colon, as in # noqa: F401. While modifying noqa comments is generally safe, doing so may introduce additional diagnostics.",
    "option": {}
  },
  "PGH005": {
    "description": "[Status: stable] [Name: invalid-mock-access] [Message: Mock method should be called: <code>{name}</code>] What it does Checks for common mistakes when using mock objects. Why is this bad? The mock module exposes an assertion API that can be used to verify that mock objects undergo expected interactions. This rule checks for common mistakes when using this API. For example, it checks for mock attribute accesses that should be replaced with mock method calls. Example my_mock.assert_called Use instead: my_mock.assert_called()",
    "option": {}
  },
  "PLC0105": {
    "description": "[Status: stable] [Name: type-name-incorrect-variance] [Message: <code>{kind}</code> name \"{param_name}\" does not reflect its {variance}; consider renaming it to \"{replacement_name}\"] What it does Checks for type names that do not match the variance of their associated type parameter. Why is this bad? PEP 484 recommends the use of the _co and _contra suffixes for covariant and contravariant type parameters, respectively (while invariant type parameters should not have any such suffix). Example from typing import TypeVar T = TypeVar(\"T\", covariant=True) U = TypeVar(\"U\", contravariant=True) V_co = TypeVar(\"V_co\") Use instead: from typing import TypeVar T_co = TypeVar(\"T_co\", covariant=True) U_contra = TypeVar(\"U_contra\", contravariant=True) V = TypeVar(\"V\")",
    "option": {}
  },
  "PLC0131": {
    "description": "[Status: stable] [Name: type-bivariance] [Message: <code>{kind}</code> cannot be both covariant and contravariant] What it does Checks for TypeVar and ParamSpec definitions in which the type is both covariant and contravariant. Why is this bad? By default, Python's generic types are invariant, but can be marked as either covariant or contravariant via the covariant and contravariant keyword arguments. While the API does allow you to mark a type as both covariant and contravariant, this is not supported by the type system, and should be avoided. Instead, change the variance of the type to be either covariant, contravariant, or invariant. If you want to describe both covariance and contravariance, consider using two separate type parameters. For context: an \"invariant\" generic type only accepts values that exactly match the type parameter; for example, list[Dog] accepts only list[Dog], not list[Animal] (superclass) or list[Bulldog] (subclass). This is the default behavior for Python's generic types. A \"covariant\" generic type accepts subclasses of the type parameter; for example, Sequence[Animal] accepts Sequence[Dog]. A \"contravariant\" generic type accepts superclasses of the type parameter; for example, Callable[Dog] accepts Callable[Animal]. Example from typing import TypeVar T = TypeVar(\"T\", covariant=True, contravariant=True) Use instead: from typing import TypeVar T_co = TypeVar(\"T_co\", covariant=True) T_contra = TypeVar(\"T_contra\", contravariant=True)",
    "option": {}
  },
  "PLC0132": {
    "description": "[Status: stable] [Name: type-param-name-mismatch] [Message: <code>{kind}</code> name <code>{param_name}</code> does not match assigned variable name <code>{var_name}</code>] What it does Checks for TypeVar, TypeVarTuple, ParamSpec, and NewType definitions in which the name of the type parameter does not match the name of the variable to which it is assigned. Why is this bad? When defining a TypeVar or a related type parameter, Python allows you to provide a name for the type parameter. According to PEP 484, the name provided to the TypeVar constructor must be equal to the name of the variable to which it is assigned. Example from typing import TypeVar T = TypeVar(\"U\") Use instead: from typing import TypeVar T = TypeVar(\"T\")",
    "option": {}
  },
  "PLC0205": {
    "description": "[Status: stable] [Name: single-string-slots] [Message: Class <code>__slots__</code> should be a non-string iterable] What it does Checks for single strings assigned to __slots__. Why is this bad? In Python, the __slots__ attribute allows you to explicitly define the attributes (instance variables) that a class can have. By default, Python uses a dictionary to store an object's attributes, which incurs some memory overhead. However, when __slots__ is defined, Python uses a more compact internal structure to store the object's attributes, resulting in memory savings. Any string iterable may be assigned to __slots__ (most commonly, a tuple of strings). If a string is assigned to __slots__, it is interpreted as a single attribute name, rather than an iterable of attribute names. This can cause confusion, as users that iterate over the __slots__ value may expect to iterate over a sequence of attributes, but would instead iterate over the characters of the string. To use a single string attribute in __slots__, wrap the string in an iterable container type, like a tuple. Example class Person: __slots__: str = \"name\" def __init__(self, name: str) -> None: self.name = name Use instead: class Person: __slots__: tuple[str, ...] = (\"name\",) def __init__(self, name: str) -> None: self.name = name",
    "option": {}
  },
  "PLC0206": {
    "description": "[Status: stable] [Name: dict-index-missing-items] [Message: Extracting value from dictionary without calling <code>.items()</code>] What it does Checks for dictionary iterations that extract the dictionary value via explicit indexing, instead of using .items(). Why is this bad? Iterating over a dictionary with .items() is semantically clearer and more efficient than extracting the value with the key. Example ORCHESTRA = { \"violin\": \"strings\", \"oboe\": \"woodwind\", \"tuba\": \"brass\", \"gong\": \"percussion\", } for instrument in ORCHESTRA: print(f\"{instrument}: {ORCHESTRA[instrument]}\") Use instead: ORCHESTRA = { \"violin\": \"strings\", \"oboe\": \"woodwind\", \"tuba\": \"brass\", \"gong\": \"percussion\", } for instrument, section in ORCHESTRA.items(): print(f\"{instrument}: {section}\")",
    "option": {}
  },
  "PLC0207": {
    "description": "[Status: preview] [Name: missing-maxsplit-arg] [Message: Replace with <code>{suggested_split_type}(..., maxsplit=1)</code>.] What it does Checks for access to the first or last element of str.split() or str.rsplit() without maxsplit=1 Why is this bad? Calling str.split() or str.rsplit() without passing maxsplit=1 splits on every delimiter in the string. When accessing only the first or last element of the result, it would be more efficient to only split once. Example url = \"www.example.com\" prefix = url.split(\".\")[0] Use instead: url = \"www.example.com\" prefix = url.split(\".\", maxsplit=1)[0] To access the last element, use str.rsplit() instead of str.split(): url = \"www.example.com\" suffix = url.rsplit(\".\", maxsplit=1)[-1] Fix Safety This rule's fix is marked as unsafe for split()/rsplit() calls that contain *args or **kwargs arguments, as adding a maxsplit argument to such a call may lead to duplicated arguments.",
    "option": {}
  },
  "PLC0208": {
    "description": "[Status: stable] [Name: iteration-over-set] [Message: Use a sequence type instead of a <code>set</code> when iterating over values] What it does Checks for iteration over a set literal where each element in the set is itself a literal value. Why is this bad? Iterating over a set is less efficient than iterating over a sequence type, like list or tuple. Example for number in {1, 2, 3}: ... Use instead: for number in (1, 2, 3): ...",
    "option": {}
  },
  "PLC0414": {
    "description": "[Status: stable] [Name: useless-import-alias] [Message: Import alias does not rename original package] What it does Checks for import aliases that do not rename the original package. This rule does not apply in __init__.py files. Why is this bad? The import alias is redundant and should be removed to avoid confusion. Fix safety This fix is marked as always unsafe because the user may be intentionally re-exporting the import. While statements like import numpy as numpy appear redundant, they can have semantic meaning in certain contexts. Example import numpy as numpy Use instead: import numpy as np or import numpy",
    "option": {
      "lint.isort.required-imports": {
        "option_description": "Add the specified import line to all files.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.isort]\nrequired-imports = [\"from __future__ import annotations\"]\n\n\n\n[lint.isort]\nrequired-imports = [\"from __future__ import annotations\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PLC0415": {
    "description": "[Status: stable] [Name: import-outside-top-level] [Message: <code>import</code> should be at the top-level of a file] What it does Checks for import statements outside of a module's top-level scope, such as within a function or class definition. Why is this bad? PEP 8 recommends placing imports not only at the top-level of a module, but at the very top of the file, \"just after any module comments and docstrings, and before module globals and constants.\" import statements have effects that are global in scope; defining them at the top level has a number of benefits. For example, it makes it easier to identify the dependencies of a module, and ensures that any invalid imports are caught regardless of whether a specific function is called or class is instantiated. An import statement would typically be placed within a function only to avoid a circular dependency, to defer a costly module load, or to avoid loading a dependency altogether in a certain runtime environment. Example def print_python_version(): import platform print(platform.python_version()) Use instead: import platform def print_python_version(): print(platform.python_version()) See also This rule will ignore import statements configured in lint.flake8-tidy-imports.banned-module-level-imports if the rule banned-module-level-imports is enabled.",
    "option": {}
  },
  "PLC1802": {
    "description": "[Status: stable] [Name: len-test] [Message: <code>len({expression})</code> used as condition without comparison] What it does Checks for len calls on sequences in a boolean test context. Why is this bad? Empty sequences are considered false in a boolean context. You can either remove the call to len or compare the length against a scalar. Example fruits = [\"orange\", \"apple\"] vegetables = [] if len(fruits): print(fruits) if not len(vegetables): print(vegetables) Use instead: fruits = [\"orange\", \"apple\"] vegetables = [] if fruits: print(fruits) if not vegetables: print(vegetables) Fix safety This rule's fix is marked as unsafe when the len call includes a comment, as the comment would be removed. For example, the fix would be marked as unsafe in the following case: fruits = [] if len( fruits # comment ): ...",
    "option": {}
  },
  "PLC1901": {
    "description": "[Status: preview] [Name: compare-to-empty-string] [Message: <code>{existing}</code> can be simplified to <code>{replacement}</code> as an empty string is falsey] What it does Checks for comparisons to empty strings. Why is this bad? An empty string is falsy, so it is unnecessary to compare it to \"\". If the value can be something else Python considers falsy, such as None, 0, or another empty container, then the code is not equivalent. Known problems High false positive rate, as the check is context-insensitive and does not consider the type of the variable being compared (#4282). Example x: str = ... if x == \"\": print(\"x is empty\") Use instead: x: str = ... if not x: print(\"x is empty\")",
    "option": {}
  },
  "PLC2401": {
    "description": "[Status: stable] [Name: non-ascii-name] [Message: {kind} name <code>{name}</code> contains a non-ASCII character] What it does Checks for the use of non-ASCII characters in variable names. Why is this bad? The use of non-ASCII characters in variable names can cause confusion and compatibility issues (see: PEP 672). Example pple_count: int Use instead: apple_count: int",
    "option": {}
  },
  "PLC2403": {
    "description": "[Status: stable] [Name: non-ascii-import-name] [Message: Module alias <code>{name}</code> contains a non-ASCII character] What it does Checks for the use of non-ASCII characters in import statements. Why is this bad? The use of non-ASCII characters in import statements can cause confusion and compatibility issues (see: PEP 672). Example import br Use instead: import bar If the module is third-party, use an ASCII-only alias: import br as bar",
    "option": {}
  },
  "PLC2701": {
    "description": "[Status: preview] [Name: import-private-name] [Message: Private name import <code>{name}</code> from external module <code>{module}</code>] What it does Checks for import statements that import a private name (a name starting with an underscore _) from another module. Why is this bad? PEP 8 states that names starting with an underscore are private. Thus, they are not intended to be used outside of the module in which they are defined. Further, as private imports are not considered part of the public API, they are prone to unexpected changes, especially outside of semantic versioning. Instead, consider using the public API of the module. This rule ignores private name imports that are exclusively used in type annotations. Ideally, types would be public; however, this is not always possible when using third-party libraries. Known problems Does not ignore private name imports from within the module that defines the private name if the module is defined with PEP 420 namespace packages (i.e., directories that omit the __init__.py file). Namespace packages must be configured via the namespace-packages setting. Example from foo import _bar",
    "option": {
      "namespace-packages": {
        "option_description": "Mark the specified directories as namespace packages. For the purpose of\nmodule resolution, Ruff will treat those directories and all their subdirectories\nas if they contained an __init__.py file.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\nnamespace-packages = [\"airflow/providers\"]\n\n\n\nnamespace-packages = [\"airflow/providers\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PLC2801": {
    "description": "[Status: preview] [Name: unnecessary-dunder-call] [Message: Unnecessary dunder call to <code>{method}</code>. {replacement}.] What it does Checks for explicit use of dunder methods, like __str__ and __add__. Why is this bad? Dunder names are not meant to be called explicitly and, in most cases, can be replaced with builtins or operators. Fix safety This fix is always unsafe. When replacing dunder method calls with operators or builtins, the behavior can change in the following ways: Types may implement only a subset of related dunder methods. Calling a missing dunder method directly returns NotImplemented, but using the equivalent operator raises a TypeError. class C: pass c = C() c.__gt__(1) # before fix: NotImplemented c > 1 # after fix: raises TypeError Instance-assigned dunder methods are ignored by operators and builtins. class C: pass c = C() c.__bool__ = lambda: False c.__bool__() # before fix: False bool(c) # after fix: True Even with built-in types, behavior can differ. (1).__gt__(1.0) # before fix: NotImplemented 1 > 1.0 # after fix: False Example three = (3.0).__str__() twelve = \"1\".__add__(\"2\") def is_greater_than_two(x: int) -> bool: return x.__gt__(2) Use instead: three = str(3.0) twelve = \"1\" + \"2\" def is_greater_than_two(x: int) -> bool: return x > 2",
    "option": {}
  },
  "PLC3002": {
    "description": "[Status: stable] [Name: unnecessary-direct-lambda-call] [Message: Lambda expression called directly. Execute the expression inline instead.] What it does Checks for unnecessary direct calls to lambda expressions. Why is this bad? Calling a lambda expression directly is unnecessary. The expression can be executed inline instead to improve readability. Example area = (lambda r: 3.14 * r**2)(radius) Use instead: area = 3.14 * radius**2",
    "option": {}
  },
  "PLE0100": {
    "description": "[Status: stable] [Name: yield-in-init] [Message: <code>__init__</code> method is a generator] What it does Checks for __init__ methods that are turned into generators by the inclusion of yield or yield from expressions. Why is this bad? The __init__ method is the constructor for a given Python class, responsible for initializing, rather than creating, new objects. The __init__ method has to return None. By including a yield or yield from expression in an __init__, the method will return a generator object when called at runtime, resulting in a runtime error. Example class InitIsGenerator: def __init__(self, i): yield i",
    "option": {}
  },
  "PLE0101": {
    "description": "[Status: stable] [Name: return-in-init] [Message: Explicit return in <code>__init__</code>] What it does Checks for __init__ methods that return values. Why is this bad? The __init__ method is the constructor for a given Python class, responsible for initializing, rather than creating, new objects. The __init__ method has to return None. Returning any value from an __init__ method will result in a runtime error. Example class Example: def __init__(self): return [] Use instead: class Example: def __init__(self): self.value = []",
    "option": {}
  },
  "PLE0115": {
    "description": "[Status: stable] [Name: nonlocal-and-global] [Message: Name <code>{name}</code> is both <code>nonlocal</code> and <code>global</code>] What it does Checks for variables which are both declared as both nonlocal and global. Why is this bad? A nonlocal variable is a variable that is defined in the nearest enclosing scope, but not in the global scope, while a global variable is a variable that is defined in the global scope. Declaring a variable as both nonlocal and global is contradictory and will raise a SyntaxError. Example counter = 0 def increment(): global counter nonlocal counter counter += 1 Use instead: counter = 0 def increment(): global counter counter += 1",
    "option": {}
  },
  "PLE0116": {
    "description": "[Status: stable] [Name: continue-in-finally] [Message: <code>continue</code> not supported inside <code>finally</code> clause] What it does Checks for continue statements inside finally Why is this bad? continue statements were not allowed within finally clauses prior to Python 3.8. Using a continue statement within a finally clause can cause a SyntaxError. Example while True: try: pass finally: continue Use instead: while True: try: pass except Exception: pass else: continue",
    "option": {
      "target-version": {
        "option_description": "The minimum Python version to target, e.g., when considering automatic\ncode upgrades, like rewriting type annotations. Ruff will not propose\nchanges using features that are not available in the given version.\n\nFor example, to represent supporting Python >=3.11 or ==3.11\nspecify target-version = \"py311\".\n\nIf you're already using a pyproject.toml file, we recommend\nproject.requires-python instead, as it's based on Python packaging\nstandards, and will be respected by other tools. For example, Ruff\ntreats the following as identical to target-version = \"py38\":\n\n[project]\nrequires-python = \">=3.8\"\n\nIf both are specified, target-version takes precedence over\nrequires-python. See Inferring the Python version\nfor a complete description of how the target-version is determined\nwhen left unspecified.\n\nNote that a stub file can sometimes make use of a typing feature\nbefore it is available at runtime, as long as the stub does not make\nuse of new syntax. For example, a type checker will understand\nint | str in a stub as being a Union type annotation, even if the\ntype checker is run using Python 3.9, despite the fact that the |\noperator can only be used to create union types at runtime on Python\n3.10+. As such, Ruff will often recommend newer features in a stub\nfile than it would for an equivalent runtime file with the same target\nversion.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"\n\n\n\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"",
        "option_data_type": "\"py37\" | \"py38\" | \"py39\" | \"py310\" | \"py311\" | \"py312\" | \"py313\" | \"py314\"",
        "default_value": "\"py310\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PLE0117": {
    "description": "[Status: stable] [Name: nonlocal-without-binding] [Message: Nonlocal name <code>{name}</code> found without binding] What it does Checks for nonlocal names without bindings. Why is this bad? nonlocal names must be bound to a name in an outer scope. Violating this rule leads to a SyntaxError at runtime. Example def foo(): def get_bar(self): nonlocal bar ... Use instead: def foo(): bar = 1 def get_bar(self): nonlocal bar ...",
    "option": {}
  },
  "PLE0118": {
    "description": "[Status: stable] [Name: load-before-global-declaration] [Message: Name <code>{name}</code> is used prior to global declaration on {row}] What it does Checks for uses of names that are declared as global prior to the relevant global declaration. Why is this bad? The global declaration applies to the entire scope. Using a name that's declared as global in a given scope prior to the relevant global declaration is a SyntaxError. Example counter = 1 def increment(): print(f\"Adding 1 to {counter}\") global counter counter += 1 Use instead: counter = 1 def increment(): global counter print(f\"Adding 1 to {counter}\") counter += 1",
    "option": {}
  },
  "PLE0237": {
    "description": "[Status: stable] [Name: non-slot-assignment] [Message: Attribute <code>{name}</code> is not defined in class's <code>__slots__</code>] What it does Checks for assignments to attributes that are not defined in __slots__. Why is this bad? When using __slots__, only the specified attributes are allowed. Attempting to assign to an attribute that is not defined in __slots__ will result in an AttributeError at runtime. Known problems This rule can't detect __slots__ implementations in superclasses, and so limits its analysis to classes that inherit from (at most) object. Example class Student: __slots__ = (\"name\",) def __init__(self, name, surname): self.name = name self.surname = surname # [assigning-non-slot] self.setup() def setup(self): pass Use instead: class Student: __slots__ = (\"name\", \"surname\") def __init__(self, name, surname): self.name = name self.surname = surname self.setup() def setup(self): pass",
    "option": {}
  },
  "PLE0241": {
    "description": "[Status: stable] [Name: duplicate-bases] [Message: Duplicate base <code>{base}</code> for class <code>{class}</code>] What it does Checks for duplicate base classes in class definitions. Why is this bad? Including duplicate base classes will raise a TypeError at runtime. Example class Foo: pass class Bar(Foo, Foo): pass Use instead: class Foo: pass class Bar(Foo): pass Fix safety This rule's fix is marked as unsafe if there's comments in the base classes, as comments may be removed. For example, the fix would be marked as unsafe in the following case: class Foo: pass class Bar( Foo, # comment Foo, ): pass",
    "option": {}
  },
  "PLE0302": {
    "description": "[Status: stable] [Name: unexpected-special-method-signature] [Message: The special method <code>{}</code> expects {}, {} {} given] What it does Checks for \"special\" methods that have an unexpected method signature. Why is this bad? \"Special\" methods, like __len__, are expected to adhere to a specific, standard function signature. Implementing a \"special\" method using a non-standard function signature can lead to unexpected and surprising behavior for users of a given class. Example class Bookshelf: def __init__(self): self._books = [\"Foo\", \"Bar\", \"Baz\"] def __len__(self, index): # __len__ does not except an index parameter return len(self._books) def __getitem__(self, index): return self._books[index] Use instead: class Bookshelf: def __init__(self): self._books = [\"Foo\", \"Bar\", \"Baz\"] def __len__(self): return len(self._books) def __getitem__(self, index): return self._books[index]",
    "option": {}
  },
  "PLE0303": {
    "description": "[Status: stable] [Name: invalid-length-return-type] [Message: <code>__len__</code> does not return a non-negative integer] What it does Checks for __len__ implementations that return values that are not non-negative integers. Why is this bad? The __len__ method should return a non-negative integer. Returning a different value may cause unexpected behavior. Note: bool is a subclass of int, so it's technically valid for __len__ to return True or False. However, for consistency with other rules, Ruff will still emit a diagnostic when __len__ returns a bool. Example class Foo: def __len__(self): return \"2\" Use instead: class Foo: def __len__(self): return 2",
    "option": {}
  },
  "PLE0304": {
    "description": "[Status: preview] [Name: invalid-bool-return-type] [Message: <code>__bool__</code> does not return <code>bool</code>] What it does Checks for __bool__ implementations that return a type other than bool. Why is this bad? The __bool__ method should return a bool object. Returning a different type may cause unexpected behavior. Example class Foo: def __bool__(self): return 2 Use instead: class Foo: def __bool__(self): return True",
    "option": {}
  },
  "PLE0305": {
    "description": "[Status: stable] [Name: invalid-index-return-type] [Message: <code>__index__</code> does not return an integer] What it does Checks for __index__ implementations that return non-integer values. Why is this bad? The __index__ method should return an integer. Returning a different type may cause unexpected behavior. Note: bool is a subclass of int, so it's technically valid for __index__ to return True or False. However, a DeprecationWarning (DeprecationWarning: __index__ returned non-int (type bool)) for such cases was already introduced, thus this is a conscious difference between the original pylint rule and the current ruff implementation. Example class Foo: def __index__(self): return \"2\" Use instead: class Foo: def __index__(self): return 2",
    "option": {}
  },
  "PLE0307": {
    "description": "[Status: stable] [Name: invalid-str-return-type] [Message: <code>__str__</code> does not return <code>str</code>] What it does Checks for __str__ implementations that return a type other than str. Why is this bad? The __str__ method should return a str object. Returning a different type may cause unexpected behavior. Example class Foo: def __str__(self): return True Use instead: class Foo: def __str__(self): return \"Foo\"",
    "option": {}
  },
  "PLE0308": {
    "description": "[Status: stable] [Name: invalid-bytes-return-type] [Message: <code>__bytes__</code> does not return <code>bytes</code>] What it does Checks for __bytes__ implementations that return types other than bytes. Why is this bad? The __bytes__ method should return a bytes object. Returning a different type may cause unexpected behavior. Example class Foo: def __bytes__(self): return 2 Use instead: class Foo: def __bytes__(self): return b\"2\"",
    "option": {}
  },
  "PLE0309": {
    "description": "[Status: stable] [Name: invalid-hash-return-type] [Message: <code>__hash__</code> does not return an integer] What it does Checks for __hash__ implementations that return non-integer values. Why is this bad? The __hash__ method should return an integer. Returning a different type may cause unexpected behavior. Note: bool is a subclass of int, so it's technically valid for __hash__ to return True or False. However, for consistency with other rules, Ruff will still emit a diagnostic when __hash__ returns a bool. Example class Foo: def __hash__(self): return \"2\" Use instead: class Foo: def __hash__(self): return 2",
    "option": {}
  },
  "PLE0604": {
    "description": "[Status: stable] [Name: invalid-all-object] [Message: Invalid object in <code>__all__</code>, must contain only strings] What it does Checks for the inclusion of invalid objects in __all__. Why is this bad? In Python, __all__ should contain a sequence of strings that represent the names of all \"public\" symbols exported by a module. Assigning anything other than a tuple or list of strings to __all__ is invalid. Example __all__ = [Foo, 1, None] Use instead: __all__ = [\"Foo\", \"Bar\", \"Baz\"]",
    "option": {}
  },
  "PLE0605": {
    "description": "[Status: stable] [Name: invalid-all-format] [Message: Invalid format for <code>__all__</code>, must be <code>tuple</code> or <code>list</code>] What it does Checks for invalid assignments to __all__. Why is this bad? In Python, __all__ should contain a sequence of strings that represent the names of all \"public\" symbols exported by a module. Assigning anything other than a tuple or list of strings to __all__ is invalid. Example __all__ = \"Foo\" Use instead: __all__ = (\"Foo\",)",
    "option": {}
  },
  "PLE0643": {
    "description": "[Status: stable] [Name: potential-index-error] [Message: Expression is likely to raise <code>IndexError</code>] What it does Checks for hard-coded sequence accesses that are known to be out of bounds. Why is this bad? Attempting to access a sequence with an out-of-bounds index will cause an IndexError to be raised at runtime. When the sequence and index are defined statically (e.g., subscripts on list and tuple literals, with integer indexes), such errors can be detected ahead of time. Example print([0, 1, 2][3])",
    "option": {}
  },
  "PLE0704": {
    "description": "[Status: stable] [Name: misplaced-bare-raise] [Message: Bare <code>raise</code> statement is not inside an exception handler] What it does Checks for bare raise statements outside of exception handlers. Why is this bad? A bare raise statement without an exception object will re-raise the last exception that was active in the current scope, and is typically used within an exception handler to re-raise the caught exception. If a bare raise is used outside of an exception handler, it will generate an error due to the lack of an active exception. Note that a bare raise within a finally block will work in some cases (namely, when the exception is raised within the try block), but should be avoided as it can lead to confusing behavior. Example from typing import Any def is_some(obj: Any) -> bool: if obj is None: raise Use instead: from typing import Any def is_some(obj: Any) -> bool: if obj is None: raise ValueError(\"`obj` cannot be `None`\")",
    "option": {}
  },
  "PLE1132": {
    "description": "[Status: stable] [Name: repeated-keyword-argument] [Message: Repeated keyword argument: <code>{duplicate_keyword}</code>] What it does Checks for repeated keyword arguments in function calls. Why is this bad? Python does not allow repeated keyword arguments in function calls. If a function is called with the same keyword argument multiple times, the interpreter will raise an exception. Example func(1, 2, c=3, **{\"c\": 4})",
    "option": {}
  },
  "PLE1141": {
    "description": "[Status: preview] [Name: dict-iter-missing-items] [Message: Unpacking a dictionary in iteration without calling <code>.items()</code>] What it does Checks for dictionary unpacking in a for loop without calling .items(). Why is this bad? When iterating over a dictionary in a for loop, if a dictionary is unpacked without calling .items(), it could lead to a runtime error if the keys are not a tuple of two elements. It is likely that you're looking for an iteration over (key, value) pairs which can only be achieved when calling .items(). Example data = {\"Paris\": 2_165_423, \"New York City\": 8_804_190, \"Tokyo\": 13_988_129} for city, population in data: print(f\"{city} has population {population}.\") Use instead: data = {\"Paris\": 2_165_423, \"New York City\": 8_804_190, \"Tokyo\": 13_988_129} for city, population in data.items(): print(f\"{city} has population {population}.\") Known problems If the dictionary key is a tuple, e.g.: d = {(1, 2): 3, (3, 4): 5} for x, y in d: print(x, y) The tuple key is unpacked into x and y instead of the key and values. This means that the suggested fix of using d.items() would result in different runtime behavior. Ruff cannot consistently infer the type of a dictionary's keys. Fix safety Due to the known problem with tuple keys, this fix is unsafe.",
    "option": {}
  },
  "PLE1142": {
    "description": "[Status: stable] [Name: await-outside-async] [Message: <code>await</code> should be used within an async function] What it does Checks for uses of await outside async functions. Why is this bad? Using await outside an async function is a syntax error. Example import asyncio def foo(): await asyncio.sleep(1) Use instead: import asyncio async def foo(): await asyncio.sleep(1) Notebook behavior As an exception, await is allowed at the top level of a Jupyter notebook (see: autoawait).",
    "option": {}
  },
  "PLE1205": {
    "description": "[Status: stable] [Name: logging-too-many-args] [Message: Too many arguments for <code>logging</code> format string] What it does Checks for too many positional arguments for a logging format string. Why is this bad? A TypeError will be raised if the statement is run. Example import logging try: function() except Exception as e: logging.error(\"Error occurred: %s\", type(e), e) raise Use instead: import logging try: function() except Exception as e: logging.error(\"%s error occurred: %s\", type(e), e) raise",
    "option": {
      "lint.logger-objects": {
        "option_description": "A list of objects that should be treated equivalently to a\nlogging.Logger object.\n\nThis is useful for ensuring proper diagnostics (e.g., to identify\nlogging deprecations and other best-practices) for projects that\nre-export a logging.Logger object from a common module.\n\nFor example, if you have a module logging_setup.py with the following\ncontents:\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nAdding \"logging_setup.logger\" to logger-objects will ensure that\nlogging_setup.logger is treated as a logging.Logger object when\nimported from other modules (e.g., from logging_setup import logger).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\nlogger-objects = [\"logging_setup.logger\"]\n\n\n\n[lint]\nlogger-objects = [\"logging_setup.logger\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PLE1206": {
    "description": "[Status: stable] [Name: logging-too-few-args] [Message: Not enough arguments for <code>logging</code> format string] What it does Checks for too few positional arguments for a logging format string. Why is this bad? A TypeError will be raised if the statement is run. Example import logging try: function() except Exception as e: logging.error(\"%s error occurred: %s\", e) raise Use instead: import logging try: function() except Exception as e: logging.error(\"%s error occurred: %s\", type(e), e) raise",
    "option": {
      "lint.logger-objects": {
        "option_description": "A list of objects that should be treated equivalently to a\nlogging.Logger object.\n\nThis is useful for ensuring proper diagnostics (e.g., to identify\nlogging deprecations and other best-practices) for projects that\nre-export a logging.Logger object from a common module.\n\nFor example, if you have a module logging_setup.py with the following\ncontents:\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nAdding \"logging_setup.logger\" to logger-objects will ensure that\nlogging_setup.logger is treated as a logging.Logger object when\nimported from other modules (e.g., from logging_setup import logger).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\nlogger-objects = [\"logging_setup.logger\"]\n\n\n\n[lint]\nlogger-objects = [\"logging_setup.logger\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PLE1300": {
    "description": "[Status: stable] [Name: bad-string-format-character] [Message: Unsupported format character '{format_char}'] What it does Checks for unsupported format types in format strings. Why is this bad? An invalid format string character will result in an error at runtime. Example # `z` is not a valid format type. print(\"%z\" % \"1\") print(\"{:z}\".format(\"1\"))",
    "option": {}
  },
  "PLE1307": {
    "description": "[Status: stable] [Name: bad-string-format-type] [Message: Format type does not match argument type] What it does Checks for mismatched argument types in \"old-style\" format strings. Why is this bad? The format string is not checked at compile time, so it is easy to introduce bugs by mistyping the format string. Example print(\"%d\" % \"1\") Use instead: print(\"%d\" % 1)",
    "option": {}
  },
  "PLE1310": {
    "description": "[Status: stable] [Name: bad-str-strip-call] [Message: String <code>{strip}</code> call contains duplicate characters (did you mean <code>{removal}</code>?)] What it does Checks duplicate characters in str.strip calls. Why is this bad? All characters in str.strip calls are removed from both the leading and trailing ends of the string. Including duplicate characters in the call is redundant and often indicative of a mistake. In Python 3.9 and later, you can use str.removeprefix and str.removesuffix to remove an exact prefix or suffix from a string, respectively, which should be preferred when possible. Example # Evaluates to \"foo\". \"bar foo baz\".strip(\"bar baz \") Use instead: # Evaluates to \"foo\". \"bar foo baz\".strip(\"abrz \") # \"foo\" Or: # Evaluates to \"foo\". \"bar foo baz\".removeprefix(\"bar \").removesuffix(\" baz\")",
    "option": {
      "target-version": {
        "option_description": "The minimum Python version to target, e.g., when considering automatic\ncode upgrades, like rewriting type annotations. Ruff will not propose\nchanges using features that are not available in the given version.\n\nFor example, to represent supporting Python >=3.11 or ==3.11\nspecify target-version = \"py311\".\n\nIf you're already using a pyproject.toml file, we recommend\nproject.requires-python instead, as it's based on Python packaging\nstandards, and will be respected by other tools. For example, Ruff\ntreats the following as identical to target-version = \"py38\":\n\n[project]\nrequires-python = \">=3.8\"\n\nIf both are specified, target-version takes precedence over\nrequires-python. See Inferring the Python version\nfor a complete description of how the target-version is determined\nwhen left unspecified.\n\nNote that a stub file can sometimes make use of a typing feature\nbefore it is available at runtime, as long as the stub does not make\nuse of new syntax. For example, a type checker will understand\nint | str in a stub as being a Union type annotation, even if the\ntype checker is run using Python 3.9, despite the fact that the |\noperator can only be used to create union types at runtime on Python\n3.10+. As such, Ruff will often recommend newer features in a stub\nfile than it would for an equivalent runtime file with the same target\nversion.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"\n\n\n\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"",
        "option_data_type": "\"py37\" | \"py38\" | \"py39\" | \"py310\" | \"py311\" | \"py312\" | \"py313\" | \"py314\"",
        "default_value": "\"py310\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PLE1507": {
    "description": "[Status: stable] [Name: invalid-envvar-value] [Message: Invalid type for initial <code>os.getenv</code> argument; expected <code>str</code>] What it does Checks for os.getenv calls with an invalid key argument. Why is this bad? os.getenv only supports strings as the first argument (key). If the provided argument is not a string, os.getenv will throw a TypeError at runtime. Example import os os.getenv(1) Use instead: import os os.getenv(\"1\")",
    "option": {}
  },
  "PLE1519": {
    "description": "[Status: stable] [Name: singledispatch-method] [Message: <code>@singledispatch</code> decorator should not be used on methods] What it does Checks for methods decorated with @singledispatch. Why is this bad? The @singledispatch decorator is intended for use with functions, not methods. Instead, use the @singledispatchmethod decorator, or migrate the method to a standalone function. Example from functools import singledispatch class Class: @singledispatch def method(self, arg): ... Use instead: from functools import singledispatchmethod class Class: @singledispatchmethod def method(self, arg): ... Fix safety This rule's fix is marked as unsafe, as migrating from @singledispatch to @singledispatchmethod may change the behavior of the code.",
    "option": {
      "lint.pep8-naming.classmethod-decorators": {
        "option_description": "A list of decorators that, when applied to a method, indicate that the\nmethod should be treated as a class method (in addition to the builtin\n@classmethod).\n\nFor example, Ruff will expect that any method decorated by a decorator\nin this list takes a cls argument as its first argument.\n\nExpects to receive a list of fully-qualified names (e.g., pydantic.validator,\nrather than validator) or alternatively a plain name which is then matched against\nthe last segment in case the decorator itself consists of a dotted name.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nclassmethod-decorators = [\n    # Allow Pydantic's `@validator` decorator to trigger class method treatment.\n    \"pydantic.validator\",\n    # Allow SQLAlchemy's dynamic decorators, like `@field.expression`, to trigger class method treatment.\n    \"declared_attr\",\n    \"expression\",\n    \"comparator\",\n]\n\n\n\n[lint.pep8-naming]\nclassmethod-decorators = [\n    # Allow Pydantic's `@validator` decorator to trigger class method treatment.\n    \"pydantic.validator\",\n    # Allow SQLAlchemy's dynamic decorators, like `@field.expression`, to trigger class method treatment.\n    \"declared_attr\",\n    \"expression\",\n    \"comparator\",\n]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pep8-naming.staticmethod-decorators": {
        "option_description": "A list of decorators that, when applied to a method, indicate that the\nmethod should be treated as a static method (in addition to the builtin\n@staticmethod).\n\nFor example, Ruff will expect that any method decorated by a decorator\nin this list has no self or cls argument.\n\nExpects to receive a list of fully-qualified names (e.g., belay.Device.teardown,\nrather than teardown) or alternatively a plain name which is then matched against\nthe last segment in case the decorator itself consists of a dotted name.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\n# Allow Belay's `@Device.teardown` decorator to trigger static method treatment.\nstaticmethod-decorators = [\"belay.Device.teardown\"]\n\n\n\n[lint.pep8-naming]\n# Allow Belay's `@Device.teardown` decorator to trigger static method treatment.\nstaticmethod-decorators = [\"belay.Device.teardown\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PLE1520": {
    "description": "[Status: stable] [Name: singledispatchmethod-function] [Message: <code>@singledispatchmethod</code> decorator should not be used on non-method functions] What it does Checks for non-method functions decorated with @singledispatchmethod. Why is this bad? The @singledispatchmethod decorator is intended for use with methods, not functions. Instead, use the @singledispatch decorator. Example from functools import singledispatchmethod @singledispatchmethod def func(arg): ... Use instead: from functools import singledispatch @singledispatch def func(arg): ... Fix safety This rule's fix is marked as unsafe, as migrating from @singledispatchmethod to @singledispatch may change the behavior of the code.",
    "option": {}
  },
  "PLE1700": {
    "description": "[Status: stable] [Name: yield-from-in-async-function] [Message: <code>yield from</code> statement in async function; use <code>async for</code> instead] What it does Checks for uses of yield from in async functions. Why is this bad? Python doesn't support the use of yield from in async functions, and will raise a SyntaxError in such cases. Instead, considering refactoring the code to use an async for loop instead. Example async def numbers(): yield from [1, 2, 3, 4, 5] Use instead: async def numbers(): async for number in [1, 2, 3, 4, 5]: yield number",
    "option": {}
  },
  "PLE2502": {
    "description": "[Status: stable] [Name: bidirectional-unicode] [Message: Contains control characters that can permit obfuscated code] What it does Checks for bidirectional formatting characters. Why is this bad? The interaction between bidirectional formatting characters and the surrounding code can be surprising to those that are unfamiliar with right-to-left writing systems. In some cases, bidirectional formatting characters can also be used to obfuscate code and introduce or mask security vulnerabilities. Example example = \"x\" * 100 # \"x\" is assigned The example uses two RIGHT-TO-LEFT MARKs to make the 100 * appear inside the comment. Without the RIGHT-TO-LEFT MARKs, the code looks like this: example = \"x\" * 100 # \"x\" is assigned",
    "option": {}
  },
  "PLE2510": {
    "description": "[Status: stable] [Name: invalid-character-backspace] [Message: Invalid unescaped character backspace, use \"\\b\" instead] What it does Checks for strings that contain the control character BS. Why is this bad? Control characters are displayed differently by different text editors and terminals. By using the \\b sequence in lieu of the BS control character, the string will contain the same value, but will render visibly in all editors. Example x = \"\" Use instead: x = \"\\b\"",
    "option": {}
  },
  "PLE2512": {
    "description": "[Status: stable] [Name: invalid-character-sub] [Message: Invalid unescaped character SUB, use \"\\x1a\" instead] What it does Checks for strings that contain the raw control character SUB. Why is this bad? Control characters are displayed differently by different text editors and terminals. By using the \\x1a sequence in lieu of the SUB control character, the string will contain the same value, but will render visibly in all editors. Example x = \"\" Use instead: x = \"\\x1a\"",
    "option": {}
  },
  "PLE2513": {
    "description": "[Status: stable] [Name: invalid-character-esc] [Message: Invalid unescaped character ESC, use \"\\x1b\" instead] What it does Checks for strings that contain the raw control character ESC. Why is this bad? Control characters are displayed differently by different text editors and terminals. By using the \\x1b sequence in lieu of the ESC control character, the string will contain the same value, but will render visibly in all editors. Example x = \"\" Use instead: x = \"\\x1b\"",
    "option": {}
  },
  "PLE2514": {
    "description": "[Status: stable] [Name: invalid-character-nul] [Message: Invalid unescaped character NUL, use \"\\0\" instead] What it does Checks for strings that contain the raw control character NUL (0 byte). Why is this bad? Control characters are displayed differently by different text editors and terminals. By using the \\0 sequence in lieu of the NUL control character, the string will contain the same value, but will render visibly in all editors. Example x = \"\" Use instead: x = \"\\0\"",
    "option": {}
  },
  "PLE2515": {
    "description": "[Status: stable] [Name: invalid-character-zero-width-space] [Message: Invalid unescaped character zero-width-space, use \"\\u200B\" instead] What it does Checks for strings that contain the zero width space character. Why is this bad? This character is rendered invisibly in some text editors and terminals. By using the \\u200B sequence, the string will contain the same value, but will render visibly in all editors. Example x = \"Dear Sir/Madam\" Use instead: x = \"Dear Sir\\u200b/\\u200bMadam\" # zero width space",
    "option": {}
  },
  "PLE4703": {
    "description": "[Status: preview] [Name: modified-iterating-set] [Message: Iterated set <code>{name}</code> is modified within the <code>for</code> loop] What it does Checks for loops in which a set is modified during iteration. Why is this bad? If a set is modified during iteration, it will cause a RuntimeError. If you need to modify a set within a loop, consider iterating over a copy of the set instead. Known problems This rule favors false negatives over false positives. Specifically, it will only detect variables that can be inferred to be a set type based on local type inference, and will only detect modifications that are made directly on the variable itself (e.g., set.add()), as opposed to modifications within other function calls (e.g., some_function(set)). Example nums = {1, 2, 3} for num in nums: nums.add(num + 5) Use instead: nums = {1, 2, 3} for num in nums.copy(): nums.add(num + 5) Fix safety This fix is always unsafe because it changes the programs behavior. Replacing the original set with a copy during iteration allows code that would previously raise a RuntimeError to run without error.",
    "option": {}
  },
  "PLR0124": {
    "description": "[Status: stable] [Name: comparison-with-itself] [Message: Name compared with itself, consider replacing <code>{actual}</code>] What it does Checks for operations that compare a name to itself. Why is this bad? Comparing a name to itself always results in the same value, and is likely a mistake. Example foo == foo In some cases, self-comparisons are used to determine whether a float is NaN. Instead, prefer math.isnan: import math math.isnan(foo)",
    "option": {}
  },
  "PLR0133": {
    "description": "[Status: stable] [Name: comparison-of-constant] [Message: Two constants compared in a comparison, consider replacing <code>{left_constant} {op} {right_constant}</code>] What it does Checks for comparisons between constants. Why is this bad? Comparing two constants will always resolve to the same value, so the comparison is redundant. Instead, the expression should be replaced with the result of the comparison. Example foo = 1 == 1 Use instead: foo = True",
    "option": {}
  },
  "PLR0202": {
    "description": "[Status: preview] [Name: no-classmethod-decorator] [Message: Class method defined without decorator] What it does Checks for the use of a classmethod being made without the decorator. Why is this bad? When it comes to consistency and readability, it's preferred to use the decorator. Example class Foo: def bar(cls): ... bar = classmethod(bar) Use instead: class Foo: @classmethod def bar(cls): ...",
    "option": {}
  },
  "PLR0203": {
    "description": "[Status: preview] [Name: no-staticmethod-decorator] [Message: Static method defined without decorator] What it does Checks for the use of a staticmethod being made without the decorator. Why is this bad? When it comes to consistency and readability, it's preferred to use the decorator. Example class Foo: def bar(arg1, arg2): ... bar = staticmethod(bar) Use instead: class Foo: @staticmethod def bar(arg1, arg2): ...",
    "option": {}
  },
  "PLR0206": {
    "description": "[Status: stable] [Name: property-with-parameters] [Message: Cannot have defined parameters for properties] What it does Checks for property definitions that accept function parameters. Why is this bad? Properties cannot be called with parameters. If you need to pass parameters to a property, create a method with the desired parameters and call that method instead. Example class Cat: @property def purr(self, volume): ... Use instead: class Cat: @property def purr(self): ... def purr_volume(self, volume): ...",
    "option": {
      "lint.pydocstyle.property-decorators": {
        "option_description": "A list of decorators that, when applied to a method, indicate that the\nmethod should be treated as a property (in addition to the builtin\n@property and standard-library @functools.cached_property).\n\nFor example, Ruff will expect that any method decorated by a decorator\nin this list can use a non-imperative summary line.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\nproperty-decorators = [\"gi.repository.GObject.Property\"]\n\n\n\n[lint.pydocstyle]\nproperty-decorators = [\"gi.repository.GObject.Property\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PLR0402": {
    "description": "[Status: stable] [Name: manual-from-import] [Message: Use <code>from {module} import {name}</code> in lieu of alias] What it does Checks for submodule imports that are aliased to the submodule name. Why is this bad? Using the from keyword to import the submodule is more concise and readable. Example import concurrent.futures as futures Use instead: from concurrent import futures",
    "option": {
      "lint.isort.required-imports": {
        "option_description": "Add the specified import line to all files.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.isort]\nrequired-imports = [\"from __future__ import annotations\"]\n\n\n\n[lint.isort]\nrequired-imports = [\"from __future__ import annotations\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PLR0904": {
    "description": "[Status: preview] [Name: too-many-public-methods] [Message: Too many public methods ({methods} > {max_methods})] What it does Checks for classes with too many public methods By default, this rule allows up to 20 public methods, as configured by the lint.pylint.max-public-methods option. Why is this bad? Classes with many public methods are harder to understand and maintain. Instead, consider refactoring the class into separate classes. Example Assuming that lint.pylint.max-public-methods is set to 5: class Linter: def __init__(self): pass def pylint(self): pass def pylint_settings(self): pass def flake8(self): pass def flake8_settings(self): pass def pydocstyle(self): pass def pydocstyle_settings(self): pass Use instead: class Linter: def __init__(self): self.pylint = Pylint() self.flake8 = Flake8() self.pydocstyle = Pydocstyle() def lint(self): pass class Pylint: def lint(self): pass def settings(self): pass class Flake8: def lint(self): pass def settings(self): pass class Pydocstyle: def lint(self): pass def settings(self): pass",
    "option": {
      "lint.pylint.max-public-methods": {
        "option_description": "Maximum number of public methods allowed for a class (see PLR0904).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pylint]\nmax-public-methods = 30\n\n\n\n[lint.pylint]\nmax-public-methods = 30",
        "option_data_type": "int",
        "default_value": "20",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PLR0911": {
    "description": "[Status: stable] [Name: too-many-return-statements] [Message: Too many return statements ({returns} > {max_returns})] What it does Checks for functions or methods with too many return statements. By default, this rule allows up to six return statements, as configured by the lint.pylint.max-returns option. Why is this bad? Functions or methods with many return statements are harder to understand and maintain, and often indicative of complex logic. Example def capital(country: str) -> str | None: if country == \"England\": return \"London\" elif country == \"France\": return \"Paris\" elif country == \"Poland\": return \"Warsaw\" elif country == \"Romania\": return \"Bucharest\" elif country == \"Spain\": return \"Madrid\" elif country == \"Thailand\": return \"Bangkok\" else: return None Use instead: def capital(country: str) -> str | None: capitals = { \"England\": \"London\", \"France\": \"Paris\", \"Poland\": \"Warsaw\", \"Romania\": \"Bucharest\", \"Spain\": \"Madrid\", \"Thailand\": \"Bangkok\", } return capitals.get(country)",
    "option": {
      "lint.pylint.max-returns": {
        "option_description": "Maximum number of return statements allowed for a function or method\nbody (see PLR0911)\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pylint]\nmax-returns = 10\n\n\n\n[lint.pylint]\nmax-returns = 10",
        "option_data_type": "int",
        "default_value": "6",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PLR0912": {
    "description": "[Status: stable] [Name: too-many-branches] [Message: Too many branches ({branches} > {max_branches})] What it does Checks for functions or methods with too many branches, including (nested) if, elif, and else branches, for loops, try-except clauses, and match and case statements. By default, this rule allows up to 12 branches. This can be configured using the lint.pylint.max-branches option. Why is this bad? Functions or methods with many branches are harder to understand and maintain than functions or methods with fewer branches. Example Given: def capital(country): if country == \"Australia\": return \"Canberra\" elif country == \"Brazil\": return \"Brasilia\" elif country == \"Canada\": return \"Ottawa\" elif country == \"England\": return \"London\" elif country == \"France\": return \"Paris\" elif country == \"Germany\": return \"Berlin\" elif country == \"Poland\": return \"Warsaw\" elif country == \"Romania\": return \"Bucharest\" elif country == \"Spain\": return \"Madrid\" elif country == \"Thailand\": return \"Bangkok\" elif country == \"Turkey\": return \"Ankara\" elif country == \"United States\": return \"Washington\" else: return \"Unknown\" # 13th branch Use instead: def capital(country): capitals = { \"Australia\": \"Canberra\", \"Brazil\": \"Brasilia\", \"Canada\": \"Ottawa\", \"England\": \"London\", \"France\": \"Paris\", \"Germany\": \"Berlin\", \"Poland\": \"Warsaw\", \"Romania\": \"Bucharest\", \"Spain\": \"Madrid\", \"Thailand\": \"Bangkok\", \"Turkey\": \"Ankara\", \"United States\": \"Washington\", } city = capitals.get(country, \"Unknown\") return city Given: def grades_to_average_number(grades): numbers = [] for grade in grades: # 1st branch if len(grade) not in {1, 2}: raise ValueError(f\"Invalid grade: {grade}\") if len(grade) == 2 and grade[1] not in {\"+\", \"-\"}: raise ValueError(f\"Invalid grade: {grade}\") letter = grade[0] if letter in {\"F\", \"E\"}: number = 0.0 elif letter == \"D\": number = 1.0 elif letter == \"C\": number = 2.0 elif letter == \"B\": number = 3.0 elif letter == \"A\": number = 4.0 else: raise ValueError(f\"Invalid grade: {grade}\") modifier = 0.0 if letter != \"F\" and grade[-1] == \"+\": modifier = 0.3 elif letter != \"F\" and grade[-1] == \"-\": modifier = -0.3 numbers.append(max(0.0, min(number + modifier, 4.0))) try: return sum(numbers) / len(numbers) except ZeroDivisionError: # 13th branch return 0 Use instead: def grades_to_average_number(grades): grade_values = {\"F\": 0.0, \"E\": 0.0, \"D\": 1.0, \"C\": 2.0, \"B\": 3.0, \"A\": 4.0} modifier_values = {\"+\": 0.3, \"-\": -0.3} numbers = [] for grade in grades: if len(grade) not in {1, 2}: raise ValueError(f\"Invalid grade: {grade}\") letter = grade[0] if letter not in grade_values: raise ValueError(f\"Invalid grade: {grade}\") number = grade_values[letter] if len(grade) == 2 and grade[1] not in modifier_values: raise ValueError(f\"Invalid grade: {grade}\") modifier = modifier_values.get(grade[-1], 0.0) if letter == \"F\": numbers.append(0.0) else: numbers.append(max(0.0, min(number + modifier, 4.0))) try: return sum(numbers) / len(numbers) except ZeroDivisionError: return 0",
    "option": {
      "lint.pylint.max-branches": {
        "option_description": "Maximum number of branches allowed for a function or method body (see PLR0912).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pylint]\nmax-branches = 15\n\n\n\n[lint.pylint]\nmax-branches = 15",
        "option_data_type": "int",
        "default_value": "12",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PLR0913": {
    "description": "[Status: stable] [Name: too-many-arguments] [Message: Too many arguments in function definition ({c_args} > {max_args})] What it does Checks for function definitions that include too many arguments. By default, this rule allows up to five arguments, as configured by the lint.pylint.max-args option. This rule exempts methods decorated with @typing.override. Changing the signature of a subclass method may cause type checkers to complain about a violation of the Liskov Substitution Principle if it means that the method now incompatibly overrides a method defined on a superclass. Explicitly decorating an overriding method with @override signals to Ruff that the method is intended to override a superclass method and that a type checker will enforce that it does so; Ruff therefore knows that it should not enforce rules about methods having too many arguments. Why is this bad? Functions with many arguments are harder to understand, maintain, and call. Consider refactoring functions with many arguments into smaller functions with fewer arguments, or using objects to group related arguments. Example def calculate_position(x_pos, y_pos, z_pos, x_vel, y_vel, z_vel, time): new_x = x_pos + x_vel * time new_y = y_pos + y_vel * time new_z = z_pos + z_vel * time return new_x, new_y, new_z Use instead: from typing import NamedTuple class Vector(NamedTuple): x: float y: float z: float def calculate_position(pos: Vector, vel: Vector, time: float) -> Vector: return Vector(*(p + v * time for p, v in zip(pos, vel)))",
    "option": {
      "lint.pylint.max-args": {
        "option_description": "Maximum number of arguments allowed for a function or method definition\n(see PLR0913).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pylint]\nmax-args = 10\n\n\n\n[lint.pylint]\nmax-args = 10",
        "option_data_type": "int",
        "default_value": "5",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PLR0914": {
    "description": "[Status: preview] [Name: too-many-locals] [Message: Too many local variables ({current_amount}/{max_amount})] What it does Checks for functions that include too many local variables. By default, this rule allows up to fifteen locals, as configured by the lint.pylint.max-locals option. Why is this bad? Functions with many local variables are harder to understand and maintain. Consider refactoring functions with many local variables into smaller functions with fewer assignments.",
    "option": {
      "lint.pylint.max-locals": {
        "option_description": "Maximum number of local variables allowed for a function or method body (see PLR0914).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pylint]\nmax-locals = 20\n\n\n\n[lint.pylint]\nmax-locals = 20",
        "option_data_type": "int",
        "default_value": "15",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PLR0915": {
    "description": "[Status: stable] [Name: too-many-statements] [Message: Too many statements ({statements} > {max_statements})] What it does Checks for functions or methods with too many statements. By default, this rule allows up to 50 statements, as configured by the lint.pylint.max-statements option. Why is this bad? Functions or methods with many statements are harder to understand and maintain. Instead, consider refactoring the function or method into smaller functions or methods, or identifying generalizable patterns and replacing them with generic logic or abstractions. Example def is_even(number: int) -> bool: if number == 0: return True elif number == 1: return False elif number == 2: return True elif number == 3: return False elif number == 4: return True elif number == 5: return False else: ... Use instead: def is_even(number: int) -> bool: return number % 2 == 0",
    "option": {
      "lint.pylint.max-statements": {
        "option_description": "Maximum number of statements allowed for a function or method body (see PLR0915).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pylint]\nmax-statements = 75\n\n\n\n[lint.pylint]\nmax-statements = 75",
        "option_data_type": "int",
        "default_value": "50",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PLR0916": {
    "description": "[Status: preview] [Name: too-many-boolean-expressions] [Message: Too many Boolean expressions ({expressions} > {max_expressions})] What it does Checks for too many Boolean expressions in an if statement. By default, this rule allows up to 5 expressions. This can be configured using the lint.pylint.max-bool-expr option. Why is this bad? if statements with many Boolean expressions are harder to understand and maintain. Consider assigning the result of the Boolean expression, or any of its sub-expressions, to a variable. Example if a and b and c and d and e and f and g and h: ...",
    "option": {
      "lint.pylint.max-bool-expr": {
        "option_description": "Maximum number of Boolean expressions allowed within a single if statement\n(see PLR0916).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pylint]\nmax-bool-expr = 10\n\n\n\n[lint.pylint]\nmax-bool-expr = 10",
        "option_data_type": "int",
        "default_value": "5",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PLR0917": {
    "description": "[Status: preview] [Name: too-many-positional-arguments] [Message: Too many positional arguments ({c_pos}/{max_pos})] What it does Checks for function definitions that include too many positional arguments. By default, this rule allows up to five arguments, as configured by the lint.pylint.max-positional-args option. Why is this bad? Functions with many arguments are harder to understand, maintain, and call. This is especially true for functions with many positional arguments, as providing arguments positionally is more error-prone and less clear to readers than providing arguments by name. Consider refactoring functions with many arguments into smaller functions with fewer arguments, using objects to group related arguments, or migrating to keyword-only arguments. This rule exempts methods decorated with @typing.override. Changing the signature of a subclass method may cause type checkers to complain about a violation of the Liskov Substitution Principle if it means that the method now incompatibly overrides a method defined on a superclass. Explicitly decorating an overriding method with @override signals to Ruff that the method is intended to override a superclass method and that a type checker will enforce that it does so; Ruff therefore knows that it should not enforce rules about methods having too many arguments. Example def plot(x, y, z, color, mark, add_trendline): ... plot(1, 2, 3, \"r\", \"*\", True) Use instead: def plot(x, y, z, *, color, mark, add_trendline): ... plot(1, 2, 3, color=\"r\", mark=\"*\", add_trendline=True)",
    "option": {
      "lint.pylint.max-positional-args": {
        "option_description": "Maximum number of positional arguments allowed for a function or method definition\n(see PLR0917).\n\nIf not specified, defaults to the value of max-args.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pylint]\nmax-positional-args = 3\n\n\n\n[lint.pylint]\nmax-positional-args = 3",
        "option_data_type": "int",
        "default_value": "5",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PLR1702": {
    "description": "[Status: preview] [Name: too-many-nested-blocks] [Message: Too many nested blocks ({nested_blocks} > {max_nested_blocks})] What it does Checks for functions or methods with too many nested blocks. By default, this rule allows up to five nested blocks. This can be configured using the lint.pylint.max-nested-blocks option. Why is this bad? Functions or methods with too many nested blocks are harder to understand and maintain.",
    "option": {
      "lint.pylint.max-nested-blocks": {
        "option_description": "Maximum number of nested blocks allowed within a function or method body\n(see PLR1702).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pylint]\nmax-nested-blocks = 10\n\n\n\n[lint.pylint]\nmax-nested-blocks = 10",
        "option_data_type": "int",
        "default_value": "5",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PLR1704": {
    "description": "[Status: stable] [Name: redefined-argument-from-local] [Message: Redefining argument with the local name <code>{name}</code>] What it does Checks for variables defined in for, try, with statements that redefine function parameters. Why is this bad? Redefined variables can cause unexpected behavior because of overridden function parameters. If nested functions are declared, an inner function's body can override an outer function's parameters. Example def show(host_id=10.11): for host_id, host in [[12.13, \"Venus\"], [14.15, \"Mars\"]]: print(host_id, host) Use instead: def show(host_id=10.11): for inner_host_id, host in [[12.13, \"Venus\"], [14.15, \"Mars\"]]: print(host_id, inner_host_id, host)",
    "option": {
      "lint.dummy-variable-rgx": {
        "option_description": "A regular expression used to identify \"dummy\" variables, or those which\nshould be ignored when enforcing (e.g.) unused-variable rules. The\ndefault expression matches _, __, and _var, but not _var_.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\n# Only ignore variables named \"_\".\ndummy-variable-rgx = \"^_$\"\n\n\n\n[lint]\n# Only ignore variables named \"_\".\ndummy-variable-rgx = \"^_$\"",
        "option_data_type": "str",
        "default_value": "\"^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PLR1708": {
    "description": "[Status: preview] [Name: stop-iteration-return] [Message: Explicit <code>raise StopIteration</code> in generator] What it does Checks for explicit raise StopIteration in generator functions. Why is this bad? Raising StopIteration in a generator function causes a RuntimeError when the generator is iterated over. Instead of raise StopIteration, use return in generator functions. Example def my_generator(): yield 1 yield 2 raise StopIteration # This causes RuntimeError at runtime Use instead: def my_generator(): yield 1 yield 2 return # Use return instead",
    "option": {}
  },
  "PLR1711": {
    "description": "[Status: stable] [Name: useless-return] [Message: Useless <code>return</code> statement at end of function] What it does Checks for functions that end with an unnecessary return or return None, and contain no other return statements. Why is this bad? Python implicitly assumes a None return at the end of a function, making it unnecessary to explicitly write return None. Example def f(): print(5) return None Use instead: def f(): print(5)",
    "option": {}
  },
  "PLR1714": {
    "description": "[Status: stable] [Name: repeated-equality-comparison] [Message: Consider merging multiple comparisons: <code>{expression}</code>. Use a <code>set</code> if the elements are hashable.] What it does Checks for repeated equality comparisons that can be rewritten as a membership test. This rule will try to determine if the values are hashable and the fix will use a set if they are. If unable to determine, the fix will use a tuple and suggest the use of a set. Why is this bad? To check if a variable is equal to one of many values, it is common to write a series of equality comparisons (e.g., foo == \"bar\" or foo == \"baz\"). Instead, prefer to combine the values into a collection and use the in operator to check for membership, which is more performant and succinct. If the items are hashable, use a set for efficiency; otherwise, use a tuple. Fix safety This rule is always unsafe since literal sets and tuples evaluate their members eagerly whereas or comparisons are short-circuited. It is therefore possible that a fix will change behavior in the presence of side-effects. Example foo == \"bar\" or foo == \"baz\" or foo == \"qux\" Use instead: foo in {\"bar\", \"baz\", \"qux\"}",
    "option": {}
  },
  "PLR1716": {
    "description": "[Status: stable] [Name: boolean-chained-comparison] [Message: Contains chained boolean comparison that can be simplified] What it does Check for chained boolean operations that can be simplified. Why is this bad? Refactoring the code will improve readability for these cases. Example a = int(input()) b = int(input()) c = int(input()) if a < b and b < c: pass Use instead: a = int(input()) b = int(input()) c = int(input()) if a < b < c: pass",
    "option": {}
  },
  "PLR1722": {
    "description": "[Status: stable] [Name: sys-exit-alias] [Message: Use <code>sys.exit()</code> instead of <code>{name}</code>] What it does Checks for uses of the exit() and quit(). Why is this bad? exit and quit come from the site module, which is typically imported automatically during startup. However, it is not guaranteed to be imported, and so using these functions may result in a NameError at runtime. Generally, these constants are intended to be used in an interactive interpreter, and not in programs. Prefer sys.exit(), as the sys module is guaranteed to exist in all contexts. Fix safety This fix is always unsafe. When replacing exit or quit with sys.exit, the behavior can change in the following ways: If the code runs in an environment where the site module is not imported (e.g., with python -S), the original code would raise a NameError, while the fixed code would execute normally. site.exit and sys.exit handle tuple arguments differently. site.exit treats tuples as regular objects and always returns exit code 1, while sys.exit interprets tuple contents to determine the exit code: an empty tuple () results in exit code 0, and a single-element tuple like (2,) uses that element's value (2) as the exit code. Example if __name__ == \"__main__\": exit() Use instead: import sys if __name__ == \"__main__\": sys.exit()",
    "option": {}
  },
  "PLR1730": {
    "description": "[Status: stable] [Name: if-stmt-min-max] [Message: Replace <code>if</code> statement with <code>{replacement}</code>] What it does Checks for if statements that can be replaced with min() or max() calls. Why is this bad? An if statement that selects the lesser or greater of two sub-expressions can be replaced with a min() or max() call respectively. Where possible, prefer min() and max(), as they're more concise and readable than the equivalent if statements. Example if score > highest_score: highest_score = score Use instead: highest_score = max(highest_score, score) Fix safety This fix is marked unsafe if it would delete any comments within the replacement range. An example to illustrate where comments are preserved and where they are not: a, b = 0, 10 if a >= b: # deleted comment # deleted comment a = b # preserved comment",
    "option": {}
  },
  "PLR1733": {
    "description": "[Status: stable] [Name: unnecessary-dict-index-lookup] [Message: Unnecessary lookup of dictionary value by key] What it does Checks for key-based dict accesses during .items() iterations. Why is this bad? When iterating over a dict via .items(), the current value is already available alongside its key. Using the key to look up the value is unnecessary. Example FRUITS = {\"apple\": 1, \"orange\": 10, \"berry\": 22} for fruit_name, fruit_count in FRUITS.items(): print(FRUITS[fruit_name]) Use instead: FRUITS = {\"apple\": 1, \"orange\": 10, \"berry\": 22} for fruit_name, fruit_count in FRUITS.items(): print(fruit_count)",
    "option": {}
  },
  "PLR1736": {
    "description": "[Status: stable] [Name: unnecessary-list-index-lookup] [Message: List index lookup in <code>enumerate()</code> loop] What it does Checks for index-based list accesses during enumerate iterations. Why is this bad? When iterating over a list with enumerate, the current item is already available alongside its index. Using the index to look up the item is unnecessary. Example letters = [\"a\", \"b\", \"c\"] for index, letter in enumerate(letters): print(letters[index]) Use instead: letters = [\"a\", \"b\", \"c\"] for index, letter in enumerate(letters): print(letter)",
    "option": {}
  },
  "PLR2004": {
    "description": "[Status: stable] [Name: magic-value-comparison] [Message: Magic value used in comparison, consider replacing <code>{value}</code> with a constant variable] What it does Checks for the use of unnamed numerical constants (\"magic\") values in comparisons. Why is this bad? The use of \"magic\" values can make code harder to read and maintain, as readers will have to infer the meaning of the value from the context. Such values are discouraged by PEP 8. For convenience, this rule excludes a variety of common values from the \"magic\" value definition, such as 0, 1, \"\", and \"__main__\". Example def apply_discount(price: float) -> float: if price <= 100: return price / 2 else: return price Use instead: MAX_DISCOUNT = 100 def apply_discount(price: float) -> float: if price <= MAX_DISCOUNT: return price / 2 else: return price",
    "option": {
      "lint.pylint.allow-magic-value-types": {
        "option_description": "Constant types to ignore when used as \"magic values\" (see PLR2004).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pylint]\nallow-magic-value-types = [\"int\"]\n\n\n\n[lint.pylint]\nallow-magic-value-types = [\"int\"]",
        "option_data_type": "list[\"str\" | \"bytes\" | \"complex\" | \"float\" | \"int\"]",
        "default_value": "[\"str\", \"bytes\"]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PLR2044": {
    "description": "[Status: stable] [Name: empty-comment] [Message: Line with empty comment] What it does Checks for a # symbol appearing on a line not followed by an actual comment. Why is this bad? Empty comments don't provide any clarity to the code, and just add clutter. Either add a comment or delete the empty comment. Example class Foo: # pass Use instead: class Foo: pass",
    "option": {}
  },
  "PLR5501": {
    "description": "[Status: stable] [Name: collapsible-else-if] [Message: Use <code>elif</code> instead of <code>else</code> then <code>if</code>, to reduce indentation] What it does Checks for else blocks that consist of a single if statement. Why is this bad? If an else block contains a single if statement, it can be collapsed into an elif, thus reducing the indentation level. Example def check_sign(value: int) -> None: if value > 0: print(\"Number is positive.\") else: if value < 0: print(\"Number is negative.\") else: print(\"Number is zero.\") Use instead: def check_sign(value: int) -> None: if value > 0: print(\"Number is positive.\") elif value < 0: print(\"Number is negative.\") else: print(\"Number is zero.\")",
    "option": {}
  },
  "PLR6104": {
    "description": "[Status: preview] [Name: non-augmented-assignment] [Message: Use <code>{operator}</code> to perform an augmented assignment directly] What it does Checks for assignments that can be replaced with augmented assignment statements. Why is this bad? If the right-hand side of an assignment statement consists of a binary operation in which one operand is the same as the assignment target, it can be rewritten as an augmented assignment. For example, x = x + 1 can be rewritten as x += 1. When performing such an operation, an augmented assignment is more concise and idiomatic. Known problems In some cases, this rule will not detect assignments in which the target is on the right-hand side of a binary operation (e.g., x = y + x, as opposed to x = x + y), as such operations are not commutative for certain data types, like strings. For example, x = \"prefix-\" + x is not equivalent to x += \"prefix-\", while x = 1 + x is equivalent to x += 1. If the type of the left-hand side cannot be trivially inferred, the rule will ignore the assignment. Example x = x + 1 Use instead: x += 1 Fix safety This rule's fix is marked as unsafe, as augmented assignments have different semantics when the target is a mutable data type, like a list or dictionary. For example, consider the following: foo = [1] bar = foo foo = foo + [2] assert (foo, bar) == ([1, 2], [1]) If the assignment is replaced with an augmented assignment, the update operation will apply to both foo and bar, as they refer to the same object: foo = [1] bar = foo foo += [2] assert (foo, bar) == ([1, 2], [1, 2])",
    "option": {}
  },
  "PLR6201": {
    "description": "[Status: preview] [Name: literal-membership] [Message: Use a set literal when testing for membership] What it does Checks for membership tests on list and tuple literals. Why is this bad? When testing for membership in a static sequence, prefer a set literal over a list or tuple, as Python optimizes set membership tests. Example 1 in [1, 2, 3] Use instead: 1 in {1, 2, 3} Fix safety This rule's fix is marked as unsafe, as the use of a set literal will error at runtime if either the element being tested for membership (the left-hand side) or any element of the sequence (the right-hand side) is unhashable (like lists or dictionaries). While Ruff will attempt to infer the hashability of both sides and skip the fix when it can determine that either side is unhashable, it may not always be able to do so.",
    "option": {}
  },
  "PLR6301": {
    "description": "[Status: preview] [Name: no-self-use] [Message: Method <code>{method_name}</code> could be a function, class method, or static method] What it does Checks for the presence of unused self parameter in methods definitions. Why is this bad? Unused self parameters are usually a sign of a method that could be replaced by a function, class method, or static method. This rule exempts methods decorated with @typing.override. Converting an instance method into a static method or class method may cause type checkers to complain about a violation of the Liskov Substitution Principle if it means that the method now incompatibly overrides a method defined on a superclass. Explicitly decorating an overriding method with @override signals to Ruff that the method is intended to override a superclass method and that a type checker will enforce that it does so; Ruff therefore knows that it should not enforce rules about unused self parameters on such methods. Example class Person: def greeting(self): print(\"Greetings friend!\") Use instead: def greeting(): print(\"Greetings friend!\") or class Person: @staticmethod def greeting(): print(\"Greetings friend!\")",
    "option": {
      "lint.pep8-naming.classmethod-decorators": {
        "option_description": "A list of decorators that, when applied to a method, indicate that the\nmethod should be treated as a class method (in addition to the builtin\n@classmethod).\n\nFor example, Ruff will expect that any method decorated by a decorator\nin this list takes a cls argument as its first argument.\n\nExpects to receive a list of fully-qualified names (e.g., pydantic.validator,\nrather than validator) or alternatively a plain name which is then matched against\nthe last segment in case the decorator itself consists of a dotted name.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nclassmethod-decorators = [\n    # Allow Pydantic's `@validator` decorator to trigger class method treatment.\n    \"pydantic.validator\",\n    # Allow SQLAlchemy's dynamic decorators, like `@field.expression`, to trigger class method treatment.\n    \"declared_attr\",\n    \"expression\",\n    \"comparator\",\n]\n\n\n\n[lint.pep8-naming]\nclassmethod-decorators = [\n    # Allow Pydantic's `@validator` decorator to trigger class method treatment.\n    \"pydantic.validator\",\n    # Allow SQLAlchemy's dynamic decorators, like `@field.expression`, to trigger class method treatment.\n    \"declared_attr\",\n    \"expression\",\n    \"comparator\",\n]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pep8-naming.staticmethod-decorators": {
        "option_description": "A list of decorators that, when applied to a method, indicate that the\nmethod should be treated as a static method (in addition to the builtin\n@staticmethod).\n\nFor example, Ruff will expect that any method decorated by a decorator\nin this list has no self or cls argument.\n\nExpects to receive a list of fully-qualified names (e.g., belay.Device.teardown,\nrather than teardown) or alternatively a plain name which is then matched against\nthe last segment in case the decorator itself consists of a dotted name.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\n# Allow Belay's `@Device.teardown` decorator to trigger static method treatment.\nstaticmethod-decorators = [\"belay.Device.teardown\"]\n\n\n\n[lint.pep8-naming]\n# Allow Belay's `@Device.teardown` decorator to trigger static method treatment.\nstaticmethod-decorators = [\"belay.Device.teardown\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PLW0108": {
    "description": "[Status: preview] [Name: unnecessary-lambda] [Message: Lambda may be unnecessary; consider inlining inner function] What it does Checks for lambda definitions that consist of a single function call with the same arguments as the lambda itself. Why is this bad? When a lambda is used to wrap a function call, and merely propagates the lambda arguments to that function, it can typically be replaced with the function itself, removing a level of indirection. Example df.apply(lambda x: str(x)) Use instead: df.apply(str) Fix safety This rule's fix is marked as unsafe for two primary reasons. First, the lambda body itself could contain an effect. For example, replacing lambda x, y: (func()(x, y)) with func() would lead to a change in behavior, as func() would be evaluated eagerly when defining the lambda, rather than when the lambda is called. However, even when the lambda body itself is pure, the lambda may change the argument names, which can lead to a change in behavior when callers pass arguments by name. For example, replacing foo = lambda x, y: func(x, y) with foo = func, where func is defined as def func(a, b): return a + b, would be a breaking change for callers that execute the lambda by passing arguments by name, as in: foo(x=1, y=2). Since func does not define the arguments x and y, unlike the lambda, the call would raise a TypeError.",
    "option": {}
  },
  "PLW0120": {
    "description": "[Status: stable] [Name: useless-else-on-loop] [Message: <code>else</code> clause on loop without a <code>break</code> statement; remove the <code>else</code> and dedent its contents] What it does Checks for else clauses on loops without a break statement. Why is this bad? When a loop includes an else statement, the code inside the else clause will be executed if the loop terminates \"normally\" (i.e., without a break). If a loop always terminates \"normally\" (i.e., does not contain a break), then the else clause is redundant, as the code inside the else clause will always be executed. In such cases, the code inside the else clause can be moved outside the loop entirely, and the else clause can be removed. Example for item in items: print(item) else: print(\"All items printed\") Use instead: for item in items: print(item) print(\"All items printed\")",
    "option": {}
  },
  "PLW0127": {
    "description": "[Status: stable] [Name: self-assigning-variable] [Message: Self-assignment of variable <code>{name}</code>] What it does Checks for self-assignment of variables. Why is this bad? Self-assignment of variables is redundant and likely a mistake. Example country = \"Poland\" country = country Use instead: country = \"Poland\"",
    "option": {}
  },
  "PLW0128": {
    "description": "[Status: stable] [Name: redeclared-assigned-name] [Message: Redeclared variable <code>{name}</code> in assignment] What it does Checks for declared assignments to the same variable multiple times in the same assignment. Why is this bad? Assigning a variable multiple times in the same assignment is redundant, as the final assignment to the variable is what the value will be. Example a, b, a = (1, 2, 3) print(a) # 3 Use instead: # this is assuming you want to assign 3 to `a` _, b, a = (1, 2, 3) print(a) # 3",
    "option": {
      "lint.dummy-variable-rgx": {
        "option_description": "A regular expression used to identify \"dummy\" variables, or those which\nshould be ignored when enforcing (e.g.) unused-variable rules. The\ndefault expression matches _, __, and _var, but not _var_.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\n# Only ignore variables named \"_\".\ndummy-variable-rgx = \"^_$\"\n\n\n\n[lint]\n# Only ignore variables named \"_\".\ndummy-variable-rgx = \"^_$\"",
        "option_data_type": "str",
        "default_value": "\"^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PLW0129": {
    "description": "[Status: stable] [Name: assert-on-string-literal] [Message: Asserting on an empty string literal will never pass] What it does Checks for assert statements that use a string literal as the first argument. Why is this bad? An assert on a non-empty string literal will always pass, while an assert on an empty string literal will always fail. Example assert \"always true\"",
    "option": {}
  },
  "PLW0131": {
    "description": "[Status: stable] [Name: named-expr-without-context] [Message: Named expression used without context] What it does Checks for uses of named expressions (e.g., a := 42) that can be replaced by regular assignment statements (e.g., a = 42). Why is this bad? While a top-level named expression is syntactically and semantically valid, it's less clear than a regular assignment statement. Named expressions are intended to be used in comprehensions and generator expressions, where assignment statements are not allowed. Example (a := 42) Use instead: a = 42",
    "option": {}
  },
  "PLW0133": {
    "description": "[Status: stable] [Name: useless-exception-statement] [Message: Missing <code>raise</code> statement on exception] What it does Checks for an exception that is not raised. Why is this bad? It's unnecessary to create an exception without raising it. For example, ValueError(\"...\") on its own will have no effect (unlike raise ValueError(\"...\")) and is likely a mistake. Known problems This rule only detects built-in exceptions, like ValueError, and does not catch user-defined exceptions. In preview, this rule will also detect user-defined exceptions, but only the ones defined in the file being checked. Example ValueError(\"...\") Use instead: raise ValueError(\"...\") Fix safety This rule's fix is marked as unsafe, as converting a useless exception statement to a raise statement will change the program's behavior.",
    "option": {}
  },
  "PLW0177": {
    "description": "[Status: stable] [Name: nan-comparison] [Message: Comparing against a NaN value; use <code>math.isnan</code> instead] What it does Checks for comparisons against NaN values. Why is this bad? Comparing against a NaN value can lead to unexpected results. For example, float(\"NaN\") == float(\"NaN\") will return False and, in general, x == float(\"NaN\") will always return False, even if x is NaN. To determine whether a value is NaN, use math.isnan or np.isnan instead of comparing against NaN directly. Example if x == float(\"NaN\"): pass Use instead: import math if math.isnan(x): pass",
    "option": {}
  },
  "PLW0211": {
    "description": "[Status: stable] [Name: bad-staticmethod-argument] [Message: First argument of a static method should not be named <code>{argument_name}</code>] What it does Checks for static methods that use self or cls as their first argument. This rule also applies to __new__ methods, which are implicitly static. Why is this bad? PEP 8 recommends the use of self and cls as the first arguments for instance methods and class methods, respectively. Naming the first argument of a static method as self or cls can be misleading, as static methods do not receive an instance or class reference as their first argument. Example class Wolf: @staticmethod def eat(self): pass Use instead: class Wolf: @staticmethod def eat(sheep): pass",
    "option": {
      "lint.pep8-naming.classmethod-decorators": {
        "option_description": "A list of decorators that, when applied to a method, indicate that the\nmethod should be treated as a class method (in addition to the builtin\n@classmethod).\n\nFor example, Ruff will expect that any method decorated by a decorator\nin this list takes a cls argument as its first argument.\n\nExpects to receive a list of fully-qualified names (e.g., pydantic.validator,\nrather than validator) or alternatively a plain name which is then matched against\nthe last segment in case the decorator itself consists of a dotted name.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nclassmethod-decorators = [\n    # Allow Pydantic's `@validator` decorator to trigger class method treatment.\n    \"pydantic.validator\",\n    # Allow SQLAlchemy's dynamic decorators, like `@field.expression`, to trigger class method treatment.\n    \"declared_attr\",\n    \"expression\",\n    \"comparator\",\n]\n\n\n\n[lint.pep8-naming]\nclassmethod-decorators = [\n    # Allow Pydantic's `@validator` decorator to trigger class method treatment.\n    \"pydantic.validator\",\n    # Allow SQLAlchemy's dynamic decorators, like `@field.expression`, to trigger class method treatment.\n    \"declared_attr\",\n    \"expression\",\n    \"comparator\",\n]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pep8-naming.staticmethod-decorators": {
        "option_description": "A list of decorators that, when applied to a method, indicate that the\nmethod should be treated as a static method (in addition to the builtin\n@staticmethod).\n\nFor example, Ruff will expect that any method decorated by a decorator\nin this list has no self or cls argument.\n\nExpects to receive a list of fully-qualified names (e.g., belay.Device.teardown,\nrather than teardown) or alternatively a plain name which is then matched against\nthe last segment in case the decorator itself consists of a dotted name.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\n# Allow Belay's `@Device.teardown` decorator to trigger static method treatment.\nstaticmethod-decorators = [\"belay.Device.teardown\"]\n\n\n\n[lint.pep8-naming]\n# Allow Belay's `@Device.teardown` decorator to trigger static method treatment.\nstaticmethod-decorators = [\"belay.Device.teardown\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PLW0244": {
    "description": "[Status: preview] [Name: redefined-slots-in-subclass] [Message: Slot <code>{slot_name}</code> redefined from base class <code>{base}</code>] What it does Checks for a re-defined slot in a subclass. Why is this bad? If a class defines a slot also defined in a base class, the instance variable defined by the base class slot is inaccessible (except by retrieving its descriptor directly from the base class). Example class Base: __slots__ = (\"a\", \"b\") class Subclass(Base): __slots__ = (\"a\", \"d\") # slot \"a\" redefined Use instead: class Base: __slots__ = (\"a\", \"b\") class Subclass(Base): __slots__ = \"d\"",
    "option": {}
  },
  "PLW0245": {
    "description": "[Status: stable] [Name: super-without-brackets] [Message: <code>super</code> call is missing parentheses] What it does Detects attempts to use super without parentheses. Why is this bad? The super() callable can be used inside method definitions to create a proxy object that delegates attribute access to a superclass of the current class. Attempting to access attributes on super itself, however, instead of the object returned by a call to super(), will raise AttributeError. Example class Animal: @staticmethod def speak(): return \"This animal says something.\" class Dog(Animal): @staticmethod def speak(): original_speak = super.speak() # ERROR: `super.speak()` return f\"{original_speak} But as a dog, it barks!\" Use instead: class Animal: @staticmethod def speak(): return \"This animal says something.\" class Dog(Animal): @staticmethod def speak(): original_speak = super().speak() # Correct: `super().speak()` return f\"{original_speak} But as a dog, it barks!\"",
    "option": {
      "lint.pep8-naming.classmethod-decorators": {
        "option_description": "A list of decorators that, when applied to a method, indicate that the\nmethod should be treated as a class method (in addition to the builtin\n@classmethod).\n\nFor example, Ruff will expect that any method decorated by a decorator\nin this list takes a cls argument as its first argument.\n\nExpects to receive a list of fully-qualified names (e.g., pydantic.validator,\nrather than validator) or alternatively a plain name which is then matched against\nthe last segment in case the decorator itself consists of a dotted name.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nclassmethod-decorators = [\n    # Allow Pydantic's `@validator` decorator to trigger class method treatment.\n    \"pydantic.validator\",\n    # Allow SQLAlchemy's dynamic decorators, like `@field.expression`, to trigger class method treatment.\n    \"declared_attr\",\n    \"expression\",\n    \"comparator\",\n]\n\n\n\n[lint.pep8-naming]\nclassmethod-decorators = [\n    # Allow Pydantic's `@validator` decorator to trigger class method treatment.\n    \"pydantic.validator\",\n    # Allow SQLAlchemy's dynamic decorators, like `@field.expression`, to trigger class method treatment.\n    \"declared_attr\",\n    \"expression\",\n    \"comparator\",\n]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pep8-naming.staticmethod-decorators": {
        "option_description": "A list of decorators that, when applied to a method, indicate that the\nmethod should be treated as a static method (in addition to the builtin\n@staticmethod).\n\nFor example, Ruff will expect that any method decorated by a decorator\nin this list has no self or cls argument.\n\nExpects to receive a list of fully-qualified names (e.g., belay.Device.teardown,\nrather than teardown) or alternatively a plain name which is then matched against\nthe last segment in case the decorator itself consists of a dotted name.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\n# Allow Belay's `@Device.teardown` decorator to trigger static method treatment.\nstaticmethod-decorators = [\"belay.Device.teardown\"]\n\n\n\n[lint.pep8-naming]\n# Allow Belay's `@Device.teardown` decorator to trigger static method treatment.\nstaticmethod-decorators = [\"belay.Device.teardown\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PLW0406": {
    "description": "[Status: stable] [Name: import-self] [Message: Module <code>{name}</code> imports itself] What it does Checks for import statements that import the current module. Why is this bad? Importing a module from itself is a circular dependency and results in an ImportError exception. Example # file: this_file.py from this_file import foo def foo(): ...",
    "option": {}
  },
  "PLW0602": {
    "description": "[Status: stable] [Name: global-variable-not-assigned] [Message: Using global for <code>{name}</code> but no assignment is done] What it does Checks for global variables that are not assigned a value in the current scope. Why is this bad? The global keyword allows an inner scope to modify a variable declared in the outer scope. If the variable is not modified within the inner scope, there is no need to use global. Example DEBUG = True def foo(): global DEBUG if DEBUG: print(\"foo() called\") ... Use instead: DEBUG = True def foo(): if DEBUG: print(\"foo() called\") ...",
    "option": {}
  },
  "PLW0603": {
    "description": "[Status: stable] [Name: global-statement] [Message: Using the global statement to update <code>{name}</code> is discouraged] What it does Checks for the use of global statements to update identifiers. Why is this bad? Pylint discourages the use of global variables as global mutable state is a common source of bugs and confusing behavior. Example var = 1 def foo(): global var # [global-statement] var = 10 print(var) foo() print(var) Use instead: var = 1 def foo(): var = 10 print(var) return var var = foo() print(var)",
    "option": {}
  },
  "PLW0604": {
    "description": "[Status: stable] [Name: global-at-module-level] [Message: <code>global</code> at module level is redundant] What it does Checks for uses of the global keyword at the module level. Why is this bad? The global keyword is used within functions to indicate that a name refers to a global variable, rather than a local variable. At the module level, all names are global by default, so the global keyword is redundant.",
    "option": {}
  },
  "PLW0642": {
    "description": "[Status: stable] [Name: self-or-cls-assignment] [Message: Reassigned <code>{}</code> variable in {method_type} method] What it does Checks for assignment of self and cls in instance and class methods respectively. This check also applies to __new__ even though this is technically a static method. Why is this bad? The identifiers self and cls are conventional in Python for the first parameter of instance methods and class methods, respectively. Assigning new values to these variables can be confusing for others reading your code; using a different variable name can lead to clearer code. Example class Version: def add(self, other): self = self + other return self @classmethod def superclass(cls): cls = cls.__mro__[-1] return cls Use instead: class Version: def add(self, other): new_version = self + other return new_version @classmethod def superclass(cls): supercls = cls.__mro__[-1] return supercls",
    "option": {
      "lint.pep8-naming.classmethod-decorators": {
        "option_description": "A list of decorators that, when applied to a method, indicate that the\nmethod should be treated as a class method (in addition to the builtin\n@classmethod).\n\nFor example, Ruff will expect that any method decorated by a decorator\nin this list takes a cls argument as its first argument.\n\nExpects to receive a list of fully-qualified names (e.g., pydantic.validator,\nrather than validator) or alternatively a plain name which is then matched against\nthe last segment in case the decorator itself consists of a dotted name.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\nclassmethod-decorators = [\n    # Allow Pydantic's `@validator` decorator to trigger class method treatment.\n    \"pydantic.validator\",\n    # Allow SQLAlchemy's dynamic decorators, like `@field.expression`, to trigger class method treatment.\n    \"declared_attr\",\n    \"expression\",\n    \"comparator\",\n]\n\n\n\n[lint.pep8-naming]\nclassmethod-decorators = [\n    # Allow Pydantic's `@validator` decorator to trigger class method treatment.\n    \"pydantic.validator\",\n    # Allow SQLAlchemy's dynamic decorators, like `@field.expression`, to trigger class method treatment.\n    \"declared_attr\",\n    \"expression\",\n    \"comparator\",\n]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pep8-naming.staticmethod-decorators": {
        "option_description": "A list of decorators that, when applied to a method, indicate that the\nmethod should be treated as a static method (in addition to the builtin\n@staticmethod).\n\nFor example, Ruff will expect that any method decorated by a decorator\nin this list has no self or cls argument.\n\nExpects to receive a list of fully-qualified names (e.g., belay.Device.teardown,\nrather than teardown) or alternatively a plain name which is then matched against\nthe last segment in case the decorator itself consists of a dotted name.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pep8-naming]\n# Allow Belay's `@Device.teardown` decorator to trigger static method treatment.\nstaticmethod-decorators = [\"belay.Device.teardown\"]\n\n\n\n[lint.pep8-naming]\n# Allow Belay's `@Device.teardown` decorator to trigger static method treatment.\nstaticmethod-decorators = [\"belay.Device.teardown\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PLW0711": {
    "description": "[Status: stable] [Name: binary-op-exception] [Message: Exception to catch is the result of a binary <code>and</code> operation] What it does Checks for except clauses that attempt to catch multiple exceptions with a binary operation (and or or). Why is this bad? A binary operation will not catch multiple exceptions. Instead, the binary operation will be evaluated first, and the result of that operation will be caught (for an or operation, this is typically the first exception in the list). This is almost never the desired behavior. Example try: pass except A or B: pass Use instead: try: pass except (A, B): pass",
    "option": {}
  },
  "PLW1501": {
    "description": "[Status: stable] [Name: bad-open-mode] [Message: <code>{mode}</code> is not a valid mode for <code>open</code>] What it does Check for an invalid mode argument in open calls. Why is this bad? The open function accepts a mode argument that specifies how the file should be opened (e.g., read-only, write-only, append-only, etc.). Python supports a variety of open modes: r, w, a, and x, to control reading, writing, appending, and creating, respectively, along with b (binary mode), + (read and write), and U (universal newlines), the latter of which is only valid alongside r. This rule detects both invalid combinations of modes and invalid characters in the mode string itself. Example with open(\"file\", \"rwx\") as f: content = f.read() Use instead: with open(\"file\", \"r\") as f: content = f.read()",
    "option": {}
  },
  "PLW1507": {
    "description": "[Status: stable] [Name: shallow-copy-environ] [Message: Shallow copy of <code>os.environ</code> via <code>copy.copy(os.environ)</code>] What it does Check for shallow os.environ copies. Why is this bad? os.environ is not a dict object, but rather, a proxy object. As such, mutating a shallow copy of os.environ will also mutate the original object. See BPO 15373 for more information. Example import copy import os env = copy.copy(os.environ) Use instead: import os env = os.environ.copy() Fix safety This rule's fix is marked as unsafe because replacing a shallow copy with a deep copy can lead to unintended side effects. If the program modifies the shallow copy at some point, changing it to a deep copy may prevent those modifications from affecting the original data, potentially altering the program's behavior.",
    "option": {}
  },
  "PLW1508": {
    "description": "[Status: stable] [Name: invalid-envvar-default] [Message: Invalid type for environment variable default; expected <code>str</code> or <code>None</code>] What it does Checks for os.getenv calls with invalid default values. Why is this bad? If an environment variable is set, os.getenv will return its value as a string. If the environment variable is not set, os.getenv will return None, or the default value if one is provided. If the default value is not a string or None, then it will be inconsistent with the return type of os.getenv, which can lead to confusing behavior. Example import os int(os.getenv(\"FOO\", 1)) Use instead: import os int(os.getenv(\"FOO\", \"1\"))",
    "option": {}
  },
  "PLW1509": {
    "description": "[Status: stable] [Name: subprocess-popen-preexec-fn] [Message: <code>preexec_fn</code> argument is unsafe when using threads] What it does Checks for uses of subprocess.Popen with a preexec_fn argument. Why is this bad? The preexec_fn argument is unsafe within threads as it can lead to deadlocks. Furthermore, preexec_fn is targeted for deprecation. Instead, consider using task-specific arguments such as env, start_new_session, and process_group. These are not prone to deadlocks and are more explicit. Example import os, subprocess subprocess.Popen(foo, preexec_fn=os.setsid) subprocess.Popen(bar, preexec_fn=os.setpgid(0, 0)) Use instead: import subprocess subprocess.Popen(foo, start_new_session=True) subprocess.Popen(bar, process_group=0) # Introduced in Python 3.11",
    "option": {}
  },
  "PLW1510": {
    "description": "[Status: stable] [Name: subprocess-run-without-check] [Message: <code>subprocess.run</code> without explicit <code>check</code> argument] What it does Checks for uses of subprocess.run without an explicit check argument. Why is this bad? By default, subprocess.run does not check the return code of the process it runs. This can lead to silent failures. Instead, consider using check=True to raise an exception if the process fails, or set check=False explicitly to mark the behavior as intentional. Example import subprocess subprocess.run([\"ls\", \"nonexistent\"]) # No exception raised. Use instead: import subprocess subprocess.run([\"ls\", \"nonexistent\"], check=True) # Raises exception. Or: import subprocess subprocess.run([\"ls\", \"nonexistent\"], check=False) # Explicitly no check. Fix safety This rule's fix is marked as display-only because it's not clear whether the potential exception was meant to be ignored by setting check=False or if the author simply forgot to include check=True. The fix adds check=False, making the existing behavior explicit but possibly masking the original intention.",
    "option": {}
  },
  "PLW1514": {
    "description": "[Status: preview] [Name: unspecified-encoding] [Message: <code>{function_name}</code> in text mode without explicit <code>encoding</code> argument] What it does Checks for uses of open and related calls without an explicit encoding argument. Why is this bad? Using open in text mode without an explicit encoding can lead to non-portable code, with differing behavior across platforms. While readers may assume that UTF-8 is the default encoding, in reality, the default is locale-specific. Instead, consider using the encoding parameter to enforce a specific encoding. PEP 597 recommends the use of encoding=\"utf-8\" as a default, and suggests that it may become the default in future versions of Python. If a locale-specific encoding is intended, use encoding=\"locale\" on Python 3.10 and later, or locale.getpreferredencoding() on earlier versions, to make the encoding explicit. Fix safety This fix is always unsafe and may change the program's behavior. It forces encoding=\"utf-8\" as the default, regardless of the platforms actual default encoding, which may cause UnicodeDecodeError on non-UTF-8 systems. with open(\"test.txt\") as f: print(f.read()) # before fix (on UTF-8 systems):  with open(\"test.txt\", encoding=\"utf-8\") as f: print(f.read()) # after fix (on Windows): UnicodeDecodeError Example open(\"file.txt\") Use instead: open(\"file.txt\", encoding=\"utf-8\")",
    "option": {}
  },
  "PLW1641": {
    "description": "[Status: stable] [Name: eq-without-hash] [Message: Object does not implement <code>__hash__</code> method] What it does Checks for classes that implement __eq__ but not __hash__. Why is this bad? A class that implements __eq__ but not __hash__ will have its hash method implicitly set to None, regardless of if a superclass defines __hash__. This will cause the class to be unhashable, which will in turn cause issues when using instances of the class as keys in a dictionary or members of a set. Example class Person: def __init__(self): self.name = \"monty\" def __eq__(self, other): return isinstance(other, Person) and other.name == self.name Use instead: class Person: def __init__(self): self.name = \"monty\" def __eq__(self, other): return isinstance(other, Person) and other.name == self.name def __hash__(self): return hash(self.name) In general, it is unsound to inherit a __hash__ implementation from a parent class while overriding the __eq__ implementation because the two must be kept in sync. However, an easy way to resolve this error in cases where it is sound is to explicitly set __hash__ to the parent class's implementation: class Developer(Person): def __init__(self): ... def __eq__(self, other): ... __hash__ = Person.__hash__",
    "option": {}
  },
  "PLW2101": {
    "description": "[Status: stable] [Name: useless-with-lock] [Message: Threading lock directly created in <code>with</code> statement has no effect] What it does Checks for lock objects that are created and immediately discarded in with statements. Why is this bad? Creating a lock (via threading.Lock or similar) in a with statement has no effect, as locks are only relevant when shared between threads. Instead, assign the lock to a variable outside the with statement, and share that variable between threads. Example import threading counter = 0 def increment(): global counter with threading.Lock(): counter += 1 Use instead: import threading counter = 0 lock = threading.Lock() def increment(): global counter with lock: counter += 1",
    "option": {}
  },
  "PLW2901": {
    "description": "[Status: stable] [Name: redefined-loop-name] [Message: Outer {outer_kind} variable <code>{name}</code> overwritten by inner {inner_kind} target] What it does Checks for variables defined in for loops and with statements that get overwritten within the body, for example by another for loop or with statement or by direct assignment. Why is this bad? Redefinition of a loop variable inside the loop's body causes its value to differ from the original loop iteration for the remainder of the block, in a way that will likely cause bugs. In Python, unlike many other languages, for loops and with statements don't define their own scopes. Therefore, a nested loop that uses the same target variable name as an outer loop will reuse the same actual variable, and the value from the last iteration will \"leak out\" into the remainder of the enclosing loop. While this mistake is easy to spot in small examples, it can be hidden in larger blocks of code, where the definition and redefinition of the variable may not be visible at the same time. Example for i in range(10): i = 9 print(i) # prints 9 every iteration for i in range(10): for i in range(10): # original value overwritten pass print(i) # also prints 9 every iteration with path1.open() as f: with path2.open() as f: f = path2.open() print(f.readline()) # prints a line from path2",
    "option": {
      "lint.dummy-variable-rgx": {
        "option_description": "A regular expression used to identify \"dummy\" variables, or those which\nshould be ignored when enforcing (e.g.) unused-variable rules. The\ndefault expression matches _, __, and _var, but not _var_.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\n# Only ignore variables named \"_\".\ndummy-variable-rgx = \"^_$\"\n\n\n\n[lint]\n# Only ignore variables named \"_\".\ndummy-variable-rgx = \"^_$\"",
        "option_data_type": "str",
        "default_value": "\"^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PLW3201": {
    "description": "[Status: preview] [Name: bad-dunder-method-name] [Message: Dunder method <code>{name}</code> has no special meaning in Python 3] What it does Checks for dunder methods that have no special meaning in Python 3. Why is this bad? Misspelled or no longer supported dunder name methods may cause your code to not function as expected. Since dunder methods are associated with customizing the behavior of a class in Python, introducing a dunder method such as __foo__ that diverges from standard Python dunder methods could potentially confuse someone reading the code. This rule will detect all methods starting and ending with at least one underscore (e.g., _str_), but ignores known dunder methods (like __init__), as well as methods that are marked with @override. Additional dunder methods names can be allowed via the lint.pylint.allow-dunder-method-names setting. Example class Foo: def __init_(self): ... Use instead: class Foo: def __init__(self): ...",
    "option": {
      "lint.pylint.allow-dunder-method-names": {
        "option_description": "Dunder methods name to allow, in addition to the default set from the\nPython standard library (see PLW3201).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pylint]\nallow-dunder-method-names = [\"__tablename__\", \"__table_args__\"]\n\n\n\n[lint.pylint]\nallow-dunder-method-names = [\"__tablename__\", \"__table_args__\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "PLW3301": {
    "description": "[Status: stable] [Name: nested-min-max] [Message: Nested <code>{func}</code> calls can be flattened] What it does Checks for nested min and max calls. Why is this bad? Nested min and max calls can be flattened into a single call to improve readability. Example minimum = min(1, 2, min(3, 4, 5)) maximum = max(1, 2, max(3, 4, 5)) diff = maximum - minimum Use instead: minimum = min(1, 2, 3, 4, 5) maximum = max(1, 2, 3, 4, 5) diff = maximum - minimum Known issues The resulting code may be slower and use more memory, especially for nested iterables. For example, this code: iterable = range(3) min(1, min(iterable)) will be fixed to: iterable = range(3) min(1, *iterable) At least on current versions of CPython, this allocates a collection for the whole iterable before calling min and could cause performance regressions, at least for large iterables. Fix safety This fix is always unsafe and may change the program's behavior for types without full equivalence relations, such as float comparisons involving NaN. print(min(2.0, min(float(\"nan\"), 1.0))) # before fix: 2.0 print(min(2.0, float(\"nan\"), 1.0)) # after fix: 1.0 print(max(1.0, max(float(\"nan\"), 2.0))) # before fix: 1.0 print(max(1.0, float(\"nan\"), 2.0)) # after fix: 2.0 The fix will also remove any comments within the outer call.",
    "option": {}
  },
  "UP001": {
    "description": "[Status: stable] [Name: useless-metaclass-type] [Message: <code>__metaclass__ = type</code> is implied] What it does Checks for the use of __metaclass__ = type in class definitions. Why is this bad? Since Python 3, __metaclass__ = type is implied and can thus be omitted. Example class Foo: __metaclass__ = type Use instead: class Foo: ...",
    "option": {}
  },
  "UP003": {
    "description": "[Status: stable] [Name: type-of-primitive] [Message: Use <code>{}</code> instead of <code>type(...)</code>] What it does Checks for uses of type that take a primitive as an argument. Why is this bad? type() returns the type of a given object. A type of a primitive can always be known in advance and accessed directly, which is more concise and explicit than using type(). Example type(1) Use instead: int",
    "option": {}
  },
  "UP004": {
    "description": "[Status: stable] [Name: useless-object-inheritance] [Message: Class <code>{name}</code> inherits from <code>object</code>] What it does Checks for classes that inherit from object. Why is this bad? Since Python 3, all classes inherit from object by default, so object can be omitted from the list of base classes. Example class Foo(object): ... Use instead: class Foo: ... Fix safety This fix is unsafe if it would cause comments to be deleted.",
    "option": {}
  },
  "UP005": {
    "description": "[Status: stable] [Name: deprecated-unittest-alias] [Message: <code>{alias}</code> is deprecated, use <code>{target}</code>] What it does Checks for uses of deprecated methods from the unittest module. Why is this bad? The unittest module has deprecated aliases for some of its methods. The deprecated aliases were removed in Python 3.12. Instead of aliases, use their non-deprecated counterparts. Example from unittest import TestCase class SomeTest(TestCase): def test_something(self): self.assertEquals(1, 1) Use instead: from unittest import TestCase class SomeTest(TestCase): def test_something(self): self.assertEqual(1, 1)",
    "option": {}
  },
  "UP006": {
    "description": "[Status: stable] [Name: non-pep585-annotation] [Message: Use <code>{to}</code> instead of <code>{from}</code> for type annotation] What it does Checks for the use of generics that can be replaced with standard library variants based on PEP 585. Why is this bad? PEP 585 enabled collections in the Python standard library (like list) to be used as generics directly, instead of importing analogous members from the typing module (like typing.List). When available, the PEP 585 syntax should be used instead of importing members from the typing module, as it's more concise and readable. Importing those members from typing is considered deprecated as of PEP 585. This rule is enabled when targeting Python 3.9 or later (see: target-version). By default, it's also enabled for earlier Python versions if from __future__ import annotations is present, as __future__ annotations are not evaluated at runtime. If your code relies on runtime type annotations (either directly or via a library like Pydantic), you can disable this behavior for Python versions prior to 3.9 by setting lint.pyupgrade.keep-runtime-typing to true. Example from typing import List foo: List[int] = [1, 2, 3] Use instead: foo: list[int] = [1, 2, 3] Fix safety This rule's fix is marked as unsafe, as it may lead to runtime errors when alongside libraries that rely on runtime type annotations, like Pydantic, on Python versions prior to Python 3.9.",
    "option": {
      "target-version": {
        "option_description": "The minimum Python version to target, e.g., when considering automatic\ncode upgrades, like rewriting type annotations. Ruff will not propose\nchanges using features that are not available in the given version.\n\nFor example, to represent supporting Python >=3.11 or ==3.11\nspecify target-version = \"py311\".\n\nIf you're already using a pyproject.toml file, we recommend\nproject.requires-python instead, as it's based on Python packaging\nstandards, and will be respected by other tools. For example, Ruff\ntreats the following as identical to target-version = \"py38\":\n\n[project]\nrequires-python = \">=3.8\"\n\nIf both are specified, target-version takes precedence over\nrequires-python. See Inferring the Python version\nfor a complete description of how the target-version is determined\nwhen left unspecified.\n\nNote that a stub file can sometimes make use of a typing feature\nbefore it is available at runtime, as long as the stub does not make\nuse of new syntax. For example, a type checker will understand\nint | str in a stub as being a Union type annotation, even if the\ntype checker is run using Python 3.9, despite the fact that the |\noperator can only be used to create union types at runtime on Python\n3.10+. As such, Ruff will often recommend newer features in a stub\nfile than it would for an equivalent runtime file with the same target\nversion.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"\n\n\n\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"",
        "option_data_type": "\"py37\" | \"py38\" | \"py39\" | \"py310\" | \"py311\" | \"py312\" | \"py313\" | \"py314\"",
        "default_value": "\"py310\"",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pyupgrade.keep-runtime-typing": {
        "option_description": "Whether to avoid PEP 585 (List[int] -> list[int]) and PEP 604\n(Union[str, int] -> str | int) rewrites even if a file imports\nfrom __future__ import annotations.\n\nThis setting is only applicable when the target Python version is below\n3.9 and 3.10 respectively, and is most commonly used when working with\nlibraries like Pydantic and FastAPI, which rely on the ability to parse\ntype annotations at runtime. The use of from __future__ import annotations\ncauses Python to treat the type annotations as strings, which typically\nallows for the use of language features that appear in later Python\nversions but are not yet supported by the current version (e.g., str |\nint). However, libraries that rely on runtime type annotations will\nbreak if the annotations are incompatible with the current Python\nversion.\n\nFor example, while the following is valid Python 3.8 code due to the\npresence of from __future__ import annotations, the use of str | int\nprior to Python 3.10 will cause Pydantic to raise a TypeError at\nruntime:\n\nfrom __future__ import annotations\n\nimport pydantic\n\nclass Foo(pydantic.BaseModel):\n    bar: str | int\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pyupgrade]\n# Preserve types, even if a file imports `from __future__ import annotations`.\nkeep-runtime-typing = true\n\n\n\n[lint.pyupgrade]\n# Preserve types, even if a file imports `from __future__ import annotations`.\nkeep-runtime-typing = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "UP007": {
    "description": "[Status: stable] [Name: non-pep604-annotation-union] [Message: Use <code>X | Y</code> for type annotations] What it does Check for type annotations that can be rewritten based on PEP 604 syntax. Why is this bad? PEP 604 introduced a new syntax for union type annotations based on the | operator. This syntax is more concise and readable than the previous typing.Union and typing.Optional syntaxes. This rule is enabled when targeting Python 3.10 or later (see: target-version). By default, it's also enabled for earlier Python versions if from __future__ import annotations is present, as __future__ annotations are not evaluated at runtime. If your code relies on runtime type annotations (either directly or via a library like Pydantic), you can disable this behavior for Python versions prior to 3.10 by setting lint.pyupgrade.keep-runtime-typing to true. Example from typing import Union foo: Union[int, str] = 1 Use instead: foo: int | str = 1 Note that this rule only checks for usages of typing.Union, while UP045 checks for typing.Optional. Fix safety This rule's fix is marked as unsafe, as it may lead to runtime errors when alongside libraries that rely on runtime type annotations, like Pydantic, on Python versions prior to Python 3.10, or as it may remove comments if they are present within the type annotation being rewritten. It may also lead to runtime errors in unusual and likely incorrect type annotations where the type does not support the | operator.",
    "option": {
      "target-version": {
        "option_description": "The minimum Python version to target, e.g., when considering automatic\ncode upgrades, like rewriting type annotations. Ruff will not propose\nchanges using features that are not available in the given version.\n\nFor example, to represent supporting Python >=3.11 or ==3.11\nspecify target-version = \"py311\".\n\nIf you're already using a pyproject.toml file, we recommend\nproject.requires-python instead, as it's based on Python packaging\nstandards, and will be respected by other tools. For example, Ruff\ntreats the following as identical to target-version = \"py38\":\n\n[project]\nrequires-python = \">=3.8\"\n\nIf both are specified, target-version takes precedence over\nrequires-python. See Inferring the Python version\nfor a complete description of how the target-version is determined\nwhen left unspecified.\n\nNote that a stub file can sometimes make use of a typing feature\nbefore it is available at runtime, as long as the stub does not make\nuse of new syntax. For example, a type checker will understand\nint | str in a stub as being a Union type annotation, even if the\ntype checker is run using Python 3.9, despite the fact that the |\noperator can only be used to create union types at runtime on Python\n3.10+. As such, Ruff will often recommend newer features in a stub\nfile than it would for an equivalent runtime file with the same target\nversion.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"\n\n\n\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"",
        "option_data_type": "\"py37\" | \"py38\" | \"py39\" | \"py310\" | \"py311\" | \"py312\" | \"py313\" | \"py314\"",
        "default_value": "\"py310\"",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pyupgrade.keep-runtime-typing": {
        "option_description": "Whether to avoid PEP 585 (List[int] -> list[int]) and PEP 604\n(Union[str, int] -> str | int) rewrites even if a file imports\nfrom __future__ import annotations.\n\nThis setting is only applicable when the target Python version is below\n3.9 and 3.10 respectively, and is most commonly used when working with\nlibraries like Pydantic and FastAPI, which rely on the ability to parse\ntype annotations at runtime. The use of from __future__ import annotations\ncauses Python to treat the type annotations as strings, which typically\nallows for the use of language features that appear in later Python\nversions but are not yet supported by the current version (e.g., str |\nint). However, libraries that rely on runtime type annotations will\nbreak if the annotations are incompatible with the current Python\nversion.\n\nFor example, while the following is valid Python 3.8 code due to the\npresence of from __future__ import annotations, the use of str | int\nprior to Python 3.10 will cause Pydantic to raise a TypeError at\nruntime:\n\nfrom __future__ import annotations\n\nimport pydantic\n\nclass Foo(pydantic.BaseModel):\n    bar: str | int\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pyupgrade]\n# Preserve types, even if a file imports `from __future__ import annotations`.\nkeep-runtime-typing = true\n\n\n\n[lint.pyupgrade]\n# Preserve types, even if a file imports `from __future__ import annotations`.\nkeep-runtime-typing = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "UP008": {
    "description": "[Status: stable] [Name: super-call-with-parameters] [Message: Use <code>super()</code> instead of <code>super(__class__, self)</code>] What it does Checks for super calls that pass redundant arguments. Why is this bad? In Python 3, super can be invoked without any arguments when: (1) the first argument is __class__, and (2) the second argument is equivalent to the first argument of the enclosing method. When possible, omit the arguments to super to make the code more concise and maintainable. Example class A: def foo(self): pass class B(A): def bar(self): super(B, self).foo() Use instead: class A: def foo(self): pass class B(A): def bar(self): super().foo() Fix safety This rule's fix is marked as unsafe because removing the arguments from a call may delete comments that are attached to the arguments. In preview, the fix is marked safe if no comments are present.",
    "option": {}
  },
  "UP009": {
    "description": "[Status: stable] [Name: utf8-encoding-declaration] [Message: UTF-8 encoding declaration is unnecessary] What it does Checks for unnecessary UTF-8 encoding declarations. Why is this bad? PEP 3120 makes UTF-8 the default encoding, so a UTF-8 encoding declaration is unnecessary. Example # -*- coding: utf-8 -*- print(\"Hello, world!\") Use instead: print(\"Hello, world!\")",
    "option": {}
  },
  "UP010": {
    "description": "[Status: stable] [Name: unnecessary-future-import] [Message: Unnecessary <code>__future__</code> import <code>{import}</code> for target Python version] What it does Checks for unnecessary __future__ imports. Why is this bad? The __future__ module is used to enable features that are not yet available in the current Python version. If a feature is already available in the minimum supported Python version, importing it from __future__ is unnecessary and should be removed to avoid confusion. Example from __future__ import print_function print(\"Hello, world!\") Use instead: print(\"Hello, world!\") Fix safety This fix is marked unsafe if applying it would delete a comment.",
    "option": {
      "target-version": {
        "option_description": "The minimum Python version to target, e.g., when considering automatic\ncode upgrades, like rewriting type annotations. Ruff will not propose\nchanges using features that are not available in the given version.\n\nFor example, to represent supporting Python >=3.11 or ==3.11\nspecify target-version = \"py311\".\n\nIf you're already using a pyproject.toml file, we recommend\nproject.requires-python instead, as it's based on Python packaging\nstandards, and will be respected by other tools. For example, Ruff\ntreats the following as identical to target-version = \"py38\":\n\n[project]\nrequires-python = \">=3.8\"\n\nIf both are specified, target-version takes precedence over\nrequires-python. See Inferring the Python version\nfor a complete description of how the target-version is determined\nwhen left unspecified.\n\nNote that a stub file can sometimes make use of a typing feature\nbefore it is available at runtime, as long as the stub does not make\nuse of new syntax. For example, a type checker will understand\nint | str in a stub as being a Union type annotation, even if the\ntype checker is run using Python 3.9, despite the fact that the |\noperator can only be used to create union types at runtime on Python\n3.10+. As such, Ruff will often recommend newer features in a stub\nfile than it would for an equivalent runtime file with the same target\nversion.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"\n\n\n\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"",
        "option_data_type": "\"py37\" | \"py38\" | \"py39\" | \"py310\" | \"py311\" | \"py312\" | \"py313\" | \"py314\"",
        "default_value": "\"py310\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "UP011": {
    "description": "[Status: stable] [Name: lru-cache-without-parameters] [Message: Unnecessary parentheses to <code>functools.lru_cache</code>] What it does Checks for unnecessary parentheses on functools.lru_cache decorators. Why is this bad? Since Python 3.8, functools.lru_cache can be used as a decorator without trailing parentheses, as long as no arguments are passed to it. Example import functools @functools.lru_cache() def foo(): ... Use instead: import functools @functools.lru_cache def foo(): ...",
    "option": {
      "target-version": {
        "option_description": "The minimum Python version to target, e.g., when considering automatic\ncode upgrades, like rewriting type annotations. Ruff will not propose\nchanges using features that are not available in the given version.\n\nFor example, to represent supporting Python >=3.11 or ==3.11\nspecify target-version = \"py311\".\n\nIf you're already using a pyproject.toml file, we recommend\nproject.requires-python instead, as it's based on Python packaging\nstandards, and will be respected by other tools. For example, Ruff\ntreats the following as identical to target-version = \"py38\":\n\n[project]\nrequires-python = \">=3.8\"\n\nIf both are specified, target-version takes precedence over\nrequires-python. See Inferring the Python version\nfor a complete description of how the target-version is determined\nwhen left unspecified.\n\nNote that a stub file can sometimes make use of a typing feature\nbefore it is available at runtime, as long as the stub does not make\nuse of new syntax. For example, a type checker will understand\nint | str in a stub as being a Union type annotation, even if the\ntype checker is run using Python 3.9, despite the fact that the |\noperator can only be used to create union types at runtime on Python\n3.10+. As such, Ruff will often recommend newer features in a stub\nfile than it would for an equivalent runtime file with the same target\nversion.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"\n\n\n\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"",
        "option_data_type": "\"py37\" | \"py38\" | \"py39\" | \"py310\" | \"py311\" | \"py312\" | \"py313\" | \"py314\"",
        "default_value": "\"py310\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "UP012": {
    "description": "[Status: stable] [Name: unnecessary-encode-utf8] [Message: Unnecessary call to <code>encode</code> as UTF-8] What it does Checks for unnecessary calls to encode as UTF-8. Why is this bad? UTF-8 is the default encoding in Python, so there is no need to call encode when UTF-8 is the desired encoding. Instead, use a bytes literal. Example \"foo\".encode(\"utf-8\") Use instead: b\"foo\"",
    "option": {}
  },
  "UP013": {
    "description": "[Status: stable] [Name: convert-typed-dict-functional-to-class] [Message: Convert <code>{name}</code> from <code>TypedDict</code> functional to class syntax] What it does Checks for TypedDict declarations that use functional syntax. Why is this bad? TypedDict types can be defined either through a functional syntax (Foo = TypedDict(...)) or a class syntax (class Foo(TypedDict): ...). The class syntax is more readable and generally preferred over the functional syntax. Nonetheless, there are some situations in which it is impossible to use the class-based syntax. This rule will not apply to those cases. Namely, it is impossible to use the class-based syntax if any TypedDict fields are: Not valid python identifiers (for example, @x) Python keywords such as in Private names such as __id that would undergo name mangling at runtime if the class-based syntax was used Dunder names such as __int__ that can confuse type checkers if they're used with the class-based syntax. Example from typing import TypedDict Foo = TypedDict(\"Foo\", {\"a\": int, \"b\": str}) Use instead: from typing import TypedDict class Foo(TypedDict): a: int b: str Fix safety This rule's fix is marked as unsafe if there are any comments within the range of the TypedDict definition, as these will be dropped by the autofix.",
    "option": {}
  },
  "UP014": {
    "description": "[Status: stable] [Name: convert-named-tuple-functional-to-class] [Message: Convert <code>{name}</code> from <code>NamedTuple</code> functional to class syntax] What it does Checks for NamedTuple declarations that use functional syntax. Why is this bad? NamedTuple subclasses can be defined either through a functional syntax (Foo = NamedTuple(...)) or a class syntax (class Foo(NamedTuple): ...). The class syntax is more readable and generally preferred over the functional syntax, which exists primarily for backwards compatibility with collections.namedtuple. Example from typing import NamedTuple Foo = NamedTuple(\"Foo\", [(\"a\", int), (\"b\", str)]) Use instead: from typing import NamedTuple class Foo(NamedTuple): a: int b: str Fix safety This rule's fix is marked as unsafe if there are any comments within the range of the NamedTuple definition, as these will be dropped by the autofix.",
    "option": {}
  },
  "UP015": {
    "description": "[Status: stable] [Name: redundant-open-modes] [Message: Unnecessary mode argument] What it does Checks for redundant open mode arguments. Why is this bad? Redundant open mode arguments are unnecessary and should be removed to avoid confusion. Example with open(\"foo.txt\", \"r\") as f: ... Use instead: with open(\"foo.txt\") as f: ...",
    "option": {}
  },
  "UP017": {
    "description": "[Status: stable] [Name: datetime-timezone-utc] [Message: Use <code>datetime.UTC</code> alias] What it does Checks for uses of datetime.timezone.utc. Why is this bad? As of Python 3.11, datetime.UTC is an alias for datetime.timezone.utc. The alias is more readable and generally preferred over the full path. Example import datetime datetime.timezone.utc Use instead: import datetime datetime.UTC",
    "option": {
      "target-version": {
        "option_description": "The minimum Python version to target, e.g., when considering automatic\ncode upgrades, like rewriting type annotations. Ruff will not propose\nchanges using features that are not available in the given version.\n\nFor example, to represent supporting Python >=3.11 or ==3.11\nspecify target-version = \"py311\".\n\nIf you're already using a pyproject.toml file, we recommend\nproject.requires-python instead, as it's based on Python packaging\nstandards, and will be respected by other tools. For example, Ruff\ntreats the following as identical to target-version = \"py38\":\n\n[project]\nrequires-python = \">=3.8\"\n\nIf both are specified, target-version takes precedence over\nrequires-python. See Inferring the Python version\nfor a complete description of how the target-version is determined\nwhen left unspecified.\n\nNote that a stub file can sometimes make use of a typing feature\nbefore it is available at runtime, as long as the stub does not make\nuse of new syntax. For example, a type checker will understand\nint | str in a stub as being a Union type annotation, even if the\ntype checker is run using Python 3.9, despite the fact that the |\noperator can only be used to create union types at runtime on Python\n3.10+. As such, Ruff will often recommend newer features in a stub\nfile than it would for an equivalent runtime file with the same target\nversion.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"\n\n\n\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"",
        "option_data_type": "\"py37\" | \"py38\" | \"py39\" | \"py310\" | \"py311\" | \"py312\" | \"py313\" | \"py314\"",
        "default_value": "\"py310\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "UP018": {
    "description": "[Status: stable] [Name: native-literals] [Message: Unnecessary <code>{literal_type}</code> call (rewrite as a literal)] What it does Checks for unnecessary calls to str, bytes, int, float, and bool. Why is this bad? The mentioned constructors can be replaced with their respective literal forms, which are more readable and idiomatic. Example str(\"foo\") Use instead: \"foo\" Fix safety The fix is marked as unsafe if it might remove comments.",
    "option": {}
  },
  "UP019": {
    "description": "[Status: stable] [Name: typing-text-str-alias] [Message: <code>{}.Text</code> is deprecated, use <code>str</code>] What it does Checks for uses of typing.Text. In preview mode, also checks for typing_extensions.Text. Why is this bad? typing.Text is an alias for str, and only exists for Python 2 compatibility. As of Python 3.11, typing.Text is deprecated. Use str instead. Example from typing import Text foo: Text = \"bar\" Use instead: foo: str = \"bar\"",
    "option": {}
  },
  "UP020": {
    "description": "[Status: stable] [Name: open-alias] [Message: Use builtin <code>open</code>] What it does Checks for uses of io.open. Why is this bad? In Python 3, io.open is an alias for open. Prefer using open directly, as it is more idiomatic. Example import io with io.open(\"file.txt\") as f: ... Use instead: with open(\"file.txt\") as f: ...",
    "option": {}
  },
  "UP021": {
    "description": "[Status: stable] [Name: replace-universal-newlines] [Message: <code>universal_newlines</code> is deprecated, use <code>text</code>] What it does Checks for uses of subprocess.run that set the universal_newlines keyword argument. Why is this bad? As of Python 3.7, the universal_newlines keyword argument has been renamed to text, and now exists for backwards compatibility. The universal_newlines keyword argument may be removed in a future version of Python. Prefer text, which is more explicit and readable. Example import subprocess subprocess.run([\"foo\"], universal_newlines=True) Use instead: import subprocess subprocess.run([\"foo\"], text=True)",
    "option": {}
  },
  "UP022": {
    "description": "[Status: stable] [Name: replace-stdout-stderr] [Message: Prefer <code>capture_output</code> over sending <code>stdout</code> and <code>stderr</code> to <code>PIPE</code>] What it does Checks for uses of subprocess.run that send stdout and stderr to a pipe. Why is this bad? As of Python 3.7, subprocess.run has a capture_output keyword argument that can be set to True to capture stdout and stderr outputs. This is equivalent to setting stdout and stderr to subprocess.PIPE, but is more explicit and readable. Example import subprocess subprocess.run([\"foo\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE) Use instead: import subprocess subprocess.run([\"foo\"], capture_output=True) Fix safety This rule's fix is marked as unsafe because replacing stdout=subprocess.PIPE and stderr=subprocess.PIPE with capture_output=True may delete comments attached to the original arguments.",
    "option": {}
  },
  "UP023": {
    "description": "[Status: stable] [Name: deprecated-c-element-tree] [Message: <code>cElementTree</code> is deprecated, use <code>ElementTree</code>] What it does Checks for uses of the xml.etree.cElementTree module. Why is this bad? In Python 3.3, xml.etree.cElementTree was deprecated in favor of xml.etree.ElementTree. Example from xml.etree import cElementTree as ET Use instead: from xml.etree import ElementTree as ET",
    "option": {}
  },
  "UP024": {
    "description": "[Status: stable] [Name: os-error-alias] [Message: Replace aliased errors with <code>OSError</code>] What it does Checks for uses of exceptions that alias OSError. Why is this bad? OSError is the builtin error type used for exceptions that relate to the operating system. In Python 3.3, a variety of other exceptions, like WindowsError were aliased to OSError. These aliases remain in place for compatibility with older versions of Python, but may be removed in future versions. Prefer using OSError directly, as it is more idiomatic and future-proof. Example raise IOError Use instead: raise OSError",
    "option": {}
  },
  "UP025": {
    "description": "[Status: stable] [Name: unicode-kind-prefix] [Message: Remove unicode literals from strings] What it does Checks for uses of the Unicode kind prefix (u) in strings. Why is this bad? In Python 3, all strings are Unicode by default. The Unicode kind prefix is unnecessary and should be removed to avoid confusion. Example u\"foo\" Use instead: \"foo\"",
    "option": {}
  },
  "UP026": {
    "description": "[Status: stable] [Name: deprecated-mock-import] [Message: <code>mock</code> is deprecated, use <code>unittest.mock</code>] What it does Checks for imports of the mock module that should be replaced with unittest.mock. Why is this bad? Since Python 3.3, mock has been a part of the standard library as unittest.mock. The mock package is deprecated; use unittest.mock instead. Example import mock Use instead: from unittest import mock",
    "option": {
      "lint.isort.required-imports": {
        "option_description": "Add the specified import line to all files.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.isort]\nrequired-imports = [\"from __future__ import annotations\"]\n\n\n\n[lint.isort]\nrequired-imports = [\"from __future__ import annotations\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "UP028": {
    "description": "[Status: stable] [Name: yield-in-for-loop] [Message: Replace <code>yield</code> over <code>for</code> loop with <code>yield from</code>] What it does Checks for for loops that can be replaced with yield from expressions. Why is this bad? If a for loop only contains a yield statement, it can be replaced with a yield from expression, which is more concise and idiomatic. Example def bar(): for x in foo: yield x global y for y in foo: yield y Use instead: def bar(): yield from foo for _element in foo: y = _element yield y Fix safety This rule's fix is marked as unsafe, as converting a for loop to a yield from expression can change the behavior of the program in rare cases. For example, if a generator is being sent values via send, then rewriting to a yield from could lead to an attribute error if the underlying generator does not implement the send method. Additionally, if at least one target is global or nonlocal, no fix will be offered. In most cases, however, the fix is safe, and such a modification should have no effect on the behavior of the program.",
    "option": {}
  },
  "UP029": {
    "description": "[Status: stable] [Name: unnecessary-builtin-import] [Message: Unnecessary builtin import: <code>{import}</code>] What it does Checks for unnecessary imports of builtins. Why is this bad? Builtins are always available. Importing them is unnecessary and should be removed to avoid confusion. Example from builtins import str str(1) Use instead: str(1) Fix safety This fix is marked as unsafe because it will remove comments attached to the unused import.",
    "option": {
      "lint.isort.required-imports": {
        "option_description": "Add the specified import line to all files.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.isort]\nrequired-imports = [\"from __future__ import annotations\"]\n\n\n\n[lint.isort]\nrequired-imports = [\"from __future__ import annotations\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "UP030": {
    "description": "[Status: stable] [Name: format-literals] [Message: Use implicit references for positional format fields] What it does Checks for unnecessary positional indices in format strings. Why is this bad? In Python 3.1 and later, format strings can use implicit positional references. For example, \"{0}, {1}\".format(\"Hello\", \"World\") can be rewritten as \"{}, {}\".format(\"Hello\", \"World\"). If the positional indices appear exactly in-order, they can be omitted in favor of automatic indices to improve readability. Example \"{0}, {1}\".format(\"Hello\", \"World\") # \"Hello, World\" Use instead: \"{}, {}\".format(\"Hello\", \"World\") # \"Hello, World\" This fix is marked as unsafe because: Comments attached to arguments are not moved, which can cause comments to mismatch the actual arguments. If arguments have side effects (e.g., print), reordering may change program behavior.",
    "option": {}
  },
  "UP031": {
    "description": "[Status: stable] [Name: printf-string-formatting] [Message: Use format specifiers instead of percent format] What it does Checks for printf-style string formatting, and offers to replace it with str.format calls. Why is this bad? printf-style string formatting has a number of quirks, and leads to less readable code than using str.format calls or f-strings. In general, prefer the newer str.format and f-strings constructs over printf-style string formatting. Example \"%s, %s\" % (\"Hello\", \"World\") # \"Hello, World\" Use instead: \"{}, {}\".format(\"Hello\", \"World\") # \"Hello, World\" f\"{'Hello'}, {'World'}\" # \"Hello, World\" Fix safety In cases where the format string contains a single generic format specifier (e.g. %s), and the right-hand side is an ambiguous expression, we cannot offer a safe fix. For example, given: \"%s\" % val val could be a single-element tuple, or a single value (not contained in a tuple). Both of these would resolve to the same formatted string when using printf-style formatting, but resolve differently when using f-strings: val = 1 print(\"%s\" % val) # \"1\" print(\"{}\".format(val)) # \"1\" val = (1,) print(\"%s\" % val) # \"1\" print(\"{}\".format(val)) # \"(1,)\"",
    "option": {}
  },
  "UP032": {
    "description": "[Status: stable] [Name: f-string] [Message: Use f-string instead of <code>format</code> call] What it does Checks for str.format calls that can be replaced with f-strings. Why is this bad? f-strings are more readable and generally preferred over str.format calls. Example \"{}\".format(foo) Use instead: f\"{foo}\"",
    "option": {}
  },
  "UP033": {
    "description": "[Status: stable] [Name: lru-cache-with-maxsize-none] [Message: Use <code>@functools.cache</code> instead of <code>@functools.lru_cache(maxsize=None)</code>] What it does Checks for uses of functools.lru_cache that set maxsize=None. Why is this bad? Since Python 3.9, functools.cache can be used as a drop-in replacement for functools.lru_cache(maxsize=None). When possible, prefer functools.cache as it is more readable and idiomatic. Example import functools @functools.lru_cache(maxsize=None) def foo(): ... Use instead: import functools @functools.cache def foo(): ...",
    "option": {
      "target-version": {
        "option_description": "The minimum Python version to target, e.g., when considering automatic\ncode upgrades, like rewriting type annotations. Ruff will not propose\nchanges using features that are not available in the given version.\n\nFor example, to represent supporting Python >=3.11 or ==3.11\nspecify target-version = \"py311\".\n\nIf you're already using a pyproject.toml file, we recommend\nproject.requires-python instead, as it's based on Python packaging\nstandards, and will be respected by other tools. For example, Ruff\ntreats the following as identical to target-version = \"py38\":\n\n[project]\nrequires-python = \">=3.8\"\n\nIf both are specified, target-version takes precedence over\nrequires-python. See Inferring the Python version\nfor a complete description of how the target-version is determined\nwhen left unspecified.\n\nNote that a stub file can sometimes make use of a typing feature\nbefore it is available at runtime, as long as the stub does not make\nuse of new syntax. For example, a type checker will understand\nint | str in a stub as being a Union type annotation, even if the\ntype checker is run using Python 3.9, despite the fact that the |\noperator can only be used to create union types at runtime on Python\n3.10+. As such, Ruff will often recommend newer features in a stub\nfile than it would for an equivalent runtime file with the same target\nversion.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"\n\n\n\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"",
        "option_data_type": "\"py37\" | \"py38\" | \"py39\" | \"py310\" | \"py311\" | \"py312\" | \"py313\" | \"py314\"",
        "default_value": "\"py310\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "UP034": {
    "description": "[Status: stable] [Name: extraneous-parentheses] [Message: Avoid extraneous parentheses] What it does Checks for extraneous parentheses. Why is this bad? Extraneous parentheses are redundant, and can be removed to improve readability while retaining identical semantics. Example print((\"Hello, world\")) Use instead: print(\"Hello, world\")",
    "option": {}
  },
  "UP035": {
    "description": "[Status: stable] [Name: deprecated-import] [Message: Import from <code>{target}</code> instead: {names}] What it does Checks for uses of deprecated imports based on the minimum supported Python version. Why is this bad? Deprecated imports may be removed in future versions of Python, and should be replaced with their new equivalents. Note that, in some cases, it may be preferable to continue importing members from typing_extensions even after they're added to the Python standard library, as typing_extensions can backport bugfixes and optimizations from later Python versions. This rule thus avoids flagging imports from typing_extensions in such cases. Example from collections import Sequence Use instead: from collections.abc import Sequence",
    "option": {}
  },
  "UP036": {
    "description": "[Status: stable] [Name: outdated-version-block] [Message: Version block is outdated for minimum Python version] What it does Checks for conditional blocks gated on sys.version_info comparisons that are outdated for the minimum supported Python version. Why is this bad? In Python, code can be conditionally executed based on the active Python version by comparing against the sys.version_info tuple. If a code block is only executed for Python versions older than the minimum supported version, it should be removed. Example import sys if sys.version_info < (3, 0): print(\"py2\") else: print(\"py3\") Use instead: print(\"py3\") Fix safety This rule's fix is marked as unsafe because it will remove all code, comments, and annotations within unreachable version blocks.",
    "option": {
      "target-version": {
        "option_description": "The minimum Python version to target, e.g., when considering automatic\ncode upgrades, like rewriting type annotations. Ruff will not propose\nchanges using features that are not available in the given version.\n\nFor example, to represent supporting Python >=3.11 or ==3.11\nspecify target-version = \"py311\".\n\nIf you're already using a pyproject.toml file, we recommend\nproject.requires-python instead, as it's based on Python packaging\nstandards, and will be respected by other tools. For example, Ruff\ntreats the following as identical to target-version = \"py38\":\n\n[project]\nrequires-python = \">=3.8\"\n\nIf both are specified, target-version takes precedence over\nrequires-python. See Inferring the Python version\nfor a complete description of how the target-version is determined\nwhen left unspecified.\n\nNote that a stub file can sometimes make use of a typing feature\nbefore it is available at runtime, as long as the stub does not make\nuse of new syntax. For example, a type checker will understand\nint | str in a stub as being a Union type annotation, even if the\ntype checker is run using Python 3.9, despite the fact that the |\noperator can only be used to create union types at runtime on Python\n3.10+. As such, Ruff will often recommend newer features in a stub\nfile than it would for an equivalent runtime file with the same target\nversion.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"\n\n\n\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"",
        "option_data_type": "\"py37\" | \"py38\" | \"py39\" | \"py310\" | \"py311\" | \"py312\" | \"py313\" | \"py314\"",
        "default_value": "\"py310\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "UP037": {
    "description": "[Status: stable] [Name: quoted-annotation] [Message: Remove quotes from type annotation] What it does Checks for the presence of unnecessary quotes in type annotations. Why is this bad? In Python, type annotations can be quoted to avoid forward references. However, if from __future__ import annotations is present, Python will always evaluate type annotations in a deferred manner, making the quotes unnecessary. Similarly, if the annotation is located in a typing-only context and won't be evaluated by Python at runtime, the quotes will also be considered unnecessary. For example, Python does not evaluate type annotations on assignments in function bodies. Example Given: from __future__ import annotations def foo(bar: \"Bar\") -> \"Bar\": ... Use instead: from __future__ import annotations def foo(bar: Bar) -> Bar: ... Given: def foo() -> None: bar: \"Bar\" Use instead: def foo() -> None: bar: Bar Preview When preview is enabled, if lint.future-annotations is set to true, from __future__ import annotations will be added if doing so would allow an annotation to be unquoted. Fix safety The rule's fix is marked as safe, unless preview and lint.future-annotations are enabled and a from __future__ import annotations import is added. Such an import may change the behavior of all annotations in the file. See also quoted-annotation-in-stub: A rule that removes all quoted annotations from stub files quoted-type-alias: A rule that removes unnecessary quotes from type aliases.",
    "option": {
      "lint.future-annotations": {
        "option_description": "Whether to allow rules to add from __future__ import annotations in cases where this would\nsimplify a fix or enable a new diagnostic.\n\nFor example, TC001, TC002, and TC003 can move more imports into TYPE_CHECKING blocks\nif __future__ annotations are enabled.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\n# Enable `from __future__ import annotations` imports\nfuture-annotations = true\n\n\n\n[lint]\n# Enable `from __future__ import annotations` imports\nfuture-annotations = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "UP039": {
    "description": "[Status: stable] [Name: unnecessary-class-parentheses] [Message: Unnecessary parentheses after class definition] What it does Checks for class definitions that include unnecessary parentheses after the class name. Why is this bad? If a class definition doesn't have any bases, the parentheses are unnecessary. Example class Foo(): ... Use instead: class Foo: ... Fix safety This rule's fix is marked as unsafe if it would delete any comments within the parentheses range.",
    "option": {}
  },
  "UP040": {
    "description": "[Status: stable] [Name: non-pep695-type-alias] [Message: Type alias <code>{name}</code> uses {type_alias_method} instead of the <code>type</code> keyword] What it does Checks for use of TypeAlias annotations and TypeAliasType assignments for declaring type aliases. Why is this bad? The type keyword was introduced in Python 3.12 by PEP 695 for defining type aliases. The type keyword is easier to read and provides cleaner support for generics. Known problems PEP 695 uses inferred variance for type parameters, instead of the covariant and contravariant keywords used by TypeVar variables. As such, rewriting a type alias using a PEP-695 type statement may change the variance of the alias's type parameters. Unlike type aliases that use simple assignments, definitions created using PEP 695 type statements cannot be used as drop-in replacements at runtime for the value on the right-hand side of the statement. This means that while for some simple old-style type aliases you can use them as the second argument to an isinstance() call (for example), doing the same with a PEP 695 type statement will always raise TypeError at runtime. Example from typing import Annotated, TypeAlias, TypeAliasType from annotated_types import Gt ListOfInt: TypeAlias = list[int] PositiveInt = TypeAliasType(\"PositiveInt\", Annotated[int, Gt(0)]) Use instead: from typing import Annotated from annotated_types import Gt type ListOfInt = list[int] type PositiveInt = Annotated[int, Gt(0)] Fix safety This fix is marked unsafe for TypeAlias assignments outside of stub files because of the runtime behavior around isinstance() calls noted above. The fix is also unsafe for TypeAliasType assignments if there are any comments in the replacement range that would be deleted. See also This rule only applies to TypeAliases and TypeAliasTypes. See non-pep695-generic-class and non-pep695-generic-function for similar transformations for generic classes and functions. This rule replaces standalone type variables in aliases but doesn't remove the corresponding type variables even if they are unused after the fix. See unused-private-type-var for a rule to clean up unused private type variables. This rule will not rename private type variables to remove leading underscores, even though the new type parameters are restricted in scope to their associated aliases. See private-type-parameter for a rule to update these names.",
    "option": {
      "target-version": {
        "option_description": "The minimum Python version to target, e.g., when considering automatic\ncode upgrades, like rewriting type annotations. Ruff will not propose\nchanges using features that are not available in the given version.\n\nFor example, to represent supporting Python >=3.11 or ==3.11\nspecify target-version = \"py311\".\n\nIf you're already using a pyproject.toml file, we recommend\nproject.requires-python instead, as it's based on Python packaging\nstandards, and will be respected by other tools. For example, Ruff\ntreats the following as identical to target-version = \"py38\":\n\n[project]\nrequires-python = \">=3.8\"\n\nIf both are specified, target-version takes precedence over\nrequires-python. See Inferring the Python version\nfor a complete description of how the target-version is determined\nwhen left unspecified.\n\nNote that a stub file can sometimes make use of a typing feature\nbefore it is available at runtime, as long as the stub does not make\nuse of new syntax. For example, a type checker will understand\nint | str in a stub as being a Union type annotation, even if the\ntype checker is run using Python 3.9, despite the fact that the |\noperator can only be used to create union types at runtime on Python\n3.10+. As such, Ruff will often recommend newer features in a stub\nfile than it would for an equivalent runtime file with the same target\nversion.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"\n\n\n\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"",
        "option_data_type": "\"py37\" | \"py38\" | \"py39\" | \"py310\" | \"py311\" | \"py312\" | \"py313\" | \"py314\"",
        "default_value": "\"py310\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "UP041": {
    "description": "[Status: stable] [Name: timeout-error-alias] [Message: Replace aliased errors with <code>TimeoutError</code>] What it does Checks for uses of exceptions that alias TimeoutError. Why is this bad? TimeoutError is the builtin error type used for exceptions when a system function timed out at the system level. In Python 3.10, socket.timeout was aliased to TimeoutError. In Python 3.11, asyncio.TimeoutError was aliased to TimeoutError. These aliases remain in place for compatibility with older versions of Python, but may be removed in future versions. Prefer using TimeoutError directly, as it is more idiomatic and future-proof. Example import asyncio raise asyncio.TimeoutError Use instead: raise TimeoutError Fix safety This rule's fix is marked as unsafe if it would delete any comments within the exception expression range.",
    "option": {}
  },
  "UP042": {
    "description": "[Status: preview] [Name: replace-str-enum] [Message: Class {name} inherits from both <code>str</code> and <code>enum.Enum</code>] What it does Checks for classes that inherit from both str and enum.Enum. Why is this bad? Python 3.11 introduced enum.StrEnum, which is preferred over inheriting from both str and enum.Enum. Example import enum class Foo(str, enum.Enum): ... Use instead: import enum class Foo(enum.StrEnum): ... Fix safety Python 3.11 introduced a breaking change for enums that inherit from both str and enum.Enum. Consider the following enum: from enum import Enum class Foo(str, Enum): BAR = \"bar\" In Python 3.11, the formatted representation of Foo.BAR changed as follows: # Python 3.10 f\"{Foo.BAR}\" # > bar # Python 3.11 f\"{Foo.BAR}\" # > Foo.BAR Migrating from str and enum.Enum to enum.StrEnum will restore the previous behavior, such that: from enum import StrEnum class Foo(StrEnum): BAR = \"bar\" f\"{Foo.BAR}\" # > bar As such, migrating to enum.StrEnum will introduce a behavior change for code that relies on the Python 3.11 behavior.",
    "option": {
      "target-version": {
        "option_description": "The minimum Python version to target, e.g., when considering automatic\ncode upgrades, like rewriting type annotations. Ruff will not propose\nchanges using features that are not available in the given version.\n\nFor example, to represent supporting Python >=3.11 or ==3.11\nspecify target-version = \"py311\".\n\nIf you're already using a pyproject.toml file, we recommend\nproject.requires-python instead, as it's based on Python packaging\nstandards, and will be respected by other tools. For example, Ruff\ntreats the following as identical to target-version = \"py38\":\n\n[project]\nrequires-python = \">=3.8\"\n\nIf both are specified, target-version takes precedence over\nrequires-python. See Inferring the Python version\nfor a complete description of how the target-version is determined\nwhen left unspecified.\n\nNote that a stub file can sometimes make use of a typing feature\nbefore it is available at runtime, as long as the stub does not make\nuse of new syntax. For example, a type checker will understand\nint | str in a stub as being a Union type annotation, even if the\ntype checker is run using Python 3.9, despite the fact that the |\noperator can only be used to create union types at runtime on Python\n3.10+. As such, Ruff will often recommend newer features in a stub\nfile than it would for an equivalent runtime file with the same target\nversion.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"\n\n\n\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"",
        "option_data_type": "\"py37\" | \"py38\" | \"py39\" | \"py310\" | \"py311\" | \"py312\" | \"py313\" | \"py314\"",
        "default_value": "\"py310\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "UP043": {
    "description": "[Status: stable] [Name: unnecessary-default-type-args] [Message: Unnecessary default type arguments] What it does Checks for unnecessary default type arguments for Generator and AsyncGenerator on Python 3.13+. In preview, this rule will also apply to stub files. Why is this bad? Python 3.13 introduced the ability for type parameters to specify default values. Following this change, several standard-library classes were updated to add default values for some of their type parameters. For example, Generator[int] is now equivalent to Generator[int, None, None], as the second and third type parameters of Generator now default to None. Omitting type arguments that match the default values can make the code more concise and easier to read. Example from collections.abc import Generator, AsyncGenerator def sync_gen() -> Generator[int, None, None]: yield 42 async def async_gen() -> AsyncGenerator[int, None]: yield 42 Use instead: from collections.abc import Generator, AsyncGenerator def sync_gen() -> Generator[int]: yield 42 async def async_gen() -> AsyncGenerator[int]: yield 42 Fix safety This rule's fix is marked as safe, unless the type annotation contains comments.",
    "option": {
      "target-version": {
        "option_description": "The minimum Python version to target, e.g., when considering automatic\ncode upgrades, like rewriting type annotations. Ruff will not propose\nchanges using features that are not available in the given version.\n\nFor example, to represent supporting Python >=3.11 or ==3.11\nspecify target-version = \"py311\".\n\nIf you're already using a pyproject.toml file, we recommend\nproject.requires-python instead, as it's based on Python packaging\nstandards, and will be respected by other tools. For example, Ruff\ntreats the following as identical to target-version = \"py38\":\n\n[project]\nrequires-python = \">=3.8\"\n\nIf both are specified, target-version takes precedence over\nrequires-python. See Inferring the Python version\nfor a complete description of how the target-version is determined\nwhen left unspecified.\n\nNote that a stub file can sometimes make use of a typing feature\nbefore it is available at runtime, as long as the stub does not make\nuse of new syntax. For example, a type checker will understand\nint | str in a stub as being a Union type annotation, even if the\ntype checker is run using Python 3.9, despite the fact that the |\noperator can only be used to create union types at runtime on Python\n3.10+. As such, Ruff will often recommend newer features in a stub\nfile than it would for an equivalent runtime file with the same target\nversion.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"\n\n\n\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"",
        "option_data_type": "\"py37\" | \"py38\" | \"py39\" | \"py310\" | \"py311\" | \"py312\" | \"py313\" | \"py314\"",
        "default_value": "\"py310\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "UP044": {
    "description": "[Status: stable] [Name: non-pep646-unpack] [Message: Use <code>*</code> for unpacking] What it does Checks for uses of Unpack[] on Python 3.11 and above, and suggests using * instead. Why is this bad? PEP 646 introduced a new syntax for unpacking sequences based on the * operator. This syntax is more concise and readable than the previous Unpack[] syntax. Example from typing import Unpack def foo(*args: Unpack[tuple[int, ...]]) -> None: pass Use instead: def foo(*args: *tuple[int, ...]) -> None: pass Fix safety This rule's fix is marked as unsafe, as Unpack[T] and *T are considered different values when introspecting types at runtime. However, in most cases, the fix should be safe to apply.",
    "option": {
      "target-version": {
        "option_description": "The minimum Python version to target, e.g., when considering automatic\ncode upgrades, like rewriting type annotations. Ruff will not propose\nchanges using features that are not available in the given version.\n\nFor example, to represent supporting Python >=3.11 or ==3.11\nspecify target-version = \"py311\".\n\nIf you're already using a pyproject.toml file, we recommend\nproject.requires-python instead, as it's based on Python packaging\nstandards, and will be respected by other tools. For example, Ruff\ntreats the following as identical to target-version = \"py38\":\n\n[project]\nrequires-python = \">=3.8\"\n\nIf both are specified, target-version takes precedence over\nrequires-python. See Inferring the Python version\nfor a complete description of how the target-version is determined\nwhen left unspecified.\n\nNote that a stub file can sometimes make use of a typing feature\nbefore it is available at runtime, as long as the stub does not make\nuse of new syntax. For example, a type checker will understand\nint | str in a stub as being a Union type annotation, even if the\ntype checker is run using Python 3.9, despite the fact that the |\noperator can only be used to create union types at runtime on Python\n3.10+. As such, Ruff will often recommend newer features in a stub\nfile than it would for an equivalent runtime file with the same target\nversion.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"\n\n\n\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"",
        "option_data_type": "\"py37\" | \"py38\" | \"py39\" | \"py310\" | \"py311\" | \"py312\" | \"py313\" | \"py314\"",
        "default_value": "\"py310\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "UP045": {
    "description": "[Status: stable] [Name: non-pep604-annotation-optional] [Message: Use <code>X | None</code> for type annotations] What it does Check for typing.Optional annotations that can be rewritten based on PEP 604 syntax. Why is this bad? PEP 604 introduced a new syntax for union type annotations based on the | operator. This syntax is more concise and readable than the previous typing.Optional syntax. This rule is enabled when targeting Python 3.10 or later (see: target-version). By default, it's also enabled for earlier Python versions if from __future__ import annotations is present, as __future__ annotations are not evaluated at runtime. If your code relies on runtime type annotations (either directly or via a library like Pydantic), you can disable this behavior for Python versions prior to 3.10 by setting lint.pyupgrade.keep-runtime-typing to true. Example from typing import Optional foo: Optional[int] = None Use instead: foo: int | None = None Fix safety This rule's fix is marked as unsafe, as it may lead to runtime errors using libraries that rely on runtime type annotations, like Pydantic, on Python versions prior to Python 3.10, or as it may remove comments if they are present within the type annotation being rewritten. It may also lead to runtime errors in unusual and likely incorrect type annotations where the type does not support the | operator.",
    "option": {
      "target-version": {
        "option_description": "The minimum Python version to target, e.g., when considering automatic\ncode upgrades, like rewriting type annotations. Ruff will not propose\nchanges using features that are not available in the given version.\n\nFor example, to represent supporting Python >=3.11 or ==3.11\nspecify target-version = \"py311\".\n\nIf you're already using a pyproject.toml file, we recommend\nproject.requires-python instead, as it's based on Python packaging\nstandards, and will be respected by other tools. For example, Ruff\ntreats the following as identical to target-version = \"py38\":\n\n[project]\nrequires-python = \">=3.8\"\n\nIf both are specified, target-version takes precedence over\nrequires-python. See Inferring the Python version\nfor a complete description of how the target-version is determined\nwhen left unspecified.\n\nNote that a stub file can sometimes make use of a typing feature\nbefore it is available at runtime, as long as the stub does not make\nuse of new syntax. For example, a type checker will understand\nint | str in a stub as being a Union type annotation, even if the\ntype checker is run using Python 3.9, despite the fact that the |\noperator can only be used to create union types at runtime on Python\n3.10+. As such, Ruff will often recommend newer features in a stub\nfile than it would for an equivalent runtime file with the same target\nversion.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"\n\n\n\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"",
        "option_data_type": "\"py37\" | \"py38\" | \"py39\" | \"py310\" | \"py311\" | \"py312\" | \"py313\" | \"py314\"",
        "default_value": "\"py310\"",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.pyupgrade.keep-runtime-typing": {
        "option_description": "Whether to avoid PEP 585 (List[int] -> list[int]) and PEP 604\n(Union[str, int] -> str | int) rewrites even if a file imports\nfrom __future__ import annotations.\n\nThis setting is only applicable when the target Python version is below\n3.9 and 3.10 respectively, and is most commonly used when working with\nlibraries like Pydantic and FastAPI, which rely on the ability to parse\ntype annotations at runtime. The use of from __future__ import annotations\ncauses Python to treat the type annotations as strings, which typically\nallows for the use of language features that appear in later Python\nversions but are not yet supported by the current version (e.g., str |\nint). However, libraries that rely on runtime type annotations will\nbreak if the annotations are incompatible with the current Python\nversion.\n\nFor example, while the following is valid Python 3.8 code due to the\npresence of from __future__ import annotations, the use of str | int\nprior to Python 3.10 will cause Pydantic to raise a TypeError at\nruntime:\n\nfrom __future__ import annotations\n\nimport pydantic\n\nclass Foo(pydantic.BaseModel):\n    bar: str | int\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pyupgrade]\n# Preserve types, even if a file imports `from __future__ import annotations`.\nkeep-runtime-typing = true\n\n\n\n[lint.pyupgrade]\n# Preserve types, even if a file imports `from __future__ import annotations`.\nkeep-runtime-typing = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "UP046": {
    "description": "[Status: stable] [Name: non-pep695-generic-class] [Message: Generic class <code>{name}</code> uses <code>Generic</code> subclass instead of type parameters] What it does Checks for use of standalone type variables and parameter specifications in generic classes. Why is this bad? Special type parameter syntax was introduced in Python 3.12 by PEP 695 for defining generic classes. This syntax is easier to read and provides cleaner support for generics. Known problems The rule currently skips generic classes nested inside of other functions or classes. It also skips type parameters with the default argument introduced in PEP 696 and implemented in Python 3.13. This rule can only offer a fix if all of the generic types in the class definition are defined in the current module. For external type parameters, a diagnostic is emitted without a suggested fix. Not all type checkers fully support PEP 695 yet, so even valid fixes suggested by this rule may cause type checking to fail. Fix safety This fix is marked as unsafe, as PEP 695 uses inferred variance for type parameters, instead of the covariant and contravariant keywords used by TypeVar variables. As such, replacing a TypeVar variable with an inline type parameter may change its variance. Example from typing import Generic, TypeVar T = TypeVar(\"T\") class GenericClass(Generic[T]): var: T Use instead: class GenericClass[T]: var: T See also This rule replaces standalone type variables in classes but doesn't remove the corresponding type variables even if they are unused after the fix. See unused-private-type-var for a rule to clean up unused private type variables. This rule will not rename private type variables to remove leading underscores, even though the new type parameters are restricted in scope to their associated class. See private-type-parameter for a rule to update these names. This rule will correctly handle classes with multiple base classes, as long as the single Generic base class is at the end of the argument list, as checked by generic-not-last-base-class. If a Generic base class is found outside of the last position, a diagnostic is emitted without a suggested fix. This rule only applies to generic classes and does not include generic functions. See non-pep695-generic-function for the function version.",
    "option": {
      "target-version": {
        "option_description": "The minimum Python version to target, e.g., when considering automatic\ncode upgrades, like rewriting type annotations. Ruff will not propose\nchanges using features that are not available in the given version.\n\nFor example, to represent supporting Python >=3.11 or ==3.11\nspecify target-version = \"py311\".\n\nIf you're already using a pyproject.toml file, we recommend\nproject.requires-python instead, as it's based on Python packaging\nstandards, and will be respected by other tools. For example, Ruff\ntreats the following as identical to target-version = \"py38\":\n\n[project]\nrequires-python = \">=3.8\"\n\nIf both are specified, target-version takes precedence over\nrequires-python. See Inferring the Python version\nfor a complete description of how the target-version is determined\nwhen left unspecified.\n\nNote that a stub file can sometimes make use of a typing feature\nbefore it is available at runtime, as long as the stub does not make\nuse of new syntax. For example, a type checker will understand\nint | str in a stub as being a Union type annotation, even if the\ntype checker is run using Python 3.9, despite the fact that the |\noperator can only be used to create union types at runtime on Python\n3.10+. As such, Ruff will often recommend newer features in a stub\nfile than it would for an equivalent runtime file with the same target\nversion.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"\n\n\n\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"",
        "option_data_type": "\"py37\" | \"py38\" | \"py39\" | \"py310\" | \"py311\" | \"py312\" | \"py313\" | \"py314\"",
        "default_value": "\"py310\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "UP047": {
    "description": "[Status: stable] [Name: non-pep695-generic-function] [Message: Generic function <code>{name}</code> should use type parameters] What it does Checks for use of standalone type variables and parameter specifications in generic functions. Why is this bad? Special type parameter syntax was introduced in Python 3.12 by PEP 695 for defining generic functions. This syntax is easier to read and provides cleaner support for generics. Known problems The rule currently skips generic functions nested inside of other functions or classes and those with type parameters containing the default argument introduced in PEP 696 and implemented in Python 3.13. Not all type checkers fully support PEP 695 yet, so even valid fixes suggested by this rule may cause type checking to fail. Fix safety This fix is marked unsafe, as PEP 695 uses inferred variance for type parameters, instead of the covariant and contravariant keywords used by TypeVar variables. As such, replacing a TypeVar variable with an inline type parameter may change its variance. Additionally, if the rule cannot determine whether a parameter annotation corresponds to a type variable (e.g. for a type imported from another module), it will not add the type to the generic type parameter list. This causes the function to have a mix of old-style type variables and new-style generic type parameters, which will be rejected by type checkers. Example from typing import TypeVar T = TypeVar(\"T\") def generic_function(var: T) -> T: return var Use instead: def generic_function[T](var: T) -> T: return var See also This rule replaces standalone type variables in function signatures but doesn't remove the corresponding type variables even if they are unused after the fix. See unused-private-type-var for a rule to clean up unused private type variables. This rule will not rename private type variables to remove leading underscores, even though the new type parameters are restricted in scope to their associated function. See private-type-parameter for a rule to update these names. This rule only applies to generic functions and does not include generic classes. See non-pep695-generic-class for the class version.",
    "option": {
      "target-version": {
        "option_description": "The minimum Python version to target, e.g., when considering automatic\ncode upgrades, like rewriting type annotations. Ruff will not propose\nchanges using features that are not available in the given version.\n\nFor example, to represent supporting Python >=3.11 or ==3.11\nspecify target-version = \"py311\".\n\nIf you're already using a pyproject.toml file, we recommend\nproject.requires-python instead, as it's based on Python packaging\nstandards, and will be respected by other tools. For example, Ruff\ntreats the following as identical to target-version = \"py38\":\n\n[project]\nrequires-python = \">=3.8\"\n\nIf both are specified, target-version takes precedence over\nrequires-python. See Inferring the Python version\nfor a complete description of how the target-version is determined\nwhen left unspecified.\n\nNote that a stub file can sometimes make use of a typing feature\nbefore it is available at runtime, as long as the stub does not make\nuse of new syntax. For example, a type checker will understand\nint | str in a stub as being a Union type annotation, even if the\ntype checker is run using Python 3.9, despite the fact that the |\noperator can only be used to create union types at runtime on Python\n3.10+. As such, Ruff will often recommend newer features in a stub\nfile than it would for an equivalent runtime file with the same target\nversion.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"\n\n\n\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"",
        "option_data_type": "\"py37\" | \"py38\" | \"py39\" | \"py310\" | \"py311\" | \"py312\" | \"py313\" | \"py314\"",
        "default_value": "\"py310\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "UP049": {
    "description": "[Status: stable] [Name: private-type-parameter] [Message: Generic {} uses private type parameters] What it does Checks for use of PEP 695 type parameters with leading underscores in generic classes and functions. Why is this bad? PEP 695 type parameters are already restricted in scope to the class or function in which they appear, so leading underscores just hurt readability without the usual privacy benefits. However, neither a diagnostic nor a fix will be emitted for \"sunder\" (_T_) or \"dunder\" (__T__) type parameter names as these are not considered private. Example class GenericClass[_T]: var: _T def generic_function[_T](var: _T) -> list[_T]: return var[0] Use instead: class GenericClass[T]: var: T def generic_function[T](var: T) -> list[T]: return var[0] Fix availability If the name without an underscore would shadow a builtin or another variable, would be a keyword, or would otherwise be an invalid identifier, a fix will not be available. In these situations, you can consider using a trailing underscore or a different name entirely to satisfy the lint rule. See also This rule renames private PEP 695 type parameters but doesn't convert pre-PEP 695 generics to the new format. See non-pep695-generic-function and non-pep695-generic-class for rules that will make this transformation. Those rules do not remove unused type variables after their changes, so you may also want to consider enabling unused-private-type-var to complete the transition to PEP 695 generics.",
    "option": {}
  },
  "UP050": {
    "description": "[Status: stable] [Name: useless-class-metaclass-type] [Message: Class <code>{name}</code> uses <code>metaclass=type</code>, which is redundant] What it does Checks for metaclass=type in class definitions. Why is this bad? Since Python 3, the default metaclass is type, so specifying it explicitly is redundant. Even though __prepare__ is not required, the default metaclass (type) implements it, for the convenience of subclasses calling it via super(). Example class Foo(metaclass=type): ... Use instead: class Foo: ...",
    "option": {}
  },
  "FURB101": {
    "description": "[Status: preview] [Name: read-whole-file] [Message: <code>Path.open()</code> followed by <code>read()</code> can be replaced by <code>{filename}.{suggestion}</code>] What it does Checks for uses of open and read that can be replaced by pathlib methods, like Path.read_text and Path.read_bytes. Why is this bad? When reading the entire contents of a file into a variable, it's simpler and more concise to use pathlib methods like Path.read_text and Path.read_bytes instead of open and read calls via with statements. Example with open(\"file.txt\") as f: contents = f.read() Use instead: from pathlib import Path contents = Path(\"file.txt\").read_text() Fix Safety This rule's fix is marked as unsafe if the replacement would remove comments attached to the original expression.",
    "option": {}
  },
  "FURB103": {
    "description": "[Status: preview] [Name: write-whole-file] [Message: <code>Path.open()</code> followed by <code>write()</code> can be replaced by <code>{filename}.{suggestion}</code>] What it does Checks for uses of open and write that can be replaced by pathlib methods, like Path.write_text and Path.write_bytes. Why is this bad? When writing a single string to a file, it's simpler and more concise to use pathlib methods like Path.write_text and Path.write_bytes instead of open and write calls via with statements. Example with open(\"file.txt\", \"w\") as f: f.write(\"some text\") Use instead: from pathlib import Path Path(\"file.txt\").write_text(\"some text\") Fix Safety This rule's fix is marked as unsafe if the replacement would remove comments attached to the original expression.",
    "option": {}
  },
  "FURB105": {
    "description": "[Status: stable] [Name: print-empty-string] [Message: Unnecessary empty string passed to <code>print</code>] What it does Checks for print calls with unnecessary empty strings as positional arguments and unnecessary sep keyword arguments. Why is this bad? Prefer calling print without any positional arguments, which is equivalent and more concise. Similarly, when printing one or fewer items, the sep keyword argument, (used to define the string that separates the print arguments) can be omitted, as it's redundant when there are no items to separate. Example print(\"\") Use instead: print() Fix safety This fix is marked as unsafe if it removes comments or an unused sep keyword argument that may have side effects. Removing such arguments may change the program's behavior by skipping the execution of those side effects.",
    "option": {}
  },
  "FURB110": {
    "description": "[Status: preview] [Name: if-exp-instead-of-or-operator] [Message: Replace ternary <code>if</code> expression with <code>or</code> operator] What it does Checks for ternary if expressions that can be replaced with the or operator. Why is this bad? Ternary if expressions are more verbose than or expressions while providing the same functionality. Example z = x if x else y Use instead: z = x or y Fix safety This rule's fix is marked as unsafe in the event that the body of the if expression contains side effects. For example, foo will be called twice in foo() if foo() else bar() (assuming foo() returns a truthy value), but only once in foo() or bar().",
    "option": {}
  },
  "FURB113": {
    "description": "[Status: preview] [Name: repeated-append] [Message: Use <code>{suggestion}</code> instead of repeatedly calling <code>{name}.append()</code>] What it does Checks for consecutive calls to append. Why is this bad? Consecutive calls to append can be less efficient than batching them into a single extend. Each append resizes the list individually, whereas an extend can resize the list once for all elements. Known problems This rule is prone to false negatives due to type inference limitations, as it will only detect lists that are instantiated as literals or annotated with a type annotation. Example nums = [1, 2, 3] nums.append(4) nums.append(5) nums.append(6) Use instead: nums = [1, 2, 3] nums.extend((4, 5, 6))",
    "option": {}
  },
  "FURB116": {
    "description": "[Status: stable] [Name: f-string-number-format] [Message: Replace <code>{function_name}</code> call with <code>{display}</code>] What it does Checks for uses of bin(...)[2:] (or hex, or oct) to convert an integer into a string. Why is this bad? When converting an integer to a baseless binary, hexadecimal, or octal string, using f-strings is more concise and readable than using the bin, hex, or oct functions followed by a slice. Example print(bin(1337)[2:]) Use instead: print(f\"{1337:b}\") Fix safety The fix is only marked as safe for integer literals, all other cases are display-only, as they may change the runtime behavior of the program or introduce syntax errors. The fix for integer literals is also marked as unsafe if the expression contains comments that would be removed by the fix.",
    "option": {}
  },
  "FURB118": {
    "description": "[Status: preview] [Name: reimplemented-operator] [Message: Use <code>operator.{operator}</code> instead of defining a {target}] What it does Checks for lambda expressions and function definitions that can be replaced with a function from the operator module. Why is this bad? The operator module provides functions that implement the same functionality as the corresponding operators. For example, operator.add is often equivalent to lambda x, y: x + y. Using the functions from the operator module is more concise and communicates the intent of the code more clearly. Example import functools nums = [1, 2, 3] total = functools.reduce(lambda x, y: x + y, nums) Use instead: import functools import operator nums = [1, 2, 3] total = functools.reduce(operator.add, nums) Fix safety The fix offered by this rule is always marked as unsafe. While the changes the fix would make would rarely break your code, there are two ways in which functions from the operator module differ from user-defined functions. It would be non-trivial for Ruff to detect whether or not these differences would matter in a specific situation where Ruff is emitting a diagnostic for this rule. The first difference is that operator functions cannot be called with keyword arguments, but most user-defined functions can. If an add function is defined as add = lambda x, y: x + y, replacing this function with operator.add will cause the later call to raise TypeError if the function is later called with keyword arguments, e.g. add(x=1, y=2). The second difference is that user-defined functions are descriptors, but this is not true of the functions defined in the operator module. Practically speaking, this means that defining a function in a class body (either by using a def statement or assigning a lambda function to a variable) is a valid way of defining an instance method on that class; monkeypatching a user-defined function onto a class after the class has been created also has the same effect. The same is not true of an operator function: assigning an operator function to a variable in a class body or monkeypatching one onto a class will not create a valid instance method. Ruff will refrain from emitting diagnostics for this rule on function definitions in class bodies; however, it does not currently have sophisticated enough type inference to avoid emitting this diagnostic if a user-defined function is being monkeypatched onto a class after the class has been constructed.",
    "option": {}
  },
  "FURB122": {
    "description": "[Status: stable] [Name: for-loop-writes] [Message: Use of <code>{}.write</code> in a for loop] What it does Checks for the use of IOBase.write in a for loop. Why is this bad? When writing a batch of elements, it's more idiomatic to use a single method call to IOBase.writelines, rather than write elements one by one. Example from pathlib import Path with Path(\"file\").open(\"w\") as f: for line in lines: f.write(line) with Path(\"file\").open(\"wb\") as f_b: for line_b in lines_b: f_b.write(line_b.encode()) Use instead: from pathlib import Path with Path(\"file\").open(\"w\") as f: f.writelines(lines) with Path(\"file\").open(\"wb\") as f_b: f_b.writelines(line_b.encode() for line_b in lines_b) Fix safety This fix is marked as unsafe if it would cause comments to be deleted.",
    "option": {}
  },
  "FURB129": {
    "description": "[Status: stable] [Name: readlines-in-for] [Message: Instead of calling <code>readlines()</code>, iterate over file object directly] What it does Checks for uses of readlines() when iterating over a file line-by-line. Why is this bad? Rather than iterating over all lines in a file by calling readlines(), it's more convenient and performant to iterate over the file object directly. Example with open(\"file.txt\") as fp: for line in fp.readlines(): ... Use instead: with open(\"file.txt\") as fp: for line in fp: ... Fix safety This rule's fix is marked as unsafe if there's comments in the readlines() call, as comments may be removed. For example, the fix would be marked as unsafe in the following case: with open(\"file.txt\") as fp: for line in ( # comment fp.readlines() # comment ): ...",
    "option": {}
  },
  "FURB131": {
    "description": "[Status: preview] [Name: delete-full-slice] [Message: Prefer <code>clear</code> over deleting a full slice] What it does Checks for del statements that delete the entire slice of a list or dictionary. Why is this bad? It is faster and more succinct to remove all items via the clear() method. Known problems This rule is prone to false negatives due to type inference limitations, as it will only detect lists and dictionaries that are instantiated as literals or annotated with a type annotation. Example names = {\"key\": \"value\"} nums = [1, 2, 3] del names[:] del nums[:] Use instead: names = {\"key\": \"value\"} nums = [1, 2, 3] names.clear() nums.clear()",
    "option": {}
  },
  "FURB132": {
    "description": "[Status: stable] [Name: check-and-remove-from-set] [Message: Use <code>{suggestion}</code> instead of check and <code>remove</code>] What it does Checks for uses of set.remove that can be replaced with set.discard. Why is this bad? If an element should be removed from a set if it is present, it is more succinct and idiomatic to use discard. Known problems This rule is prone to false negatives due to type inference limitations, as it will only detect sets that are instantiated as literals or annotated with a type annotation. Example nums = {123, 456} if 123 in nums: nums.remove(123) Use instead: nums = {123, 456} nums.discard(123)",
    "option": {}
  },
  "FURB136": {
    "description": "[Status: stable] [Name: if-expr-min-max] [Message: Replace <code>if</code> expression with <code>{min_max}</code> call] What it does Checks for if expressions that can be replaced with min() or max() calls. Why is this bad? An if expression that selects the lesser or greater of two sub-expressions can be replaced with a min() or max() call respectively. When possible, prefer min() and max(), as they're more concise and readable than the equivalent if expression. Example score1, score2 = 4, 5 highest_score = score1 if score1 > score2 else score2 Use instead: score1, score2 = 4, 5 highest_score = max(score2, score1) Fix safety This rule's fix is marked as safe, unless the expression contains comments.",
    "option": {}
  },
  "FURB140": {
    "description": "[Status: preview] [Name: reimplemented-starmap] [Message: Use <code>itertools.starmap</code> instead of the generator] What it does Checks for generator expressions, list and set comprehensions that can be replaced with itertools.starmap. Why is this bad? When unpacking values from iterators to pass them directly to a function, prefer itertools.starmap. Using itertools.starmap is more concise and readable. Furthermore, it is more efficient than generator expressions, and in some versions of Python, it is more efficient than comprehensions. Known problems Since Python 3.12, itertools.starmap is less efficient than comprehensions (#7771). This is due to PEP 709, which made comprehensions faster. Example all(predicate(a, b) for a, b in some_iterable) Use instead: from itertools import starmap all(starmap(predicate, some_iterable)) Fix safety This rule's fix is marked as safe, unless the expression contains comments.",
    "option": {}
  },
  "FURB142": {
    "description": "[Status: preview] [Name: for-loop-set-mutations] [Message: Use of <code>set.{}()</code> in a for loop] What it does Checks for code that updates a set with the contents of an iterable by using a for loop to call .add() or .discard() on each element separately. Why is this bad? When adding or removing a batch of elements to or from a set, it's more idiomatic to use a single method call rather than adding or removing elements one by one. Example s = set() for x in (1, 2, 3): s.add(x) for x in (1, 2, 3): s.discard(x) Use instead: s = set() s.update((1, 2, 3)) s.difference_update((1, 2, 3)) Fix safety The fix will be marked as unsafe if applying the fix would delete any comments. Otherwise, it is marked as safe.",
    "option": {}
  },
  "FURB145": {
    "description": "[Status: preview] [Name: slice-copy] [Message: Prefer <code>copy</code> method over slicing] What it does Checks for unbounded slice expressions to copy a list. Why is this bad? The list.copy method is more readable and consistent with copying other types. Known problems This rule is prone to false negatives due to type inference limitations, as it will only detect lists that are instantiated as literals or annotated with a type annotation. Example a = [1, 2, 3] b = a[:] Use instead: a = [1, 2, 3] b = a.copy() Fix safety This rule's fix is marked as safe, unless the slice expression contains comments.",
    "option": {}
  },
  "FURB148": {
    "description": "[Status: preview] [Name: unnecessary-enumerate] [Message: <code>enumerate</code> value is unused, use <code>for x in range(len(y))</code> instead] What it does Checks for uses of enumerate that discard either the index or the value when iterating over a sequence. Why is this bad? The built-in enumerate function is useful when you need both the index and value of a sequence. If you only need the index or values of a sequence, you should iterate over range(len(...)) or the sequence itself, respectively, instead. This is more efficient and communicates the intent of the code more clearly. Known problems This rule is prone to false negatives due to type inference limitations; namely, it will only suggest a fix using the len builtin function if the sequence passed to enumerate is an instantiated as a list, set, dict, or tuple literal, or annotated as such with a type annotation. The len builtin function is not defined for all object types (such as generators), and so refactoring to use len over enumerate is not always safe. Example for index, _ in enumerate(sequence): print(index) for _, value in enumerate(sequence): print(value) Use instead: for index in range(len(sequence)): print(index) for value in sequence: print(value)",
    "option": {}
  },
  "FURB152": {
    "description": "[Status: preview] [Name: math-constant] [Message: Replace <code>{literal}</code> with <code>math.{constant}</code>] What it does Checks for literals that are similar to constants in math module. Why is this bad? Hard-coding mathematical constants like  increases code duplication, reduces readability, and may lead to a lack of precision. Example A = 3.141592 * r**2 Use instead: A = math.pi * r**2",
    "option": {}
  },
  "FURB154": {
    "description": "[Status: preview] [Name: repeated-global] [Message: Use of repeated consecutive <code>{}</code>] What it does Checks for consecutive global (or nonlocal) statements. Why is this bad? The global and nonlocal keywords accepts multiple comma-separated names. Instead of using multiple global (or nonlocal) statements for separate variables, you can use a single statement to declare multiple variables at once. Example def func(): global x global y print(x, y) Use instead: def func(): global x, y print(x, y) Fix safety This rule's fix is marked as safe, unless the statements contain comments.",
    "option": {}
  },
  "FURB156": {
    "description": "[Status: preview] [Name: hardcoded-string-charset] [Message: Use of hardcoded string charset] What it does Checks for uses of hardcoded charsets, which are defined in Python string module. Why is this bad? Usage of named charsets from the standard library is more readable and less error-prone. Example x = \"0123456789\" y in \"abcdefghijklmnopqrstuvwxyz\" Use instead import string x = string.digits y in string.ascii_lowercase",
    "option": {}
  },
  "FURB157": {
    "description": "[Status: stable] [Name: verbose-decimal-constructor] [Message: Verbose expression in <code>Decimal</code> constructor] What it does Checks for unnecessary string literal or float casts in Decimal constructors. Why is this bad? The Decimal constructor accepts a variety of arguments, including integers, floats, and strings. However, it's not necessary to cast integer literals to strings when passing them to the Decimal. Similarly, Decimal accepts inf, -inf, and nan as string literals, so there's no need to wrap those values in a float call when passing them to the Decimal constructor. Prefer the more concise form of argument passing for Decimal constructors, as it's more readable and idiomatic. Note that this rule does not flag quoted float literals such as Decimal(\"0.1\"), which will produce a more precise Decimal value than the unquoted Decimal(0.1). Example from decimal import Decimal Decimal(\"0\") Decimal(float(\"Infinity\")) Use instead: from decimal import Decimal Decimal(0) Decimal(\"Infinity\") Fix safety This rule's fix is marked as safe, unless the expression contains comments.",
    "option": {}
  },
  "FURB161": {
    "description": "[Status: stable] [Name: bit-count] [Message: Use of <code>bin({existing}).count('1')</code>] What it does Checks for uses of bin(...).count(\"1\") to perform a population count. Why is this bad? In Python 3.10, a bit_count() method was added to the int class, which is more concise and efficient than converting to a binary representation via bin(...). Example x = bin(123).count(\"1\") y = bin(0b1111011).count(\"1\") Use instead: x = (123).bit_count() y = 0b1111011.bit_count() Fix safety This rule's fix is marked as unsafe unless the argument to bin can be inferred as an instance of a type that implements the __index__ and bit_count methods because this can change the exception raised at runtime for an invalid argument.",
    "option": {
      "target-version": {
        "option_description": "The minimum Python version to target, e.g., when considering automatic\ncode upgrades, like rewriting type annotations. Ruff will not propose\nchanges using features that are not available in the given version.\n\nFor example, to represent supporting Python >=3.11 or ==3.11\nspecify target-version = \"py311\".\n\nIf you're already using a pyproject.toml file, we recommend\nproject.requires-python instead, as it's based on Python packaging\nstandards, and will be respected by other tools. For example, Ruff\ntreats the following as identical to target-version = \"py38\":\n\n[project]\nrequires-python = \">=3.8\"\n\nIf both are specified, target-version takes precedence over\nrequires-python. See Inferring the Python version\nfor a complete description of how the target-version is determined\nwhen left unspecified.\n\nNote that a stub file can sometimes make use of a typing feature\nbefore it is available at runtime, as long as the stub does not make\nuse of new syntax. For example, a type checker will understand\nint | str in a stub as being a Union type annotation, even if the\ntype checker is run using Python 3.9, despite the fact that the |\noperator can only be used to create union types at runtime on Python\n3.10+. As such, Ruff will often recommend newer features in a stub\nfile than it would for an equivalent runtime file with the same target\nversion.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"\n\n\n\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"",
        "option_data_type": "\"py37\" | \"py38\" | \"py39\" | \"py310\" | \"py311\" | \"py312\" | \"py313\" | \"py314\"",
        "default_value": "\"py310\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "FURB162": {
    "description": "[Status: stable] [Name: fromisoformat-replace-z] [Message: Unnecessary timezone replacement with zero offset] What it does Checks for datetime.fromisoformat() calls where the only argument is an inline replacement of Z with a zero offset timezone. Why is this bad? On Python 3.11 and later, datetime.fromisoformat() can handle most ISO 8601 formats including ones affixed with Z, so such an operation is unnecessary. More information on unsupported formats can be found in the official documentation. Example from datetime import datetime date = \"2025-01-01T00:00:00Z\" datetime.fromisoformat(date.replace(\"Z\", \"+00:00\")) datetime.fromisoformat(date[:-1] + \"-00\") datetime.fromisoformat(date.strip(\"Z\", \"-0000\")) datetime.fromisoformat(date.rstrip(\"Z\", \"-00:00\")) Use instead: from datetime import datetime date = \"2025-01-01T00:00:00Z\" datetime.fromisoformat(date) Fix safety The fix is always marked as unsafe, as it might change the program's behaviour. For example, working code might become non-working: d = \"Z2025-01-01T00:00:00Z\" # Note the leading `Z` datetime.fromisoformat(d.strip(\"Z\") + \"+00:00\") # Fine datetime.fromisoformat(d) # Runtime error",
    "option": {}
  },
  "FURB163": {
    "description": "[Status: stable] [Name: redundant-log-base] [Message: Prefer <code>math.{log_function}({arg})</code> over <code>math.log</code> with a redundant base] What it does Checks for math.log calls with a redundant base. Why is this bad? The default base of math.log is e, so specifying it explicitly is redundant. Instead of passing 2 or 10 as the base, use math.log2 or math.log10 respectively, as these dedicated variants are typically more accurate than math.log. Example import math math.log(4, math.e) math.log(4, 2) math.log(4, 10) Use instead: import math math.log(4) math.log2(4) math.log10(4) Fix safety This fix is marked unsafe when the argument is a starred expression, as this changes the call semantics and may raise runtime errors. It is also unsafe if comments are present within the call, as they will be removed. Additionally, math.log(x, base) and math.log2(x) / math.log10(x) may differ due to floating-point rounding, so the fix is also unsafe when making this transformation.",
    "option": {}
  },
  "FURB164": {
    "description": "[Status: preview] [Name: unnecessary-from-float] [Message: Verbose method <code>{method_name}</code> in <code>{constructor}</code> construction] What it does Checks for unnecessary from_float and from_decimal usages to construct Decimal and Fraction instances. Why is this bad? Since Python 3.2, the Fraction and Decimal classes can be constructed by passing float or decimal instances to the constructor directly. As such, the use of from_float and from_decimal methods is unnecessary, and should be avoided in favor of the more concise constructor syntax. However, there are important behavioral differences between the from_* methods and the constructors: The from_* methods validate their argument types and raise TypeError for invalid types The constructors accept broader argument types without validation The from_* methods have different parameter names than the constructors Example from decimal import Decimal from fractions import Fraction Decimal.from_float(4.2) Decimal.from_float(float(\"inf\")) Fraction.from_float(4.2) Fraction.from_decimal(Decimal(\"4.2\")) Use instead: from decimal import Decimal from fractions import Fraction Decimal(4.2) Decimal(\"inf\") Fraction(4.2) Fraction(Decimal(\"4.2\")) Fix safety This rule's fix is marked as unsafe by default because: The from_* methods provide type validation that the constructors don't Removing type validation can change program behavior The parameter names are different between methods and constructors The fix may remove comments attached to the original expression The fix is marked as safe only when: The argument type is known to be valid for the target constructor No keyword arguments are used, or they match the constructor's parameters",
    "option": {}
  },
  "FURB166": {
    "description": "[Status: stable] [Name: int-on-sliced-str] [Message: Use of <code>int</code> with explicit <code>base={base}</code> after removing prefix] What it does Checks for uses of int with an explicit base in which a string expression is stripped of its leading prefix (i.e., 0b, 0o, or 0x). Why is this bad? Given an integer string with a prefix (e.g., 0xABC), Python can automatically determine the base of the integer by the prefix without needing to specify it explicitly. Instead of int(num[2:], 16), use int(num, 0), which will automatically deduce the base based on the prefix. Example num = \"0xABC\" if num.startswith(\"0b\"): i = int(num[2:], 2) elif num.startswith(\"0o\"): i = int(num[2:], 8) elif num.startswith(\"0x\"): i = int(num[2:], 16) print(i) Use instead: num = \"0xABC\" i = int(num, 0) print(i) Fix safety The rule's fix is marked as unsafe, as Ruff cannot guarantee that the argument to int will remain valid when its base is included in the function call.",
    "option": {}
  },
  "FURB167": {
    "description": "[Status: stable] [Name: regex-flag-alias] [Message: Use of regular expression alias <code>re.{}</code>] What it does Checks for the use of shorthand aliases for regular expression flags (e.g., re.I instead of re.IGNORECASE). Why is this bad? The regular expression module provides descriptive names for each flag, along with single-letter aliases. Prefer the descriptive names, as they are more readable and self-documenting. Example import re if re.match(\"^hello\", \"hello world\", re.I): ... Use instead: import re if re.match(\"^hello\", \"hello world\", re.IGNORECASE): ...",
    "option": {}
  },
  "FURB168": {
    "description": "[Status: stable] [Name: isinstance-type-none] [Message: Prefer <code>is</code> operator over <code>isinstance</code> to check if an object is <code>None</code>] What it does Checks for uses of isinstance that check if an object is of type None. Why is this bad? There is only ever one instance of None, so it is more efficient and readable to use the is operator to check if an object is None. Example isinstance(obj, type(None)) Use instead: obj is None Fix safety The fix will be marked as unsafe if there are any comments within the call.",
    "option": {}
  },
  "FURB169": {
    "description": "[Status: stable] [Name: type-none-comparison] [Message: When checking against <code>None</code>, use <code>{}</code> instead of comparison with <code>type(None)</code>] What it does Checks for uses of type that compare the type of an object to the type of None. Why is this bad? There is only ever one instance of None, so it is more efficient and readable to use the is operator to check if an object is None. Example type(obj) is type(None) Use instead: obj is None Fix safety If the fix might remove comments, it will be marked as unsafe.",
    "option": {}
  },
  "FURB171": {
    "description": "[Status: preview] [Name: single-item-membership-test] [Message: Membership test against single-item container] What it does Checks for membership tests against single-item containers. Why is this bad? Performing a membership test against a container (like a list or set) with a single item is less readable and less efficient than comparing against the item directly. Example 1 in [1] Use instead: 1 == 1 Fix safety The fix is always marked as unsafe. When the right-hand side is a string, this fix can change the behavior of your program. This is because c in \"a\" is true both when c is \"a\" and when c is the empty string. Additionally, converting in/not in against a single-item container to ==/!= can change runtime behavior: in may consider identity (e.g., NaN) and always yields a bool. Comments within the replacement range will also be removed.",
    "option": {}
  },
  "FURB177": {
    "description": "[Status: stable] [Name: implicit-cwd] [Message: Prefer <code>Path.cwd()</code> over <code>Path().resolve()</code> for current-directory lookups] What it does Checks for current-directory lookups using Path().resolve(). Why is this bad? When looking up the current directory, prefer Path.cwd() over Path().resolve(), as Path.cwd() is more explicit in its intent. Example from pathlib import Path cwd = Path().resolve() Use instead: from pathlib import Path cwd = Path.cwd()",
    "option": {}
  },
  "FURB180": {
    "description": "[Status: preview] [Name: meta-class-abc-meta] [Message: Use of <code>metaclass=abc.ABCMeta</code> to define abstract base class] What it does Checks for uses of metaclass=abc.ABCMeta to define abstract base classes (ABCs). Why is this bad? Instead of class C(metaclass=abc.ABCMeta): ..., use class C(ABC): ... to define an abstract base class. Inheriting from the ABC wrapper class is semantically identical to setting metaclass=abc.ABCMeta, but more succinct. Example import abc class C(metaclass=abc.ABCMeta): pass Use instead: import abc class C(abc.ABC): pass Fix safety The rule's fix is unsafe if the class has base classes. This is because the base classes might be validating the class's other base classes (e.g., typing.Protocol does this) or otherwise alter runtime behavior if more base classes are added.",
    "option": {}
  },
  "FURB181": {
    "description": "[Status: stable] [Name: hashlib-digest-hex] [Message: Use of hashlib's <code>.digest().hex()</code>] What it does Checks for the use of .digest().hex() on a hashlib hash, like sha512. Why is this bad? When generating a hex digest from a hash, it's preferable to use the .hexdigest() method, rather than calling .digest() and then .hex(), as the former is more concise and readable. Example from hashlib import sha512 hashed = sha512(b\"some data\").digest().hex() Use instead: from hashlib import sha512 hashed = sha512(b\"some data\").hexdigest() Fix safety This rule's fix is marked as safe, unless the expression contains comments.",
    "option": {}
  },
  "FURB187": {
    "description": "[Status: stable] [Name: list-reverse-copy] [Message: Use of assignment of <code>reversed</code> on list <code>{name}</code>] What it does Checks for list reversals that can be performed in-place in lieu of creating a new list. Why is this bad? When reversing a list, it's more efficient to use the in-place method .reverse() instead of creating a new list, if the original list is no longer needed. Example l = [1, 2, 3] l = reversed(l) l = [1, 2, 3] l = list(reversed(l)) l = [1, 2, 3] l = l[::-1] Use instead: l = [1, 2, 3] l.reverse() Fix safety This rule's fix is marked as unsafe, as calling .reverse() on a list will mutate the list in-place, unlike reversed, which creates a new list and leaves the original list unchanged. If the list is referenced elsewhere, this could lead to unexpected behavior.",
    "option": {}
  },
  "FURB188": {
    "description": "[Status: stable] [Name: slice-to-remove-prefix-or-suffix] [Message: Prefer <code>str.removeprefix()</code> over conditionally replacing with slice.] What it does Checks for code that could be written more idiomatically using str.removeprefix() or str.removesuffix(). Specifically, the rule flags code that conditionally removes a prefix or suffix using a slice operation following an if test that uses str.startswith() or str.endswith(). The rule is only applied if your project targets Python 3.9 or later. Why is this bad? The methods str.removeprefix() and str.removesuffix(), introduced in Python 3.9, have the same behavior while being more readable and efficient. Example def example(filename: str, text: str): filename = filename[:-4] if filename.endswith(\".txt\") else filename if text.startswith(\"pre\"): text = text[3:] Use instead: def example(filename: str, text: str): filename = filename.removesuffix(\".txt\") text = text.removeprefix(\"pre\") Fix safety This rule's fix is marked as safe, unless the expression contains comments.",
    "option": {}
  },
  "FURB189": {
    "description": "[Status: preview] [Name: subclass-builtin] [Message: Subclassing <code>{subclass}</code> can be error prone, use <code>collections.{replacement}</code> instead] What it does Checks for subclasses of dict, list or str. Why is this bad? Built-in types don't consistently use their own dunder methods. For example, dict.__init__ and dict.update() bypass __setitem__, making inheritance unreliable. Use the UserDict, UserList, and UserString objects from the collections module instead. Example class UppercaseDict(dict): def __setitem__(self, key, value): super().__setitem__(key.upper(), value) d = UppercaseDict({\"a\": 1, \"b\": 2}) # Bypasses __setitem__ print(d) # {'a': 1, 'b': 2} Use instead: from collections import UserDict class UppercaseDict(UserDict): def __setitem__(self, key, value): super().__setitem__(key.upper(), value) d = UppercaseDict({\"a\": 1, \"b\": 2}) # Uses __setitem__ print(d) # {'A': 1, 'B': 2} Fix safety This fix is marked as unsafe because isinstance() checks for dict, list, and str types will fail when using the corresponding User class. If you need to pass custom dict or list objects to code you don't control, ignore this check. If you do control the code, consider using the following type checks instead: dict -> collections.abc.MutableMapping list -> collections.abc.MutableSequence str -> No such conversion exists",
    "option": {}
  },
  "FURB192": {
    "description": "[Status: preview] [Name: sorted-min-max] [Message: Prefer <code>min</code> over <code>sorted()</code> to compute the minimum value in a sequence] What it does Checks for uses of sorted() to retrieve the minimum or maximum value in a sequence. Why is this bad? Using sorted() to compute the minimum or maximum value in a sequence is inefficient and less readable than using min() or max() directly. Example nums = [3, 1, 4, 1, 5] lowest = sorted(nums)[0] highest = sorted(nums)[-1] highest = sorted(nums, reverse=True)[0] Use instead: nums = [3, 1, 4, 1, 5] lowest = min(nums) highest = max(nums) Fix safety In some cases, migrating to min or max can lead to a change in behavior, notably when breaking ties. As an example, sorted(data, key=itemgetter(0), reverse=True)[0] will return the last \"minimum\" element in the list, if there are multiple elements with the same key. However, min(data, key=itemgetter(0)) will return the first \"minimum\" element in the list in the same scenario. As such, this rule's fix is marked as unsafe.",
    "option": {}
  },
  "RUF001": {
    "description": "[Status: stable] [Name: ambiguous-unicode-character-string] [Message: String contains ambiguous {}. Did you mean {}?] What it does Checks for ambiguous Unicode characters in strings. Why is this bad? Some Unicode characters are visually similar to ASCII characters, but have different code points. For example, GREEK CAPITAL LETTER ALPHA (U+0391) is visually similar, but not identical, to the ASCII character A. The use of ambiguous Unicode characters can confuse readers, cause subtle bugs, and even make malicious code look harmless. In preview, this rule will also flag Unicode characters that are confusable with other, non-preferred Unicode characters. For example, the spec recommends GREEK CAPITAL LETTER OMEGA over OHM SIGN. You can omit characters from being flagged as ambiguous via the lint.allowed-confusables setting. Example print(\"ello, world!\") # \"\" is the Greek eta (`U+0397`). Use instead: print(\"Hello, world!\") # \"H\" is the Latin capital H (`U+0048`).",
    "option": {
      "lint.allowed-confusables": {
        "option_description": "A list of allowed \"confusable\" Unicode characters to ignore when\nenforcing RUF001, RUF002, and RUF003.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\n# Allow minus-sign (U+2212), greek-small-letter-rho (U+03C1), and the asterisk-operator (U+2217),\n# which could be confused for \"-\", \"p\", and \"*\", respectively.\nallowed-confusables = [\"\", \"\", \"\"]\n\n\n\n[lint]\n# Allow minus-sign (U+2212), greek-small-letter-rho (U+03C1), and the asterisk-operator (U+2217),\n# which could be confused for \"-\", \"p\", and \"*\", respectively.\nallowed-confusables = [\"\", \"\", \"\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "RUF002": {
    "description": "[Status: stable] [Name: ambiguous-unicode-character-docstring] [Message: Docstring contains ambiguous {}. Did you mean {}?] What it does Checks for ambiguous Unicode characters in docstrings. Why is this bad? Some Unicode characters are visually similar to ASCII characters, but have different code points. For example, GREEK CAPITAL LETTER ALPHA (U+0391) is visually similar, but not identical, to the ASCII character A. The use of ambiguous Unicode characters can confuse readers, cause subtle bugs, and even make malicious code look harmless. In preview, this rule will also flag Unicode characters that are confusable with other, non-preferred Unicode characters. For example, the spec recommends GREEK CAPITAL LETTER OMEGA over OHM SIGN. You can omit characters from being flagged as ambiguous via the lint.allowed-confusables setting. Example \"\"\"A lovely docstring (with a `U+FF09` parenthesis.\"\"\" Use instead: \"\"\"A lovely docstring (with no strange parentheses).\"\"\"",
    "option": {
      "lint.allowed-confusables": {
        "option_description": "A list of allowed \"confusable\" Unicode characters to ignore when\nenforcing RUF001, RUF002, and RUF003.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\n# Allow minus-sign (U+2212), greek-small-letter-rho (U+03C1), and the asterisk-operator (U+2217),\n# which could be confused for \"-\", \"p\", and \"*\", respectively.\nallowed-confusables = [\"\", \"\", \"\"]\n\n\n\n[lint]\n# Allow minus-sign (U+2212), greek-small-letter-rho (U+03C1), and the asterisk-operator (U+2217),\n# which could be confused for \"-\", \"p\", and \"*\", respectively.\nallowed-confusables = [\"\", \"\", \"\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "RUF003": {
    "description": "[Status: stable] [Name: ambiguous-unicode-character-comment] [Message: Comment contains ambiguous {}. Did you mean {}?] What it does Checks for ambiguous Unicode characters in comments. Why is this bad? Some Unicode characters are visually similar to ASCII characters, but have different code points. For example, GREEK CAPITAL LETTER ALPHA (U+0391) is visually similar, but not identical, to the ASCII character A. The use of ambiguous Unicode characters can confuse readers, cause subtle bugs, and even make malicious code look harmless. In preview, this rule will also flag Unicode characters that are confusable with other, non-preferred Unicode characters. For example, the spec recommends GREEK CAPITAL LETTER OMEGA over OHM SIGN. You can omit characters from being flagged as ambiguous via the lint.allowed-confusables setting. Example foo() # nqa # \"\" is Cyrillic (`U+043E`) Use instead: foo() # noqa # \"o\" is Latin (`U+006F`)",
    "option": {
      "lint.allowed-confusables": {
        "option_description": "A list of allowed \"confusable\" Unicode characters to ignore when\nenforcing RUF001, RUF002, and RUF003.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\n# Allow minus-sign (U+2212), greek-small-letter-rho (U+03C1), and the asterisk-operator (U+2217),\n# which could be confused for \"-\", \"p\", and \"*\", respectively.\nallowed-confusables = [\"\", \"\", \"\"]\n\n\n\n[lint]\n# Allow minus-sign (U+2212), greek-small-letter-rho (U+03C1), and the asterisk-operator (U+2217),\n# which could be confused for \"-\", \"p\", and \"*\", respectively.\nallowed-confusables = [\"\", \"\", \"\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "RUF005": {
    "description": "[Status: stable] [Name: collection-literal-concatenation] [Message: Consider <code>{expression}</code> instead of concatenation] What it does Checks for uses of the + operator to concatenate collections. Why is this bad? In Python, the + operator can be used to concatenate collections (e.g., x + y to concatenate the lists x and y). However, collections can be concatenated more efficiently using the unpacking operator (e.g., [*x, *y] to concatenate x and y). Prefer the unpacking operator to concatenate collections, as it is more readable and flexible. The * operator can unpack any iterable, whereas + operates only on particular sequences which, in many cases, must be of the same type. Example foo = [2, 3, 4] bar = [1] + foo + [5, 6] Use instead: foo = [2, 3, 4] bar = [1, *foo, 5, 6] Fix safety The fix is always marked as unsafe because the + operator uses the __add__ magic method and *-unpacking uses the __iter__ magic method. Both of these could have custom implementations, causing the fix to change program behaviour.",
    "option": {}
  },
  "RUF006": {
    "description": "[Status: stable] [Name: asyncio-dangling-task] [Message: Store a reference to the return value of <code>{expr}.{method}</code>] What it does Checks for asyncio.create_task and asyncio.ensure_future calls that do not store a reference to the returned result. Why is this bad? Per the asyncio documentation, the event loop only retains a weak reference to tasks. If the task returned by asyncio.create_task and asyncio.ensure_future is not stored in a variable, or a collection, or otherwise referenced, it may be garbage collected at any time. This can lead to unexpected and inconsistent behavior, as your tasks may or may not run to completion. Example import asyncio for i in range(10): # This creates a weak reference to the task, which may be garbage # collected at any time. asyncio.create_task(some_coro(param=i)) Use instead: import asyncio background_tasks = set() for i in range(10): task = asyncio.create_task(some_coro(param=i)) # Add task to the set. This creates a strong reference. background_tasks.add(task) # To prevent keeping references to finished tasks forever, # make each task remove its own reference from the set after # completion: task.add_done_callback(background_tasks.discard)",
    "option": {}
  },
  "RUF007": {
    "description": "[Status: stable] [Name: zip-instead-of-pairwise] [Message: Prefer <code>itertools.pairwise()</code> over <code>zip()</code> when iterating over successive pairs] What it does Checks for use of zip() to iterate over successive pairs of elements. Why is this bad? When iterating over successive pairs of elements, prefer itertools.pairwise() over zip(). itertools.pairwise() is more readable and conveys the intent of the code more clearly. Example letters = \"ABCD\" zip(letters, letters[1:]) # (\"A\", \"B\"), (\"B\", \"C\"), (\"C\", \"D\") Use instead: from itertools import pairwise letters = \"ABCD\" pairwise(letters) # (\"A\", \"B\"), (\"B\", \"C\"), (\"C\", \"D\") Fix safety The fix is always marked unsafe because it assumes that slicing an object (e.g., obj[1:]) produces a value with the same type and iteration behavior as the original object, which is not guaranteed for user-defined types that override __getitem__ without properly handling slices. Moreover, the fix could delete comments.",
    "option": {}
  },
  "RUF008": {
    "description": "[Status: stable] [Name: mutable-dataclass-default] [Message: Do not use mutable default values for dataclass attributes] What it does Checks for mutable default values in dataclass attributes. Why is this bad? Mutable default values share state across all instances of the dataclass. This can lead to bugs when the attributes are changed in one instance, as those changes will unexpectedly affect all other instances. Instead of sharing mutable defaults, use the field(default_factory=...) pattern. If the default value is intended to be mutable, it must be annotated with typing.ClassVar; otherwise, a ValueError will be raised. Example from dataclasses import dataclass @dataclass class A: # A list without a `default_factory` or `ClassVar` annotation # will raise a `ValueError`. mutable_default: list[int] = [] Use instead: from dataclasses import dataclass, field @dataclass class A: mutable_default: list[int] = field(default_factory=list) Or: from dataclasses import dataclass from typing import ClassVar @dataclass class A: mutable_default: ClassVar[list[int]] = []",
    "option": {}
  },
  "RUF009": {
    "description": "[Status: stable] [Name: function-call-in-dataclass-default-argument] [Message: Do not perform function call <code>{name}</code> in dataclass defaults] What it does Checks for function calls in dataclass attribute defaults. Why is this bad? Function calls are only performed once, at definition time. The returned value is then reused by all instances of the dataclass. This can lead to unexpected behavior when the function call returns a mutable object, as changes to the object will be shared across all instances. If a field needs to be initialized with a mutable object, use the field(default_factory=...) pattern. Attributes whose default arguments are NewType calls where the original type is immutable are ignored. Example from dataclasses import dataclass def simple_list() -> list[int]: return [1, 2, 3, 4] @dataclass class A: mutable_default: list[int] = simple_list() Use instead: from dataclasses import dataclass, field def creating_list() -> list[int]: return [1, 2, 3, 4] @dataclass class A: mutable_default: list[int] = field(default_factory=creating_list)",
    "option": {
      "lint.flake8-bugbear.extend-immutable-calls": {
        "option_description": "Additional callable functions to consider \"immutable\" when evaluating, e.g., the\nfunction-call-in-default-argument rule (B008) or function-call-in-dataclass-defaults\nrule (RUF009).\n\nExpects to receive a list of fully-qualified names (e.g., fastapi.Query, rather than\nQuery).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.flake8-bugbear]\n# Allow default arguments like, e.g., `data: List[str] = fastapi.Query(None)`.\nextend-immutable-calls = [\"fastapi.Depends\", \"fastapi.Query\"]\n\n\n\n[lint.flake8-bugbear]\n# Allow default arguments like, e.g., `data: List[str] = fastapi.Query(None)`.\nextend-immutable-calls = [\"fastapi.Depends\", \"fastapi.Query\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "RUF010": {
    "description": "[Status: stable] [Name: explicit-f-string-type-conversion] [Message: Use explicit conversion flag] What it does Checks for uses of str(), repr(), and ascii() as explicit type conversions within f-strings. Why is this bad? f-strings support dedicated conversion flags for these types, which are more succinct and idiomatic. Note that, in many cases, calling str() within an f-string is unnecessary and can be removed entirely, as the value will be converted to a string automatically, the notable exception being for classes that implement a custom __format__ method. Example a = \"some string\" f\"{repr(a)}\" Use instead: a = \"some string\" f\"{a!r}\"",
    "option": {}
  },
  "RUF012": {
    "description": "[Status: stable] [Name: mutable-class-default] [Message: Mutable class attributes should be annotated with <code>typing.ClassVar</code>] What it does Checks for mutable default values in class attributes. Why is this bad? Mutable default values share state across all instances of the class, while not being obvious. This can lead to bugs when the attributes are changed in one instance, as those changes will unexpectedly affect all other instances. Generally speaking, you probably want to avoid having mutable default values in the class body at all; instead, these variables should usually be initialized in __init__. However, other possible fixes for the issue can include: Explicitly annotating the variable with typing.ClassVar to indicate that it is intended to be shared across all instances. Using an immutable data type (e.g. a tuple instead of a list) for the default value. Example class A: variable_1: list[int] = [] variable_2: set[int] = set() variable_3: dict[str, int] = {} Use instead: class A: def __init__(self) -> None: self.variable_1: list[int] = [] self.variable_2: set[int] = set() self.variable_3: dict[str, int] = {} Or: from typing import ClassVar class A: variable_1: ClassVar[list[int]] = [] variable_2: ClassVar[set[int]] = set() variable_3: ClassVar[dict[str, int]] = {} Or: class A: variable_1: list[int] | None = None variable_2: set[int] | None = None variable_3: dict[str, int] | None = None Or: from collections.abc import Sequence, Mapping, Set as AbstractSet from types import MappingProxyType class A: variable_1: Sequence[int] = () variable_2: AbstractSet[int] = frozenset() variable_3: Mapping[str, int] = MappingProxyType({})",
    "option": {}
  },
  "RUF013": {
    "description": "[Status: stable] [Name: implicit-optional] [Message: PEP 484 prohibits implicit <code>Optional</code>] What it does Checks for the use of implicit Optional in type annotations when the default parameter value is None. If lint.future-annotations is set to true, from __future__ import annotations will be added if doing so would allow using the | operator on a Python version before 3.10. Why is this bad? Implicit Optional is prohibited by PEP 484. It is confusing and inconsistent with the rest of the type system. It's recommended to use Optional[T] instead. For Python 3.10 and later, you can also use T | None. Example def foo(arg: int = None): pass Use instead: from typing import Optional def foo(arg: Optional[int] = None): pass Or, for Python 3.10 and later: def foo(arg: int | None = None): pass If you want to use the | operator in Python 3.9 and earlier, you can use future imports: from __future__ import annotations def foo(arg: int | None = None): pass Limitations Type aliases are not supported and could result in false negatives. For example, the following code will not be flagged: Text = str | bytes def foo(arg: Text = None): pass Fix safety This fix is always marked as unsafe because it can change the behavior of code that relies on type hints, and it assumes the default value is always appropriatewhich might not be the case.",
    "option": {
      "target-version": {
        "option_description": "The minimum Python version to target, e.g., when considering automatic\ncode upgrades, like rewriting type annotations. Ruff will not propose\nchanges using features that are not available in the given version.\n\nFor example, to represent supporting Python >=3.11 or ==3.11\nspecify target-version = \"py311\".\n\nIf you're already using a pyproject.toml file, we recommend\nproject.requires-python instead, as it's based on Python packaging\nstandards, and will be respected by other tools. For example, Ruff\ntreats the following as identical to target-version = \"py38\":\n\n[project]\nrequires-python = \">=3.8\"\n\nIf both are specified, target-version takes precedence over\nrequires-python. See Inferring the Python version\nfor a complete description of how the target-version is determined\nwhen left unspecified.\n\nNote that a stub file can sometimes make use of a typing feature\nbefore it is available at runtime, as long as the stub does not make\nuse of new syntax. For example, a type checker will understand\nint | str in a stub as being a Union type annotation, even if the\ntype checker is run using Python 3.9, despite the fact that the |\noperator can only be used to create union types at runtime on Python\n3.10+. As such, Ruff will often recommend newer features in a stub\nfile than it would for an equivalent runtime file with the same target\nversion.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff]\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"\n\n\n\n# Always generate Python 3.7-compatible code.\ntarget-version = \"py37\"",
        "option_data_type": "\"py37\" | \"py38\" | \"py39\" | \"py310\" | \"py311\" | \"py312\" | \"py313\" | \"py314\"",
        "default_value": "\"py310\"",
        "option_values": {
          "valuelist": []
        }
      },
      "lint.future-annotations": {
        "option_description": "Whether to allow rules to add from __future__ import annotations in cases where this would\nsimplify a fix or enable a new diagnostic.\n\nFor example, TC001, TC002, and TC003 can move more imports into TYPE_CHECKING blocks\nif __future__ annotations are enabled.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\n# Enable `from __future__ import annotations` imports\nfuture-annotations = true\n\n\n\n[lint]\n# Enable `from __future__ import annotations` imports\nfuture-annotations = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "RUF015": {
    "description": "[Status: stable] [Name: unnecessary-iterable-allocation-for-first-element] [Message: Prefer <code>next({iterable})</code> over single element slice] What it does Checks the following constructs, all of which can be replaced by next(iter(...)): list(...)[0] tuple(...)[0] list(i for i in ...)[0] [i for i in ...][0] list(...).pop(0) Why is this bad? Calling e.g. list(...) will create a new list of the entire collection, which can be very expensive for large collections. If you only need the first element of the collection, you can use next(...) or next(iter(...) to lazily fetch the first element. The same is true for the other constructs. Example head = list(x)[0] head = [x * x for x in range(10)][0] Use instead: head = next(iter(x)) head = next(x * x for x in range(10)) Fix safety This rule's fix is marked as unsafe, as migrating from (e.g.) list(...)[0] to next(iter(...)) can change the behavior of your program in two ways: First, all above-mentioned constructs will eagerly evaluate the entire collection, while next(iter(...)) will only evaluate the first element. As such, any side effects that occur during iteration will be delayed. Second, accessing members of a collection via square bracket notation [0] or the pop() function will raise IndexError if the collection is empty, while next(iter(...)) will raise StopIteration.",
    "option": {}
  },
  "RUF016": {
    "description": "[Status: stable] [Name: invalid-index-type] [Message: Slice in indexed access to type <code>{value_type}</code> uses type <code>{index_type}</code> instead of an integer] What it does Checks for indexed access to lists, strings, tuples, bytes, and comprehensions using a type other than an integer or slice. Why is this bad? Only integers or slices can be used as indices to these types. Using other types will result in a TypeError at runtime and a SyntaxWarning at import time. Example var = [1, 2, 3][\"x\"] Use instead: var = [1, 2, 3][0]",
    "option": {}
  },
  "RUF017": {
    "description": "[Status: stable] [Name: quadratic-list-summation] [Message: Avoid quadratic list summation] What it does Checks for the use of sum() to flatten lists of lists, which has quadratic complexity. Why is this bad? The use of sum() to flatten lists of lists is quadratic in the number of lists, as sum() creates a new list for each element in the summation. Instead, consider using another method of flattening lists to avoid quadratic complexity. The following methods are all linear in the number of lists: functools.reduce(operator.iadd, lists, []) list(itertools.chain.from_iterable(lists)) [item for sublist in lists for item in sublist] When fixing relevant violations, Ruff defaults to the functools.reduce form, which outperforms the other methods in microbenchmarks. Example lists = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] joined = sum(lists, []) Use instead: import functools import operator lists = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] functools.reduce(operator.iadd, lists, []) Fix safety This fix is always marked as unsafe because sum uses the __add__ magic method while operator.iadd uses the __iadd__ magic method, and these behave differently on lists. The former requires the right summand to be a list, whereas the latter allows for any iterable. Therefore, the fix could inadvertently cause code that previously raised an error to silently succeed. Moreover, the fix could remove comments from the original code.",
    "option": {}
  },
  "RUF018": {
    "description": "[Status: stable] [Name: assignment-in-assert] [Message: Avoid assignment expressions in <code>assert</code> statements] What it does Checks for named assignment expressions (e.g., x := 0) in assert statements. Why is this bad? Named assignment expressions (also known as \"walrus operators\") are used to assign a value to a variable as part of a larger expression. Named assignments are syntactically valid in assert statements. However, when the Python interpreter is run under the -O flag, assert statements are not executed. In this case, the named assignment will also be ignored, which may result in unexpected behavior (e.g., undefined variable accesses). Example assert (x := 0) == 0 print(x) Use instead: x = 0 assert x == 0 print(x) The rule avoids flagging named expressions that define variables which are only referenced from inside assert statements; the following will not trigger the rule: assert (x := y**2) > 42, f\"Expected >42 but got {x}\" Nor will this: assert (x := y**2) > 42 assert x < 1_000_000",
    "option": {}
  },
  "RUF019": {
    "description": "[Status: stable] [Name: unnecessary-key-check] [Message: Unnecessary key check before dictionary access] What it does Checks for unnecessary key checks prior to accessing a dictionary. Why is this bad? When working with dictionaries, the get can be used to access a value without having to check if the dictionary contains the relevant key, returning None if the key is not present. Example if \"key\" in dct and dct[\"key\"]: ... Use instead: if dct.get(\"key\"): ... Fix safety This rule's fix is marked as safe, unless the expression contains comments.",
    "option": {}
  },
  "RUF020": {
    "description": "[Status: stable] [Name: never-union] [Message: <code>{never_like} | T</code> is equivalent to <code>T</code>] What it does Checks for uses of typing.NoReturn and typing.Never in union types. Why is this bad? typing.NoReturn and typing.Never are special types, used to indicate that a function never returns, or that a type has no values. Including typing.NoReturn or typing.Never in a union type is redundant, as, e.g., typing.Never | T is equivalent to T. Example from typing import Never def func() -> Never | int: ... Use instead: def func() -> int: ... Fix safety This rule's fix is marked as safe, unless the union type contains comments.",
    "option": {}
  },
  "RUF021": {
    "description": "[Status: stable] [Name: parenthesize-chained-operators] [Message: Parenthesize <code>a and b</code> expressions when chaining <code>and</code> and <code>or</code> together, to make the precedence clear] What it does Checks for chained operators where adding parentheses could improve the clarity of the code. Why is this bad? and always binds more tightly than or when chaining the two together, but this can be hard to remember (and sometimes surprising). Adding parentheses in these situations can greatly improve code readability, with no change to semantics or performance. For example: a, b, c = 1, 0, 2 x = a or b and c d, e, f = 0, 1, 2 y = d and e or f Use instead: a, b, c = 1, 0, 2 x = a or (b and c) d, e, f = 0, 1, 2 y = (d and e) or f",
    "option": {}
  },
  "RUF022": {
    "description": "[Status: stable] [Name: unsorted-dunder-all] [Message: <code>__all__</code> is not sorted] What it does Checks for __all__ definitions that are not ordered according to an \"isort-style\" sort. An isort-style sort orders items first according to their casing: SCREAMING_SNAKE_CASE names (conventionally used for global constants) come first, followed by CamelCase names (conventionally used for classes), followed by anything else. Within each category, a natural sort is used to order the elements. Why is this bad? Consistency is good. Use a common convention for __all__ to make your code more readable and idiomatic. Example import sys __all__ = [ \"b\", \"c\", \"a\", ] if sys.platform == \"win32\": __all__ += [\"z\", \"y\"] Use instead: import sys __all__ = [ \"a\", \"b\", \"c\", ] if sys.platform == \"win32\": __all__ += [\"y\", \"z\"] Fix safety This rule's fix is marked as unsafe if there are any comments that take up a whole line by themselves inside the __all__ definition, for example: __all__ = [ # eggy things \"duck_eggs\", \"chicken_eggs\", # hammy things \"country_ham\", \"parma_ham\", ] This is a common pattern used to delimit categories within a module's API, but it would be out of the scope of this rule to attempt to maintain these categories when alphabetically sorting the items of __all__. The fix is also marked as unsafe if there are more than two __all__ items on a single line and that line also has a trailing comment, since here it is impossible to accurately gauge which item the comment should be moved with when sorting __all__: __all__ = [ \"a\", \"c\", \"e\", # a comment \"b\", \"d\", \"f\", # a second comment ] Other than this, the rule's fix is marked as always being safe, in that it should very rarely alter the semantics of any Python code. However, note that (although it's rare) the value of __all__ could be read by code elsewhere that depends on the exact iteration order of the items in __all__, in which case this rule's fix could theoretically cause breakage.",
    "option": {}
  },
  "RUF023": {
    "description": "[Status: stable] [Name: unsorted-dunder-slots] [Message: <code>{}.__slots__</code> is not sorted] What it does Checks for __slots__ definitions that are not ordered according to a natural sort. Why is this bad? Consistency is good. Use a common convention for this special variable to make your code more readable and idiomatic. Example class Dog: __slots__ = \"name\", \"breed\" Use instead: class Dog: __slots__ = \"breed\", \"name\" Fix safety This rule's fix is marked as unsafe in three situations. Firstly, the fix is unsafe if there are any comments that take up a whole line by themselves inside the __slots__ definition, for example: class Foo: __slots__ = [ # eggy things \"duck_eggs\", \"chicken_eggs\", # hammy things \"country_ham\", \"parma_ham\", ] This is a common pattern used to delimit categories within a class's slots, but it would be out of the scope of this rule to attempt to maintain these categories when applying a natural sort to the items of __slots__. Secondly, the fix is also marked as unsafe if there are more than two __slots__ items on a single line and that line also has a trailing comment, since here it is impossible to accurately gauge which item the comment should be moved with when sorting __slots__: class Bar: __slots__ = [ \"a\", \"c\", \"e\", # a comment \"b\", \"d\", \"f\", # a second comment ] Lastly, this rule's fix is marked as unsafe whenever Ruff can detect that code elsewhere in the same file reads the __slots__ variable in some way and the __slots__ variable is not assigned to a set. This is because the order of the items in __slots__ may have semantic significance if the __slots__ of a class is being iterated over, or being assigned to another value. In the vast majority of other cases, this rule's fix is unlikely to cause breakage; as such, Ruff will otherwise mark this rule's fix as safe. However, note that (although it's rare) the value of __slots__ could still be read by code outside of the module in which the __slots__ definition occurs, in which case this rule's fix could theoretically cause breakage.",
    "option": {}
  },
  "RUF024": {
    "description": "[Status: stable] [Name: mutable-fromkeys-value] [Message: Do not pass mutable objects as values to <code>dict.fromkeys</code>] What it does Checks for mutable objects passed as a value argument to dict.fromkeys. Why is this bad? All values in the dictionary created by the dict.fromkeys method refer to the same instance of the provided object. If that object is modified, all values are modified, which can lead to unexpected behavior. For example, if the empty list ([]) is provided as the default value, all values in the dictionary will use the same list; as such, appending to any one entry will append to all entries. Instead, use a comprehension to generate a dictionary with distinct instances of the default value. Example cities = dict.fromkeys([\"UK\", \"Poland\"], []) cities[\"UK\"].append(\"London\") cities[\"Poland\"].append(\"Poznan\") print(cities) # {'UK': ['London', 'Poznan'], 'Poland': ['London', 'Poznan']} Use instead: cities = {country: [] for country in [\"UK\", \"Poland\"]} cities[\"UK\"].append(\"London\") cities[\"Poland\"].append(\"Poznan\") print(cities) # {'UK': ['London'], 'Poland': ['Poznan']} Fix safety This rule's fix is marked as unsafe, as the edit will change the behavior of the program by using a distinct object for every value in the dictionary, rather than a shared mutable instance. In some cases, programs may rely on the previous behavior.",
    "option": {}
  },
  "RUF026": {
    "description": "[Status: stable] [Name: default-factory-kwarg] [Message: <code>default_factory</code> is a positional-only argument to <code>defaultdict</code>] What it does Checks for incorrect usages of default_factory as a keyword argument when initializing a defaultdict. Why is this bad? The defaultdict constructor accepts a callable as its first argument. For example, it's common to initialize a defaultdict with int or list via defaultdict(int) or defaultdict(list), to create a dictionary that returns 0 or [] respectively when a key is missing. The default factory must be provided as a positional argument, as all keyword arguments to defaultdict are interpreted as initial entries in the dictionary. For example, defaultdict(foo=1, bar=2) will create a dictionary with {\"foo\": 1, \"bar\": 2} as its initial entries. As such, defaultdict(default_factory=list) will create a dictionary with {\"default_factory\": list} as its initial entry, instead of a dictionary that returns [] when a key is missing. Specifying a default_factory keyword argument is almost always a mistake, and one that type checkers can't reliably detect. Fix safety This rule's fix is marked as unsafe, as converting default_factory from a keyword to a positional argument will change the behavior of the code, even if the keyword argument was used erroneously. Example defaultdict(default_factory=int) defaultdict(default_factory=list) Use instead: defaultdict(int) defaultdict(list)",
    "option": {}
  },
  "RUF027": {
    "description": "[Status: preview] [Name: missing-f-string-syntax] [Message: Possible f-string without an <code>f</code> prefix] What it does Searches for strings that look like they were meant to be f-strings, but are missing an f prefix. Why is this bad? Expressions inside curly braces are only evaluated if the string has an f prefix. Details There are many possible string literals which are not meant to be f-strings despite containing f-string-like syntax. As such, this lint ignores all strings where one of the following conditions applies: The string is a standalone expression. For example, the rule ignores all docstrings. The string is part of a function call with argument names that match at least one variable (for example: format(\"Message: {value}\", value=\"Hello World\")) The string (or a parent expression of the string) has a direct method call on it (for example: \"{value}\".format(...)) The string has no {...} expression sections, or uses invalid f-string syntax. The string references variables that are not in scope, or it doesn't capture variables at all. Any format specifiers in the potential f-string are invalid. The string is part of a function call that is known to expect a template string rather than an evaluated f-string: for example, a logging call, a gettext call, or a FastAPI path. Example name = \"Sarah\" day_of_week = \"Tuesday\" print(\"Hello {name}! It is {day_of_week} today!\") Use instead: name = \"Sarah\" day_of_week = \"Tuesday\" print(f\"Hello {name}! It is {day_of_week} today!\") Fix safety This fix will always change the behavior of the program and, despite the precautions detailed above, this may be undesired. As such the fix is always marked as unsafe.",
    "option": {
      "lint.logger-objects": {
        "option_description": "A list of objects that should be treated equivalently to a\nlogging.Logger object.\n\nThis is useful for ensuring proper diagnostics (e.g., to identify\nlogging deprecations and other best-practices) for projects that\nre-export a logging.Logger object from a common module.\n\nFor example, if you have a module logging_setup.py with the following\ncontents:\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nAdding \"logging_setup.logger\" to logger-objects will ensure that\nlogging_setup.logger is treated as a logging.Logger object when\nimported from other modules (e.g., from logging_setup import logger).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\nlogger-objects = [\"logging_setup.logger\"]\n\n\n\n[lint]\nlogger-objects = [\"logging_setup.logger\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "RUF028": {
    "description": "[Status: stable] [Name: invalid-formatter-suppression-comment] [Message: This suppression comment is invalid because {}] What it does Checks for formatter suppression comments that are ineffective or incompatible with Ruff's formatter. Why is this bad? Suppression comments that do not actually prevent formatting could cause unintended changes when the formatter is run. Example In the following example, all suppression comments would cause a rule violation. def decorator(): pass @decorator # fmt: off def example(): if True: # fmt: skip expression = [ # fmt: off 1, 2, ] # yapf: disable # fmt: on # yapf: enable Fix safety This fix is always marked as unsafe because it deletes the invalid suppression comment, rather than trying to move it to a valid position, which the user more likely intended.",
    "option": {}
  },
  "RUF029": {
    "description": "[Status: preview] [Name: unused-async] [Message: Function <code>{name}</code> is declared <code>async</code>, but doesn't <code>await</code> or use <code>async</code> features.] What it does Checks for functions declared async that do not await or otherwise use features requiring the function to be declared async. Why is this bad? Declaring a function async when it's not is usually a mistake, and will artificially limit the contexts where that function may be called. In some cases, labeling a function async is semantically meaningful (e.g. with the trio library). Example async def foo(): bar() Use instead: def foo(): bar()",
    "option": {}
  },
  "RUF030": {
    "description": "[Status: stable] [Name: assert-with-print-message] [Message: <code>print()</code> call in <code>assert</code> statement is likely unintentional] What it does Checks for uses of assert expression, print(message). Why is this bad? If an assert x, y assertion fails, the Python interpreter raises an AssertionError, and the evaluated value of y is used as the contents of that assertion error. The print function always returns None, however, so the evaluated value of a call to print will always be None. Using a print call in this context will therefore output the message to stdout, before raising an empty AssertionError(None). Instead, remove the print and pass the message directly as the second expression, allowing stderr to capture the message in a well-formatted context. Example assert False, print(\"This is a message\") Use instead: assert False, \"This is a message\" Fix safety This rule's fix is marked as unsafe, as changing the second expression will result in a different AssertionError message being raised, as well as a change in stdout output.",
    "option": {}
  },
  "RUF031": {
    "description": "[Status: preview] [Name: incorrectly-parenthesized-tuple-in-subscript] [Message: Use parentheses for tuples in subscripts] What it does Checks for consistent style regarding whether nonempty tuples in subscripts are parenthesized. The exact nature of this violation depends on the setting lint.ruff.parenthesize-tuple-in-subscript. By default, the use of parentheses is considered a violation. This rule is not applied inside \"typing contexts\" (type annotations, type aliases and subscripted class bases), as these have their own specific conventions around them. Why is this bad? It is good to be consistent and, depending on the codebase, one or the other convention may be preferred. Example directions = {(0, 1): \"North\", (1, 0): \"East\", (0, -1): \"South\", (-1, 0): \"West\"} directions[(0, 1)] Use instead (with default setting): directions = {(0, 1): \"North\", (1, 0): \"East\", (0, -1): \"South\", (-1, 0): \"West\"} directions[0, 1]",
    "option": {
      "lint.ruff.parenthesize-tuple-in-subscript": {
        "option_description": "Whether to prefer accessing items keyed by tuples with\nparentheses around the tuple (see RUF031).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.ruff]\n# Make it a violation to use a tuple in a subscript without parentheses.\nparenthesize-tuple-in-subscript = true\n\n\n\n[lint.ruff]\n# Make it a violation to use a tuple in a subscript without parentheses.\nparenthesize-tuple-in-subscript = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "RUF032": {
    "description": "[Status: stable] [Name: decimal-from-float-literal] [Message: <code>Decimal()</code> called with float literal argument] What it does Checks for Decimal calls passing a float literal. Why is this bad? Float literals have limited precision that can lead to unexpected results. The Decimal class is designed to handle numbers with fixed-point precision, so a string literal should be used instead. Example num = Decimal(1.2345) Use instead: num = Decimal(\"1.2345\") Fix safety This rule's fix is marked as unsafe because it changes the underlying value of the Decimal instance that is constructed. This can lead to unexpected behavior if your program relies on the previous value (whether deliberately or not).",
    "option": {}
  },
  "RUF033": {
    "description": "[Status: stable] [Name: post-init-default] [Message: <code>__post_init__</code> method with argument defaults] What it does Checks for __post_init__ dataclass methods with parameter defaults. Why is this bad? Adding a default value to a parameter in a __post_init__ method has no impact on whether the parameter will have a default value in the dataclass's generated __init__ method. To create an init-only dataclass parameter with a default value, you should use an InitVar field in the dataclass's class body and give that InitVar field a default value. As the documentation states: Init-only fields are added as parameters to the generated __init__() method, and are passed to the optional __post_init__() method. They are not otherwise used by dataclasses. Example from dataclasses import InitVar, dataclass @dataclass class Foo: bar: InitVar[int] = 0 def __post_init__(self, bar: int = 1, baz: int = 2) -> None: print(bar, baz) foo = Foo() # Prints '0 2'. Use instead: from dataclasses import InitVar, dataclass @dataclass class Foo: bar: InitVar[int] = 1 baz: InitVar[int] = 2 def __post_init__(self, bar: int, baz: int) -> None: print(bar, baz) foo = Foo() # Prints '1 2'. Fix safety This fix is always marked as unsafe because, although switching to InitVar is usually correct, it is incorrect when the parameter is not intended to be part of the public API or when the value is meant to be shared across all instances.",
    "option": {}
  },
  "RUF034": {
    "description": "[Status: stable] [Name: useless-if-else] [Message: Useless <code>if</code>-<code>else</code> condition] What it does Checks for useless if-else conditions with identical arms. Why is this bad? Useless if-else conditions add unnecessary complexity to the code without providing any logical benefit. Assigning the value directly is clearer. Example foo = x if y else x Use instead: foo = x",
    "option": {}
  },
  "RUF036": {
    "description": "[Status: preview] [Name: none-not-at-end-of-union] [Message: <code>None</code> not at the end of the type annotation.] What it does Checks for type annotations where None is not at the end of an union. Why is this bad? Type annotation unions are commutative, meaning that the order of the elements does not matter. The None literal represents the absence of a value. For readability, it's preferred to write the more informative type expressions first. Example def func(arg: None | int): ... Use instead: def func(arg: int | None): ...",
    "option": {}
  },
  "RUF037": {
    "description": "[Status: preview] [Name: unnecessary-empty-iterable-within-deque-call] [Message: Unnecessary empty iterable within a deque call] What it does Checks for usages of collections.deque that have an empty iterable as the first argument. Why is this bad? It's unnecessary to use an empty literal as a deque's iterable, since this is already the default behavior. Example from collections import deque queue = deque(set()) queue = deque([], 10) Use instead: from collections import deque queue = deque() queue = deque(maxlen=10) Fix safety The fix is marked as unsafe whenever it would delete comments present in the deque call or if there are unrecognized arguments other than iterable and maxlen. Fix availability This rule's fix is unavailable if any starred arguments are present after the initial iterable.",
    "option": {}
  },
  "RUF038": {
    "description": "[Status: preview] [Name: redundant-bool-literal] [Message: <code>Literal[True, False, ...]</code> can be replaced with <code>Literal[...] | bool</code>] What it does Checks for Literal[True, False] type annotations. Why is this bad? Literal[True, False] can be replaced with bool in type annotations, which has the same semantic meaning but is more concise and readable. bool type has exactly two constant instances: True and False. Static type checkers such as mypy treat Literal[True, False] as equivalent to bool in a type annotation. Example from typing import Literal x: Literal[True, False] y: Literal[True, False, \"hello\", \"world\"] Use instead: from typing import Literal x: bool y: Literal[\"hello\", \"world\"] | bool Fix safety The fix for this rule is marked as unsafe, as it may change the semantics of the code. Specifically: Type checkers may not treat bool as equivalent when overloading boolean arguments with Literal[True] and Literal[False] (see, e.g., #14764 and #5421). bool is not strictly equivalent to Literal[True, False], as bool is a subclass of int, and this rule may not apply if the type annotations are used in a numeric context. Further, the Literal slice may contain trailing-line comments which the fix would remove.",
    "option": {}
  },
  "RUF039": {
    "description": "[Status: preview] [Name: unraw-re-pattern] [Message: First argument to {call} is not raw string] What it does Reports the following re and regex calls when their first arguments are not raw strings: For regex and re: compile, findall, finditer, fullmatch, match, search, split, sub, subn. regex-specific: splititer, subf, subfn, template. Why is this bad? Regular expressions should be written using raw strings to avoid double escaping. Fix safety The fix is unsafe if the string/bytes literal contains an escape sequence because the fix alters the runtime value of the literal while retaining the regex semantics. For example # Literal is `1\\n2`. re.compile(\"1\\n2\") # Literal is `1\\\\n2`, but the regex library will interpret `\\\\n` and will still match a newline # character as before. re.compile(r\"1\\n2\") Fix availability A fix is not available if either the argument is a string with a (no-op) u prefix (e.g., u\"foo\") as the prefix is incompatible with the raw prefix r the argument is a string or bytes literal with an escape sequence that has a different meaning in the context of a regular expression such as \\b, which is word boundary or backspace in a regex, depending on the context, but always a backspace in string and bytes literals. Example re.compile(\"foo\\\\bar\") Use instead: re.compile(r\"foo\\bar\")",
    "option": {}
  },
  "RUF040": {
    "description": "[Status: stable] [Name: invalid-assert-message-literal-argument] [Message: Non-string literal used as assert message] What it does Checks for invalid use of literals in assert message arguments. Why is this bad? An assert message which is a non-string literal was likely intended to be used in a comparison assertion, rather than as a message. Example fruits = [\"apples\", \"plums\", \"pears\"] fruits.filter(lambda fruit: fruit.startwith(\"p\")) assert len(fruits), 2 # True unless the list is empty Use instead: fruits = [\"apples\", \"plums\", \"pears\"] fruits.filter(lambda fruit: fruit.startwith(\"p\")) assert len(fruits) == 2",
    "option": {}
  },
  "RUF041": {
    "description": "[Status: stable] [Name: unnecessary-nested-literal] [Message: Unnecessary nested <code>Literal</code>] What it does Checks for unnecessary nested Literal. Why is this bad? Prefer using a single Literal, which is equivalent and more concise. Parameterization of literals by other literals is supported as an ergonomic feature as proposed in PEP 586, to enable patterns such as: ReadOnlyMode = Literal[\"r\", \"r+\"] WriteAndTruncateMode = Literal[\"w\", \"w+\", \"wt\", \"w+t\"] WriteNoTruncateMode = Literal[\"r+\", \"r+t\"] AppendMode = Literal[\"a\", \"a+\", \"at\", \"a+t\"] AllModes = Literal[ReadOnlyMode, WriteAndTruncateMode, WriteNoTruncateMode, AppendMode] As a consequence, type checkers also support nesting of literals which is less readable than a flat Literal: AllModes = Literal[Literal[\"r\", \"r+\"], Literal[\"w\", \"w+\", \"wt\", \"w+t\"], Literal[\"r+\", \"r+t\"], Literal[\"a\", \"a+\", \"at\", \"a+t\"]] Example AllModes = Literal[ Literal[\"r\", \"r+\"], Literal[\"w\", \"w+\", \"wt\", \"w+t\"], Literal[\"r+\", \"r+t\"], Literal[\"a\", \"a+\", \"at\", \"a+t\"], ] Use instead: AllModes = Literal[ \"r\", \"r+\", \"w\", \"w+\", \"wt\", \"w+t\", \"r+\", \"r+t\", \"a\", \"a+\", \"at\", \"a+t\" ] or assign the literal to a variable as in the first example. Fix safety The fix for this rule is marked as unsafe when the Literal slice is split across multiple lines and some of the lines have trailing comments.",
    "option": {}
  },
  "RUF043": {
    "description": "[Status: stable] [Name: pytest-raises-ambiguous-pattern] [Message: Pattern passed to <code>match=</code> contains metacharacters but is neither escaped nor raw] What it does Checks for non-raw literal string arguments passed to the match parameter of pytest.raises() where the string contains at least one unescaped regex metacharacter. Why is this bad? The match argument is implicitly converted to a regex under the hood. It should be made explicit whether the string is meant to be a regex or a \"plain\" pattern by prefixing the string with the r suffix, escaping the metacharacter(s) in the string using backslashes, or wrapping the entire string in a call to re.escape(). Example import pytest with pytest.raises(Exception, match=\"A full sentence.\"): do_thing_that_raises() If the pattern is intended to be a regular expression, use a raw string to signal this intention: import pytest with pytest.raises(Exception, match=r\"A full sentence.\"): do_thing_that_raises() Alternatively, escape any regex metacharacters with re.escape: import pytest import re with pytest.raises(Exception, match=re.escape(\"A full sentence.\")): do_thing_that_raises() or directly with backslashes: import pytest import re with pytest.raises(Exception, \"A full sentence\\\\.\"): do_thing_that_raises()",
    "option": {}
  },
  "RUF045": {
    "description": "[Status: preview] [Name: implicit-class-var-in-dataclass] [Message: Assignment without annotation found in dataclass body] What it does Checks for implicit class variables in dataclasses. Variables matching the lint.dummy-variable-rgx are excluded from this rule. Why is this bad? Class variables are shared between all instances of that class. In dataclasses, fields with no annotations at all are implicitly considered class variables, and a TypeError is raised if a user attempts to initialize an instance of the class with this field. @dataclass class C: a = 1 b: str = \"\" C(a = 42) # TypeError: C.__init__() got an unexpected keyword argument 'a' Example @dataclass class C: a = 1 Use instead: from typing import ClassVar @dataclass class C: a: ClassVar[int] = 1",
    "option": {
      "lint.dummy-variable-rgx": {
        "option_description": "A regular expression used to identify \"dummy\" variables, or those which\nshould be ignored when enforcing (e.g.) unused-variable rules. The\ndefault expression matches _, __, and _var, but not _var_.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\n# Only ignore variables named \"_\".\ndummy-variable-rgx = \"^_$\"\n\n\n\n[lint]\n# Only ignore variables named \"_\".\ndummy-variable-rgx = \"^_$\"",
        "option_data_type": "str",
        "default_value": "\"^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "RUF046": {
    "description": "[Status: stable] [Name: unnecessary-cast-to-int] [Message: Value being cast to <code>int</code> is already an integer] What it does Checks for int conversions of values that are already integers. Why is this bad? Such a conversion is unnecessary. Known problems This rule may produce false positives for round, math.ceil, math.floor, and math.trunc calls when values override the __round__, __ceil__, __floor__, or __trunc__ operators such that they don't return an integer. Example int(len([])) int(round(foo, None)) Use instead: len([]) round(foo) Fix safety The fix for round, math.ceil, math.floor, and math.truncate is unsafe because removing the int conversion can change the semantics for values overriding the __round__, __ceil__, __floor__, or __trunc__ dunder methods such that they don't return an integer.",
    "option": {}
  },
  "RUF047": {
    "description": "[Status: preview] [Name: needless-else] [Message: Empty <code>else</code> clause] What it does Checks for else clauses that only contains pass and ... statements. Why is this bad? Such an else clause does nothing and can be removed. Example if foo: bar() else: pass Use instead: if foo: bar()",
    "option": {}
  },
  "RUF048": {
    "description": "[Status: stable] [Name: map-int-version-parsing] [Message: <code>__version__</code> may contain non-integral-like elements] What it does Checks for calls of the form map(int, __version__.split(\".\")). Why is this bad? __version__ does not always contain integral-like elements. import matplotlib # `__version__ == \"3.9.1.post-1\"` in our environment # ValueError: invalid literal for int() with base 10: 'post1' tuple(map(int, matplotlib.__version__.split(\".\"))) See also Version specifiers | Packaging spec. Example tuple(map(int, matplotlib.__version__.split(\".\"))) Use instead: import packaging.version as version version.parse(matplotlib.__version__)",
    "option": {}
  },
  "RUF049": {
    "description": "[Status: stable] [Name: dataclass-enum] [Message: An enum class should not be decorated with <code>@dataclass</code>] What it does Checks for enum classes which are also decorated with @dataclass. Why is this bad? Decorating an enum with @dataclass() does not cause any errors at runtime, but may cause erroneous results: @dataclass class E(Enum): A = 1 B = 2 print(E.A == E.B) # True Example from dataclasses import dataclass from enum import Enum @dataclass class E(Enum): ... Use instead: from enum import Enum class E(Enum): ...",
    "option": {}
  },
  "RUF051": {
    "description": "[Status: stable] [Name: if-key-in-dict-del] [Message: Use <code>pop</code> instead of <code>key in dict</code> followed by <code>del dict[key]</code>] What it does Checks for if key in dictionary: del dictionary[key]. Why is this bad? To remove a key-value pair from a dictionary, it's more concise to use .pop(..., None). Example if key in dictionary: del dictionary[key] Use instead: dictionary.pop(key, None) Fix safety This rule's fix is marked as safe, unless the if statement contains comments.",
    "option": {}
  },
  "RUF052": {
    "description": "[Status: preview] [Name: used-dummy-variable] [Message: Local dummy variable <code>{}</code> is accessed] What it does Checks for \"dummy variables\" (variables that are named as if to indicate they are unused) that are in fact used. By default, \"dummy variables\" are any variables with names that start with leading underscores. However, this is customisable using the lint.dummy-variable-rgx setting). Why is this bad? Marking a variable with a leading underscore conveys that it is intentionally unused within the function or method. When these variables are later referenced in the code, it causes confusion and potential misunderstandings about the code's intention. If a variable marked as \"unused\" is subsequently used, it suggests that either the variable could be given a clearer name, or that the code is accidentally making use of the wrong variable. Sometimes leading underscores are used to avoid variables shadowing other variables, Python builtins, or Python keywords. However, PEP 8 recommends to use trailing underscores for this rather than leading underscores. Dunder variables are ignored by this rule, as are variables named _. Only local variables in function scopes are flagged by the rule. Example def function(): _variable = 3 # important: avoid shadowing the builtin `id()` function! _id = 4 return _variable + _id Use instead: def function(): variable = 3 # important: avoid shadowing the builtin `id()` function! id_ = 4 return variable + id_ Fix availability The rule's fix is only available for variables that start with leading underscores. It will also only be available if the \"obvious\" new name for the variable would not shadow any other known variables already accessible from the scope in which the variable is defined. Fix safety This rule's fix is marked as unsafe. For this rule's fix, Ruff renames the variable and fixes up all known references to it so they point to the renamed variable. However, some renamings also require other changes such as different arguments to constructor calls or alterations to comments. Ruff is aware of some of these cases: _T = TypeVar(\"_T\") will be fixed to T = TypeVar(\"T\") if the _T binding is flagged by this rule. However, in general, cases like these are hard to detect and hard to automatically fix.",
    "option": {
      "lint.dummy-variable-rgx": {
        "option_description": "A regular expression used to identify \"dummy\" variables, or those which\nshould be ignored when enforcing (e.g.) unused-variable rules. The\ndefault expression matches _, __, and _var, but not _var_.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\n# Only ignore variables named \"_\".\ndummy-variable-rgx = \"^_$\"\n\n\n\n[lint]\n# Only ignore variables named \"_\".\ndummy-variable-rgx = \"^_$\"",
        "option_data_type": "str",
        "default_value": "\"^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "RUF053": {
    "description": "[Status: stable] [Name: class-with-mixed-type-vars] [Message: Class with type parameter list inherits from <code>Generic</code>] What it does Checks for classes that have PEP 695 while also inheriting from typing.Generic or typing_extensions.Generic. Why is this bad? Such classes cause errors at runtime: from typing import Generic, TypeVar U = TypeVar(\"U\") # TypeError: Cannot inherit from Generic[...] multiple times. class C[T](Generic[U]): ... Example from typing import Generic, ParamSpec, TypeVar, TypeVarTuple U = TypeVar(\"U\") P = ParamSpec(\"P\") Ts = TypeVarTuple(\"Ts\") class C[T](Generic[U, P, *Ts]): ... Use instead: class C[T, U, **P, *Ts]: ... Fix safety As the fix changes runtime behaviour, it is always marked as unsafe. Additionally, comments within the fix range will not be preserved.",
    "option": {}
  },
  "RUF054": {
    "description": "[Status: preview] [Name: indented-form-feed] [Message: Indented form feed] What it does Checks for form feed characters preceded by either a space or a tab. Why is this bad? The language reference states: A formfeed character may be present at the start of the line; it will be ignored for the indentation calculations above. Formfeed characters occurring elsewhere in the leading whitespace have an undefined effect (for instance, they may reset the space count to zero). Example if foo():\\n \\fbar() Use instead: if foo():\\n bar()",
    "option": {}
  },
  "RUF055": {
    "description": "[Status: preview] [Name: unnecessary-regular-expression] [Message: Plain string pattern passed to <code>re</code> function] What it does Checks for uses of the re module that can be replaced with builtin str methods. Why is this bad? Performing checks on strings directly can make the code simpler, may require less escaping, and will often be faster. Example re.sub(\"abc\", \"\", s) Use instead: s.replace(\"abc\", \"\") Details The rule reports the following calls when the first argument to the call is a plain string literal, and no additional flags are passed: re.sub re.match re.search re.fullmatch re.split For re.sub, the repl (replacement) argument must also be a string literal, not a function. For re.match, re.search, and re.fullmatch, the return value must also be used only for its truth value. Fix safety This rule's fix is marked as unsafe if the affected expression contains comments. Otherwise, the fix can be applied safely.",
    "option": {}
  },
  "RUF056": {
    "description": "[Status: preview] [Name: falsy-dict-get-fallback] [Message: Avoid providing a falsy fallback to <code>dict.get()</code> in boolean test positions. The default fallback <code>None</code> is already falsy.] What it does Checks for dict.get(key, falsy_value) calls in boolean test positions. Why is this bad? The default fallback None is already falsy. Example if dict.get(key, False): ... Use instead: if dict.get(key): ... Fix safety This rule's fix is marked as safe, unless the dict.get() call contains comments between arguments that will be deleted. Fix availability This rule's fix is unavailable in cases where invalid arguments are provided to dict.get. As shown in the documentation, dict.get takes two positional-only arguments, so invalid cases are identified by the presence of more than two arguments or any keyword arguments.",
    "option": {}
  },
  "RUF057": {
    "description": "[Status: stable] [Name: unnecessary-round] [Message: Value being rounded is already an integer] What it does Checks for round() calls that have no effect on the input. Why is this bad? Rounding a value that's already an integer is unnecessary. It's clearer to use the value directly. Example a = round(1, 0) Use instead: a = 1 Fix safety The fix is marked unsafe if it is not possible to guarantee that the first argument of round() is of type int, or if the fix deletes comments.",
    "option": {}
  },
  "RUF058": {
    "description": "[Status: stable] [Name: starmap-zip] [Message: <code>itertools.starmap</code> called on <code>zip</code> iterable] What it does Checks for itertools.starmap calls where the second argument is a zip call. Why is this bad? zip-ping iterables only to unpack them later from within starmap is unnecessary. For such cases, map() should be used instead. Example from itertools import starmap starmap(func, zip(a, b)) starmap(func, zip(a, b, strict=True)) Use instead: map(func, a, b) map(func, a, b, strict=True) # 3.14+ Fix safety This rule's fix is marked as unsafe if the starmap or zip expressions contain comments that would be deleted by applying the fix. Otherwise, the fix can be applied safely. Fix availability This rule will emit a diagnostic but not suggest a fix if map has been shadowed from its builtin binding.",
    "option": {}
  },
  "RUF059": {
    "description": "[Status: stable] [Name: unused-unpacked-variable] [Message: Unpacked variable <code>{name}</code> is never used] What it does Checks for the presence of unused variables in unpacked assignments. Why is this bad? A variable that is defined but never used can confuse readers. If a variable is intentionally defined-but-not-used, it should be prefixed with an underscore, or some other value that adheres to the lint.dummy-variable-rgx pattern. Example def get_pair(): return 1, 2 def foo(): x, y = get_pair() return x Use instead: def foo(): x, _ = get_pair() return x See also This rule applies only to unpacked assignments. For regular assignments, see unused-variable.",
    "option": {
      "lint.dummy-variable-rgx": {
        "option_description": "A regular expression used to identify \"dummy\" variables, or those which\nshould be ignored when enforcing (e.g.) unused-variable rules. The\ndefault expression matches _, __, and _var, but not _var_.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\n# Only ignore variables named \"_\".\ndummy-variable-rgx = \"^_$\"\n\n\n\n[lint]\n# Only ignore variables named \"_\".\ndummy-variable-rgx = \"^_$\"",
        "option_data_type": "str",
        "default_value": "\"^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$\"",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "RUF060": {
    "description": "[Status: preview] [Name: in-empty-collection] [Message: Unnecessary membership test on empty collection] What it does Checks for membership tests on empty collections (such as list, tuple, set or dict). Why is this bad? If the collection is always empty, the check is unnecessary, and can be removed. Example if 1 not in set(): print(\"got it!\") Use instead: print(\"got it!\")",
    "option": {}
  },
  "RUF061": {
    "description": "[Status: preview] [Name: legacy-form-pytest-raises] [Message: Use context-manager form of <code>pytest.{}()</code>] What it does Checks for non-contextmanager use of pytest.raises, pytest.warns, and pytest.deprecated_call. Why is this bad? The context-manager form is more readable, easier to extend, and supports additional kwargs. Example import pytest excinfo = pytest.raises(ValueError, int, \"hello\") pytest.warns(UserWarning, my_function, arg) pytest.deprecated_call(my_deprecated_function, arg1, arg2) Use instead: import pytest with pytest.raises(ValueError) as excinfo: int(\"hello\") with pytest.warns(UserWarning): my_function(arg) with pytest.deprecated_call(): my_deprecated_function(arg1, arg2)",
    "option": {}
  },
  "RUF063": {
    "description": "[Status: preview] [Name: access-annotations-from-class-dict] [Message: Use <code>{suggestion}</code> instead of <code>__dict__</code> access] What it does Checks for uses of foo.__dict__.get(\"__annotations__\") or foo.__dict__[\"__annotations__\"] on Python 3.10+ and Python < 3.10 when typing-extensions is enabled. Why is this bad? Starting with Python 3.14, directly accessing __annotations__ via foo.__dict__.get(\"__annotations__\") or foo.__dict__[\"__annotations__\"] will only return annotations if the class is defined under from __future__ import annotations. Therefore, it is better to use dedicated library functions like annotationlib.get_annotations (Python 3.14+), inspect.get_annotations (Python 3.10+), or typing_extensions.get_annotations (for Python < 3.10 if typing-extensions is available). The benefits of using these functions include: Avoiding Undocumented Internals: They provide a stable, public API, unlike direct __dict__ access which relies on implementation details. Forward-Compatibility: They are designed to handle changes in Python's annotation system across versions, ensuring your code remains robust (e.g., correctly handling the Python 3.14 behavior mentioned above). See Python Annotations Best Practices for alternatives. Example foo.__dict__.get(\"__annotations__\", {}) # or foo.__dict__[\"__annotations__\"] On Python 3.14+, use instead: import annotationlib annotationlib.get_annotations(foo) On Python 3.10+, use instead: import inspect inspect.get_annotations(foo) On Python < 3.10 with typing-extensions installed, use instead: import typing_extensions typing_extensions.get_annotations(foo) Fix safety No autofix is currently provided for this rule. Fix availability No autofix is currently provided for this rule.",
    "option": {}
  },
  "RUF064": {
    "description": "[Status: preview] [Name: non-octal-permissions] [Message: Non-octal mode] What it does Checks for standard library functions which take a numeric mode argument where a non-octal integer literal is passed. Why is this bad? Numeric modes are made up of one to four octal digits. Converting a non-octal integer to octal may not be the mode the author intended. Example os.chmod(\"foo\", 644) Use instead: os.chmod(\"foo\", 0o644) Fix safety There are two categories of fix, the first of which is where it looks like the author intended to use an octal literal but the 0o prefix is missing: os.chmod(\"foo\", 400) os.chmod(\"foo\", 644) This class of fix changes runtime behaviour. In the first case, 400 corresponds to 0o620 (u=rw,g=w,o=). As this mode is not deemed likely, it is changed to 0o400 (u=r,go=). Similarly, 644 corresponds to 0o1204 (u=ws,g=,o=r) and is changed to 0o644 (u=rw,go=r). The second category is decimal literals which are recognised as likely valid but in decimal form: os.chmod(\"foo\", 256) os.chmod(\"foo\", 493) 256 corresponds to 0o400 (u=r,go=) and 493 corresponds to 0o755 (u=rwx,go=rx). Both of these fixes keep runtime behavior unchanged. If the original code really intended to use 0o256 (u=w,g=rx,o=rw) instead of 256, this fix should not be accepted. As a special case, zero is allowed to omit the 0o prefix unless it has multiple digits: os.chmod(\"foo\", 0) # Ok os.chmod(\"foo\", 0o000) # Ok os.chmod(\"foo\", 000) # Lint emitted and fix suggested Ruff will suggest a safe fix for multi-digit zeros to add the 0o prefix. Fix availability A fix is only available if the integer literal matches a set of common modes.",
    "option": {}
  },
  "RUF065": {
    "description": "[Status: preview] [Name: logging-eager-conversion] [Message: Unnecessary <code>oct()</code> conversion when formatting with <code>%s</code>. Use <code>%#o</code> instead of <code>%s</code>] What it does Checks for eager string conversion of arguments to logging calls. Why is this bad? Arguments to logging calls will be formatted as strings automatically, so it is unnecessary and less efficient to eagerly format the arguments before passing them in. Known problems This rule detects uses of the logging module via a heuristic. Specifically, it matches against: Uses of the logging module itself (e.g., import logging; logging.info(...)). Uses of flask.current_app.logger (e.g., from flask import current_app; current_app.logger.info(...)). Objects whose name starts with log or ends with logger or logging, when used in the same file in which they are defined (e.g., logger = logging.getLogger(); logger.info(...)). Imported objects marked as loggers via the lint.logger-objects setting, which can be used to enforce these rules against shared logger objects (e.g., from module import logger; logger.info(...), when lint.logger-objects is set to [\"module.logger\"]). Example import logging logging.basicConfig(format=\"%(message)s\", level=logging.INFO) user = \"Maria\" logging.info(\"%s - Something happened\", str(user)) Use instead: import logging logging.basicConfig(format=\"%(message)s\", level=logging.INFO) user = \"Maria\" logging.info(\"%s - Something happened\", user)",
    "option": {
      "lint.logger-objects": {
        "option_description": "A list of objects that should be treated equivalently to a\nlogging.Logger object.\n\nThis is useful for ensuring proper diagnostics (e.g., to identify\nlogging deprecations and other best-practices) for projects that\nre-export a logging.Logger object from a common module.\n\nFor example, if you have a module logging_setup.py with the following\ncontents:\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nAdding \"logging_setup.logger\" to logger-objects will ensure that\nlogging_setup.logger is treated as a logging.Logger object when\nimported from other modules (e.g., from logging_setup import logger).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\nlogger-objects = [\"logging_setup.logger\"]\n\n\n\n[lint]\nlogger-objects = [\"logging_setup.logger\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "RUF066": {
    "description": "[Status: preview] [Name: property-without-return] [Message: <code>{name}</code> is a property without a <code>return</code> statement] What it does Detects class @property methods that does not have a return statement. Why is this bad? Property methods are expected to return a computed value, a missing return in a property usually indicates an implementation mistake. Example class User: @property def full_name(self): f\"{self.first_name} {self.last_name}\" Use instead: class User: @property def full_name(self): return f\"{self.first_name} {self.last_name}\"",
    "option": {
      "lint.pydocstyle.property-decorators": {
        "option_description": "A list of decorators that, when applied to a method, indicate that the\nmethod should be treated as a property (in addition to the builtin\n@property and standard-library @functools.cached_property).\n\nFor example, Ruff will expect that any method decorated by a decorator\nin this list can use a non-imperative summary line.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.pydocstyle]\nproperty-decorators = [\"gi.repository.GObject.Property\"]\n\n\n\n[lint.pydocstyle]\nproperty-decorators = [\"gi.repository.GObject.Property\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "RUF067": {
    "description": "[Status: preview] [Name: non-empty-init-module] [Message: <code>__init__</code> module should not contain any code] What it does Detects the presence of code in __init__.py files. Why is this bad? __init__.py files are often empty or only contain simple code to modify a module's API. As such, it's easy to overlook them and their possible side effects when debugging. Example Instead of defining MyClass directly in __init__.py: \"\"\"My module docstring.\"\"\" class MyClass: def my_method(self): ... move the definition to another file, import it, and include it in __all__: \"\"\"My module docstring.\"\"\" from submodule import MyClass __all__ = [\"MyClass\"] Code in __init__.py files is also run at import time and can cause surprising slowdowns. To disallow any code in __init__.py files, you can enable the lint.ruff.strictly-empty-init-modules setting. In this case: from submodule import MyClass __all__ = [\"MyClass\"] the only fix is entirely emptying the file: Details In non-strict mode, this rule allows several common patterns in __init__.py files: Imports Assignments to __all__, __path__, __version__, and __author__ Module-level and attribute docstrings if TYPE_CHECKING blocks PEP-562 module-level __getattr__ and __dir__ functions",
    "option": {
      "lint.ruff.strictly-empty-init-modules": {
        "option_description": "Whether to require __init__.py files to contain no code at all, including imports and\ndocstrings (see RUF067).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint.ruff]\n# Make it a violation to include any code, including imports and docstrings in `__init__.py`\nstrictly-empty-init-modules = true\n\n\n\n[lint.ruff]\n# Make it a violation to include any code, including imports and docstrings in `__init__.py`\nstrictly-empty-init-modules = true",
        "option_data_type": "bool",
        "default_value": "false",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "RUF068": {
    "description": "[Status: preview] [Name: duplicate-entry-in-dunder-all] [Message: <code>__all__</code> contains duplicate entries] What it does Detects duplicate elements in __all__ definitions. Why is this bad? Duplicate elements in __all__ serve no purpose and can indicate copy-paste errors or incomplete refactoring. Example __all__ = [ \"DatabaseConnection\", \"Product\", \"User\", \"DatabaseConnection\", # Duplicate ] Use instead: __all__ = [ \"DatabaseConnection\", \"Product\", \"User\", ] Fix Safety This rule's fix is marked as unsafe if the replacement would remove comments attached to the original expression, potentially losing important context or documentation. For example: __all__ = [ \"PublicAPI\", # TODO: Remove this in v2.0 \"PublicAPI\", # Deprecated alias ]",
    "option": {}
  },
  "RUF100": {
    "description": "[Status: stable] [Name: unused-noqa] [Message: Unused {}] What it does Checks for noqa directives that are no longer applicable. Why is this bad? A noqa directive that no longer matches any diagnostic violations is likely included by mistake, and should be removed to avoid confusion. Example import foo # noqa: F401 def bar(): foo.bar() Use instead: import foo def bar(): foo.bar() Conflict with other linters When using RUF100 with the --fix option, Ruff may remove trailing comments that follow a # noqa directive on the same line, as it interprets the remainder of the line as a description for the suppression. To prevent Ruff from removing suppressions for other tools (like pylint or mypy), separate them with a second # character: # Bad: Ruff --fix will remove the pylint comment def visit_ImportFrom(self, node): # noqa: N802, pylint: disable=invalid-name pass # Good: Ruff will preserve the pylint comment def visit_ImportFrom(self, node): # noqa: N802 # pylint: disable=invalid-name pass",
    "option": {
      "lint.external": {
        "option_description": "A list of rule codes or prefixes that are unsupported by Ruff, but should be\npreserved when (e.g.) validating # noqa directives. Useful for\nretaining # noqa directives that cover plugins not yet implemented\nby Ruff.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\n# Avoiding flagging (and removing) any codes starting with `V` from any\n# `# noqa` directives, despite Ruff's lack of support for `vulture`.\nexternal = [\"V\"]\n\n\n\n[lint]\n# Avoiding flagging (and removing) any codes starting with `V` from any\n# `# noqa` directives, despite Ruff's lack of support for `vulture`.\nexternal = [\"V\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "RUF101": {
    "description": "[Status: stable] [Name: redirected-noqa] [Message: <code>{original}</code> is a redirect to <code>{target}</code>] What it does Checks for noqa directives that use redirected rule codes. Why is this bad? When one of Ruff's rule codes has been redirected, the implication is that the rule has been deprecated in favor of another rule or code. To keep your codebase consistent and up-to-date, prefer the canonical rule code over the deprecated code. Example x = eval(command) # noqa: PGH001 Use instead: x = eval(command) # noqa: S307",
    "option": {}
  },
  "RUF102": {
    "description": "[Status: preview] [Name: invalid-rule-code] [Message: Invalid rule code in {}: {}] What it does Checks for noqa codes that are invalid. Why is this bad? Invalid rule codes serve no purpose and may indicate outdated code suppressions. Example import os # noqa: XYZ999 Use instead: import os Or if there are still valid codes needed: import os # noqa: E402",
    "option": {
      "lint.external": {
        "option_description": "A list of rule codes or prefixes that are unsupported by Ruff, but should be\npreserved when (e.g.) validating # noqa directives. Useful for\nretaining # noqa directives that cover plugins not yet implemented\nby Ruff.\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\n# Avoiding flagging (and removing) any codes starting with `V` from any\n# `# noqa` directives, despite Ruff's lack of support for `vulture`.\nexternal = [\"V\"]\n\n\n\n[lint]\n# Avoiding flagging (and removing) any codes starting with `V` from any\n# `# noqa` directives, despite Ruff's lack of support for `vulture`.\nexternal = [\"V\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "RUF103": {
    "description": "[Status: preview] [Name: invalid-suppression-comment] [Message: Invalid suppression comment: {msg}] What it does Checks for invalid suppression comments Why is this bad? Invalid suppression comments are ignored by Ruff, and should either be fixed or removed to avoid confusion. Example # ruff: disable # missing codes Use instead: # ruff: disable[E501] Or delete the invalid suppression comment.",
    "option": {}
  },
  "RUF104": {
    "description": "[Status: preview] [Name: unmatched-suppression-comment] [Message: Suppression comment without matching <code>#ruff:enable</code> comment] What it does Checks for unmatched range suppression comments Why is this bad? Unmatched range suppression comments can inadvertently suppress violations over larger sections of code than intended, particularly at module scope. Example def foo(): # ruff: disable[E501] # unmatched REALLY_LONG_VALUES = [...] print(REALLY_LONG_VALUES) Use instead: def foo(): # ruff: disable[E501] REALLY_LONG_VALUES = [...] # ruff: enable[E501] print(REALLY_LONG_VALUES)",
    "option": {}
  },
  "RUF200": {
    "description": "[Status: stable] [Name: invalid-pyproject-toml] [Message: Failed to parse pyproject.toml: {message}] What it does Checks for any pyproject.toml that does not conform to the schema from the relevant PEPs. Why is this bad? Your project may contain invalid metadata or configuration without you noticing Example [project] name = \"crab\" version = \"1.0.0\" authors = [\"Ferris the Crab <[email protected]>\"] Use instead: [project] name = \"crab\" version = \"1.0.0\" authors = [ { name = \"Ferris the Crab\", email = \"[email protected]\" } ]",
    "option": {}
  },
  "TRY002": {
    "description": "[Status: stable] [Name: raise-vanilla-class] [Message: Create your own exception] What it does Checks for code that raises Exception or BaseException directly. Why is this bad? Handling such exceptions requires the use of except Exception or except BaseException. These will capture almost any raised exception, including failed assertions, division by zero, and more. Prefer to raise your own exception, or a more specific built-in exception, so that you can avoid over-capturing exceptions that you don't intend to handle. Example def main_function(): if not cond: raise Exception() def consumer_func(): try: do_step() prepare() main_function() except Exception: logger.error(\"Oops\") Use instead: def main_function(): if not cond: raise CustomException() def consumer_func(): try: do_step() prepare() main_function() except CustomException: logger.error(\"Main function failed\") except Exception: logger.error(\"Oops\")",
    "option": {}
  },
  "TRY003": {
    "description": "[Status: stable] [Name: raise-vanilla-args] [Message: Avoid specifying long messages outside the exception class] What it does Checks for long exception messages that are not defined in the exception class itself. Why is this bad? By formatting an exception message at the raise site, the exception class becomes less reusable, and may now raise inconsistent messages depending on where it is raised. If the exception message is instead defined within the exception class, it will be consistent across all raise invocations. This rule is not enforced for some built-in exceptions that are commonly raised with a message and would be unusual to subclass, such as NotImplementedError. Example class CantBeNegative(Exception): pass def foo(x): if x < 0: raise CantBeNegative(f\"{x} is negative\") Use instead: class CantBeNegative(Exception): def __init__(self, number): super().__init__(f\"{number} is negative\") def foo(x): if x < 0: raise CantBeNegative(x)",
    "option": {}
  },
  "TRY004": {
    "description": "[Status: stable] [Name: type-check-without-type-error] [Message: Prefer <code>TypeError</code> exception for invalid type] What it does Checks for type checks that do not raise TypeError. Why is this bad? The Python documentation states that TypeError should be raised upon encountering an inappropriate type. Example def foo(n: int): if isinstance(n, int): pass else: raise ValueError(\"n must be an integer\") Use instead: def foo(n: int): if isinstance(n, int): pass else: raise TypeError(\"n must be an integer\")",
    "option": {}
  },
  "TRY201": {
    "description": "[Status: stable] [Name: verbose-raise] [Message: Use <code>raise</code> without specifying exception name] What it does Checks for needless exception names in raise statements. Why is this bad? It's redundant to specify the exception name in a raise statement if the exception is being re-raised. Example def foo(): try: ... except ValueError as exc: raise exc Use instead: def foo(): try: ... except ValueError: raise Fix safety This rule's fix is marked as unsafe, as it doesn't properly handle bound exceptions that are shadowed between the except and raise statements.",
    "option": {}
  },
  "TRY203": {
    "description": "[Status: stable] [Name: useless-try-except] [Message: Remove exception handler; error is immediately re-raised] What it does Checks for immediate uses of raise within exception handlers. Why is this bad? Capturing an exception, only to immediately reraise it, has no effect. Instead, remove the error-handling code and let the exception propagate upwards without the unnecessary try-except block. Example def foo(): try: bar() except NotImplementedError: raise Use instead: def foo(): bar()",
    "option": {}
  },
  "TRY300": {
    "description": "[Status: stable] [Name: try-consider-else] [Message: Consider moving this statement to an <code>else</code> block] What it does Checks for return statements in try blocks. Why is this bad? The try-except statement has an else clause for code that should run only if no exceptions were raised. Returns in try blocks may exhibit confusing or unwanted behavior, such as being overridden by control flow in except and finally blocks, or unintentionally suppressing an exception. Example import logging def reciprocal(n): try: rec = 1 / n print(f\"reciprocal of {n} is {rec}\") return rec except ZeroDivisionError: logging.exception(\"Exception occurred\") raise Use instead: import logging def reciprocal(n): try: rec = 1 / n except ZeroDivisionError: logging.exception(\"Exception occurred\") raise else: print(f\"reciprocal of {n} is {rec}\") return rec",
    "option": {}
  },
  "TRY301": {
    "description": "[Status: stable] [Name: raise-within-try] [Message: Abstract <code>raise</code> to an inner function] What it does Checks for raise statements within try blocks. The only raises caught are those that throw exceptions caught by the try statement itself. Why is this bad? Raising and catching exceptions within the same try block is redundant, as the code can be refactored to avoid the try block entirely. Alternatively, the raise can be moved within an inner function, making the exception reusable across multiple call sites. Example def bar(): pass def foo(): try: a = bar() if not a: raise ValueError except ValueError: raise Use instead: def bar(): raise ValueError def foo(): try: a = bar() # refactored `bar` to raise `ValueError` except ValueError: raise",
    "option": {}
  },
  "TRY400": {
    "description": "[Status: stable] [Name: error-instead-of-exception] [Message: Use <code>logging.exception</code> instead of <code>logging.error</code>] What it does Checks for uses of logging.error instead of logging.exception when logging an exception. Why is this bad? logging.exception logs the exception and the traceback, while logging.error only logs the exception. The former is more appropriate when logging an exception, as the traceback is often useful for debugging. Example import logging def func(): try: raise NotImplementedError except NotImplementedError: logging.error(\"Exception occurred\") Use instead: import logging def func(): try: raise NotImplementedError except NotImplementedError: logging.exception(\"Exception occurred\") Fix safety This rule's fix is marked as safe when run against logging.error calls, but unsafe when marked against other logger-like calls (e.g., logger.error), since the rule is prone to false positives when detecting logger-like calls outside of the logging module.",
    "option": {
      "lint.logger-objects": {
        "option_description": "A list of objects that should be treated equivalently to a\nlogging.Logger object.\n\nThis is useful for ensuring proper diagnostics (e.g., to identify\nlogging deprecations and other best-practices) for projects that\nre-export a logging.Logger object from a common module.\n\nFor example, if you have a module logging_setup.py with the following\ncontents:\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nAdding \"logging_setup.logger\" to logger-objects will ensure that\nlogging_setup.logger is treated as a logging.Logger object when\nimported from other modules (e.g., from logging_setup import logger).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\nlogger-objects = [\"logging_setup.logger\"]\n\n\n\n[lint]\nlogger-objects = [\"logging_setup.logger\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  },
  "TRY401": {
    "description": "[Status: stable] [Name: verbose-log-message] [Message: Redundant exception object included in <code>logging.exception</code> call] What it does Checks for excessive logging of exception objects. Why is this bad? When logging exceptions via logging.exception, the exception object is logged automatically. Including the exception object in the log message is redundant and can lead to excessive logging. Example try: ... except ValueError as e: logger.exception(f\"Found an error: {e}\") Use instead: try: ... except ValueError: logger.exception(\"Found an error\")",
    "option": {
      "lint.logger-objects": {
        "option_description": "A list of objects that should be treated equivalently to a\nlogging.Logger object.\n\nThis is useful for ensuring proper diagnostics (e.g., to identify\nlogging deprecations and other best-practices) for projects that\nre-export a logging.Logger object from a common module.\n\nFor example, if you have a module logging_setup.py with the following\ncontents:\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nAdding \"logging_setup.logger\" to logger-objects will ensure that\nlogging_setup.logger is treated as a logging.Logger object when\nimported from other modules (e.g., from logging_setup import logger).\n\n\n\nExample usage:\n\npyproject.tomlruff.toml\n\n\n[tool.ruff.lint]\nlogger-objects = [\"logging_setup.logger\"]\n\n\n\n[lint]\nlogger-objects = [\"logging_setup.logger\"]",
        "option_data_type": "list[str]",
        "default_value": "[]",
        "option_values": {
          "valuelist": []
        }
      }
    }
  }
}